{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68b2a55-00cb-4b84-b408-58521def3fec",
   "metadata": {},
   "source": [
    "# LDA details "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f18556-3a79-402b-b137-4fee6e0c36a2",
   "metadata": {},
   "source": [
    "### (Optional) Plate notation\n",
    "\n",
    "- Used in Bayesian inference for representing variables that repeat. \n",
    "- It shows the generative process of the LDA model. It also shows the dependency structure in the probability distribution.\n",
    "- We are not going into the details but I would like you to be familiar with this picture at a high-level because it's likely that you might see it in the context of topic modeling. \n",
    "\n",
    "![](img/topic_modeling_plate_diagram.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_plate_diagram.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "- $\\beta_k \\rightarrow$ Distribution over words for topic $k$\n",
    "- $\\theta_d \\rightarrow$ Distribution over topics for document $d$ \n",
    "- $w_n \\rightarrow$ word\n",
    "- $Z_n \\rightarrow$ topic\n",
    "- $N \\rightarrow$ Size of the vocabulary\n",
    "- $M \\rightarrow$ Number of documents\n",
    "- $\\lambda \\rightarrow$ Hyperparameter for word proportion\n",
    "- $\\alpha\\rightarrow$ Hyperparameter for topic proportion  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc1d17-31a3-4982-b232-349d4c95724f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Optional) Mathematical presentation of the generative story (plate diagram)\n",
    "\n",
    "![](img/topic_modeling_plate_diagram.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_plate_diagram.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "\n",
    "- For each topic $k \\in \\{1, \\dots, K\\}$ draw a multinomial distribution $\\beta_k$ from a Dirichlet distribution with parameter $\\lambda$. \n",
    "- For each document $d \\in \\{1, \\dots, M\\}$, draw a multinomial distribution $\\theta_d$\n",
    "from a Dirichlet distribution with parameter $\\alpha$. \n",
    "- For each word position $n \\in \\{1, \\dots, N\\}$, select a hidden topic $Z_n$ from the multinomial distribution parameterized by $\\theta$.\n",
    "- Choose the observed word $w_n$ from the distribution $\\beta_{Z_n}$. \n",
    "\n",
    "[Source](http://users.umiacs.umd.edu/~jbg/teaching/CMSC_726/16a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2c7a7c-6318-434d-b9a1-480c0edc6e2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Optional) LDA Inference\n",
    "\n",
    "- Infer the underlying topic structure in the documents. In particular, \n",
    "    - Learn the discrete probability distributions of topics in each document\n",
    "    - Learn the discrete probability distributions of words in each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc630c0-30dd-499a-8473-59a8c3778b8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Optional) LDA Inference\n",
    "\n",
    "- We are interested in the posterior distribution: $P(z, \\beta, \\theta| w_n, \\alpha, \\lambda)$\n",
    "- Observations: words. Everything else is hidden (latent). \n",
    "\n",
    "![](img/topic_modeling_plate_diagram.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_plate_diagram.png\" height=\"600\" width=\"600\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "\n",
    "- $\\lambda$: Hyperparameter for word proportion\n",
    "    - High $\\lambda$ &rarr; every topic contains a mixture of most of the words\n",
    "    - Low $\\lambda$ &rarr; every topic contains a mixture of only few words\n",
    "    \n",
    "- $\\alpha$: Hyperparameter for topic proportion  \n",
    "   - High $\\alpha$ &rarr; every document contains a mixture of most of the topics\n",
    "   - Low $\\alpha$ &rarr; every document is representative of only a few topics    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c6749-059e-4361-9f36-4450e97cd25f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Optional) Calculating the conditional probability\n",
    "\n",
    "- Sets up a Markov chain that converges into the posterior distribution of the model parameters or wordâ€“topic assignments.\n",
    "- Two components\n",
    "    - How much this document likes topic $k$: \n",
    "    $$\\frac{n_{d,k} + \\alpha_k}{\\sum^K_i n_{d,i} + \\alpha_i}$$\n",
    "    - How much this topic likes word $w_{d,n}$: $$\\frac{V_{k, w_{d,n}} + \\lambda_{w_{d,n}}}{\\sum_i V_{k,i} + \\lambda_i}$$ \n",
    "- The conditional probability of word topic assignment given everything else in the model: \n",
    "\n",
    "$$\\frac{n_{d,k} + \\alpha_k}{\\sum^K_i n_{d,i} + \\alpha_i} \\frac{V_{k, w_{d,n}} + \\lambda_{w_{d,n}}}{\\sum_i V_{k,i} + \\lambda_i}$$\n",
    "\n",
    "- $n_{d,k} \\rightarrow$ number of times document $d$ uses topic $k$ \n",
    "- $V_{k, w_{d,n}} \\rightarrow$ number of times topic $k$ uses word type $w_{d,n}$\n",
    "- $\\alpha_k \\rightarrow$ Dirichlet parameter for document to topic distribution\n",
    "- $\\lambda_{w_{d,n}} \\rightarrow$ Dirichlet parameter for topic to word distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec8fff-98f1-44df-afc9-91c0eb8200ba",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### (Optional) LDA algorithm \n",
    "\n",
    "- Suppose $K$ is number of topics\n",
    "- For each iteration $i$\n",
    "    - For each document $d$ and word $n$ currently assigned to topic $Z_{old}$\n",
    "        - Decrement $n_{d,Z_{old}}$ and $V_{Z_{old}, w_{d,n}}$\n",
    "        - Sample $Z_{new} = k$ with probability proportional to $\\frac{n_{d,k} + \\alpha_k}{\\sum^K_i n_{d,i} + \\alpha_i} \\frac{V_{k, w_{d,n}} + \\lambda_{w_{d,n}}}{\\sum_i V_{k,i} + \\lambda_i}$\n",
    "        - Increment $n_{d, Z_{new}} and V_{Z_{new}, w_{d,n}}$\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
