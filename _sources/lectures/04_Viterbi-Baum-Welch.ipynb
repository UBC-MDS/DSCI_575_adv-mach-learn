{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/575_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 4: Decoding and Learning in HMMs\n",
    "\n",
    "UBC Master of Data Science program, 2023-24\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture plan, imports, LO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.abspath(\".\"), \"code\"))\n",
    "from plotting_functions import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Learning outcomes\n",
    "\n",
    "From this lesson you will be able to\n",
    "\n",
    "- Explain the general idea and purpose of the Viterbi algorithm.\n",
    "- Explain the three steps in the Viterbi algorithm and apply it given an HMM and an observation sequence.\n",
    "- Compute $\\delta_i(t)$ and $\\psi_i(t)$ for a given state $i$ at time step $t$. \n",
    "- Explain the general idea of the backward algorithm.\n",
    "- Compute $\\beta_i(t)$ for a given state $i$ at time step $t$. \n",
    "- Explain the purpose and the general idea of the Baum-Welch algorithm.\n",
    "- Use `hmmlearn` for liklihood, decoding, and HMM unsupervised training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Hidden Markov models (HMMs) model sequential data with latent factors.\n",
    "- There are tons of applications associated with them and they are more realistic than Markov models. \n",
    "\n",
    "![](img/HMM_example_small.png)\n",
    "\n",
    "<!-- <img src=\"img/HMM_example.png\" height=\"500\" width=\"500\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HMM ingredients\n",
    "- Hidden states (e.g., Happy, Sad)\n",
    "- Output alphabet or output symbols or observations (e.g., learn, study, cry, facebook)\n",
    "- Discrete initial state probability distribution\n",
    "- Transition probabilities\n",
    "- Emission probabilities    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The three fundamental questions for an HMM. \n",
    "\n",
    "- **Likelihood**\n",
    "Given a model with parameters $\\theta = <\\pi, A, B>$, how do we efficiently compute the likelihood of a particular observation sequence $O$?\n",
    "\n",
    "- **Decoding**\n",
    "Given an observation sequence $O$ and a model $\\theta$ how do we choose a state sequence $Q={q_0, q_1, \\dots q_T}$ that best explains the observation sequence?\n",
    "\n",
    "- **Learning**\n",
    "Training: Given a large observation sequence $O$ how do we choose the best parameters $\\theta$ that explain the data $O$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: The forward algorithm\n",
    "- The forward algorithm is a dynamic programming algorithm to efficiently estimate the probability of an observation sequence $P(O;\\theta)$ given an HMM.\n",
    "  \n",
    "- For each state $i$, we calculated $\\alpha_i(0), \\alpha_i(1), \\alpha_i(2), ...\\alpha_i(t)$, which represent the probabilities of being in state $i$ at times $t$ knowing all the observations which came before and at time $t$. \n",
    "- The trellis was computed left to right and top to bottom.\n",
    "- The forward algorithm stores the probabilities of all possible 1-state sequences (from the start), to store all possible 2-state sequences (from the start), to store all possible 3-state sequences (from the start) and so on. \n",
    "\n",
    "![](img/hmm_alpha_values_small.png)\n",
    "\n",
    "- Sum over all possible final states:\n",
    "  * $P(O;\\theta) = \\sum\\limits_{i=1}^{n}\\alpha_i(T-1)$\n",
    "  * $P(E,L,F,C) = \\alpha_ğŸ™‚(3) + \\alpha_ğŸ˜”(3) = 0.00023 + 0.00207 = 0.0023$ \n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/hmm_alpha_values.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Recall the three fundamental questions for an HMM. \n",
    "\n",
    "- **Likelihood**\n",
    "Given a model with parameters $\\theta = <\\pi, A, B>$, how do we efficiently compute the likelihood of a particular observation sequence $O$?\n",
    "\n",
    "- **Decoding**\n",
    "Given an observation sequence $O$ and a model $\\theta$ how do we choose a state sequence $Q={q_0, q_1, \\dots q_T}$ that best explains the observation sequence?\n",
    "\n",
    "- **Learning**\n",
    "Training: Given a large observation sequence $O$ how do we choose the best parameters $\\theta$ that explain the data $O$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Decoding: The Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Given an observation sequence $O$ and a model $\\theta$ how do we choose a state sequence $Q={q_0, q_1, \\dots q_T}$ that best explains the observation sequence?\n",
    "  \n",
    "- Purpose: finding what's most likely going on under the hood. \n",
    "- For example: It tells us the most likely part-of-speech tags given an English sentence.\n",
    "\n",
    "<blockquote>\n",
    "Will/MD the/DT chair/NN chair/VB the/DT meeting/NN from/IN that/DT chair/NN?\n",
    "</blockquote>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "More formally, \n",
    "\n",
    "- Given an HMM, choose the state sequence that maximizes the probability of the output sequence.\n",
    "  \n",
    " * $Q^* = \\arg \\max\\limits_Q P(O,Q;\\theta)$, \n",
    " * $P(O,Q;\\theta) = \\pi_{q_0}b_{q_0}(o_0) \\prod\\limits_{t=1}^{T}a_{q_{t-1}}a_{q_t}b_{q_t}(o_t)$\n",
    " \n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\">  -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Can we use the forward algorithm for decoding?**\n",
    "\n",
    "If we want to pick an optimal state sequence which maximizes the probability of the observation sequence, how about picking the state with maximum $\\alpha$ value at each time step? \n",
    "\n",
    "![](img/hmm_alpha_values_small.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/hmm_alpha_values.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- </center> -->\n",
    "\n",
    "If we pick the most probable state at each time step based on the $\\alpha$ values, it might not end up as the best state sequence because it might be possible that the transition between two highly probable states in a sequence is very unlikely. \n",
    "\n",
    "We need something else. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Viterbi algorithm: Overview**\n",
    "\n",
    "- Dynamic programming algorithm.\n",
    "  \n",
    "- We use a different kind of trellis.\n",
    "  \n",
    "- Want: Given an HMM, choose the state sequence that maximizes the probability of the output sequence.  \n",
    " * $Q^* = \\arg \\max\\limits_Q P(O,Q;\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We store $\\delta$ and $\\psi$ values at each node in the trellis\n",
    "\n",
    "- $\\delta_i(t)$ represents the probability of the most probable path leading to the trellis node at state $i$ and time $t$\n",
    "- $\\psi_i(t)$ represents the best possible previous state if I am in state $i$ at time $t$. \n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\">  -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the algorithm step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2 Viterbi: Initialization\n",
    "- Initialize with $\\delta_i(0) = \\pi_i b_i(o_0)$ for all states\n",
    "    - $\\delta_ğŸ™‚(0) = \\pi_ğŸ™‚ b_ğŸ™‚(E) = 0.8 \\times 0.2 = 0.16$\n",
    "    - $\\delta_ğŸ˜”(0) = \\pi_ğŸ˜” b_ğŸ˜”(E) = 0.2 \\times 0.1 = 0.02$\n",
    "    \n",
    "- Initialize with $\\psi_i(0) = 0 $, for all states   \n",
    "    - $\\psi_ğŸ™‚(0) = 0, \\psi_ğŸ˜”(0) = 0$\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3 Viterbi: Induction\n",
    "\n",
    "The best path $\\delta_t$ to state $j$ at time $t$ depends on the best path to each\n",
    "possible previous state $\\delta_i(t-1)$ and their transitions to $j$ ($a_{ij}$). \n",
    "\n",
    "- $\\delta_j(t) = \\max\\limits_i \\{\\delta_i(t-1)a_{ij}\\} b_j(o_t)$\n",
    "- $\\psi_j(t) = \\arg \\max\\limits_i \\{\\delta_i(t-1)a_{ij}\\} $\n",
    "\n",
    "![](img/HMM_example_trellis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![](img/viterbi_explanation_small.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/viterbi_explanation.png\" height=\"150\" width=\"150\">  -->\n",
    "<!-- </center> -->\n",
    "\n",
    "- There are two possible paths to state ğŸ™‚ at $T = 1$. Which is the best one? \n",
    "- $\\delta_ğŸ™‚(1) = \\max \\begin{Bmatrix} \\delta_ğŸ™‚(0) \\times a_{ğŸ™‚ğŸ™‚},\\\\ \\delta_ğŸ˜”(0) \\times a_{ğŸ˜”ğŸ™‚}\\end{Bmatrix}  \\times b_ğŸ™‚(L)$\n",
    "- First take the max between $\\delta_ğŸ™‚(0) \\times a_{ğŸ™‚ğŸ™‚}$ and $\\delta_ğŸ˜”(0) \\times a_{ğŸ˜”ğŸ™‚}$ and then multiply the max by $b_ğŸ™‚(L)$.   \n",
    "    \n",
    "- $\\psi_ğŸ™‚(1)$ = the state at $T=0$ from where the path to ğŸ™‚ at $T=1$ was the best one.     \n",
    "\n",
    "- **Note that we use parentheses to show two quantities for taking the max. (Not the best notation but I have seen it being used in this context.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Viterbi: Induction (T = 1)**\n",
    "\n",
    "$\\delta$ and $\\psi$ at state ğŸ™‚ and T = 1\n",
    "- $\\delta_{ğŸ™‚}(1) = \\max\\limits_i \\{\\delta_i(0)a_{ij}\\} b_j(o_t) = \n",
    "\\max \\begin{Bmatrix} 0.16 \\times 0.7, \\\\ 0.02 \\times 0.4\\end{Bmatrix} \\times 0.7 = 0.0784$\n",
    "- $\\psi_{ğŸ™‚}(1) = \\arg \\max\\limits_i \\{\\delta_i(0)a_{ij}\\} = ğŸ™‚$\n",
    "\n",
    "$\\delta$ and $\\psi$ at state ğŸ˜” and T = 1\n",
    "- $\\delta_{ğŸ˜”}(1) = \\max\\limits_i \\{\\delta_i(0)a_{ij}\\} b_j(o_t) =  \\max \\begin{Bmatrix} 0.16 \\times 0.3, \\\\ 0.02 \\times 0.6\\end{Bmatrix} \\times 0.1 = 0.0048$\n",
    "- $\\psi_{ğŸ˜”}(1) = \\arg \\max\\limits_i \\{\\delta_i(0)a_{ij}\\} = ğŸ™‚$\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Viterbi: Induction (T = 1)**\n",
    "\n",
    "$\\delta$ and $\\psi$ at state ğŸ™‚ and T = 1\n",
    "- $\\delta_{ğŸ™‚}(1) = \\max \\begin{Bmatrix} \\delta_ğŸ™‚(0) \\times a_{ğŸ™‚ğŸ™‚}, \\\\ \\delta_ğŸ˜”(0) \\times a_{ğŸ˜”ğŸ™‚}\\end{Bmatrix}  \\times b_ğŸ™‚(L) = \n",
    "\\max \\begin{Bmatrix} 0.16 \\times 0.7, \\\\ 0.02 \\times 0.4\\end{Bmatrix} \\times 0.7 = 0.0784$\n",
    "- $\\psi_{ğŸ™‚}(1)  = ğŸ™‚$\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Viterbi: Induction (T = 1)**\n",
    "\n",
    "$\\delta$ and $\\psi$ at state ğŸ˜” and T = 1\n",
    "- $\\delta_{ğŸ˜”}(1) = \\max\\limits_i \\{\\delta_i(0)a_{ij}\\} b_j(o_t) =  \\max \\begin{Bmatrix} 0.16 \\times 0.3 ,\\\\ 0.02 \\times 0.6\\end{Bmatrix} \\times 0.1 = 0.0048$\n",
    "- $\\psi_{ğŸ˜”}(1) = \\arg \\max\\limits_i \\{\\delta_i(0)a_{ij}\\} = ğŸ™‚$\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Viterbi: Induction (T = 2)**\n",
    "\n",
    "- $\\delta$ and $\\psi$ at state ğŸ™‚ and T = 2\n",
    "    - $\\delta_{ğŸ™‚}(2) = \\max\\limits_i \\{\\delta_i(1)a_{ij}\\} b_j(o_t) =  \\max \\begin{Bmatrix} 0.784 \\times 0.7, \\\\ 0.0048 \\times 0.4 \\end{Bmatrix}\\times 0 = 0\n",
    "$\n",
    "    - $\\psi_{ğŸ™‚}(2) = \\arg \\max\\limits_i \\{\\delta_i(1)a_{ij}\\} = ğŸ™‚$\n",
    "\n",
    "- $\\delta$ and $\\psi$ at state ğŸ˜” and T = 2\n",
    "    - $\\delta_{ğŸ˜”}(2) = \\max\\limits_i \\{\\delta_i(1)a_{ij}\\} b_j(o_t) =  \\max \\begin{Bmatrix} 0.784 \\times 0.3, \\\\ 0.0048 \\times 0.6 \\end{Bmatrix}\\times 0.2 = 4.704 \\times 10^{-3}$\n",
    "    - $\\psi_{ğŸ˜”}(2) = \\arg \\max\\limits_i \\{\\delta_i(1)a_{ij}\\} = ğŸ™‚$\n",
    "\n",
    "<br>\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"400\" width=\"400\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Viterbi: Induction (T = 3)**\n",
    "\n",
    "- $\\delta$ and $\\psi$ at state ğŸ™‚ and T = 3\n",
    "    - $\\delta_{ğŸ™‚}(3) = \\max\\limits_i \\{\\delta_i(2)a_{ij}\\} b_j(o_t) = \\max \\begin{Bmatrix} 0 \\times 0.7, \\\\ 4.704 \\times 10^{-3} \\times 0.4 \\end{Bmatrix} \\times 0.1 = 1.88\\times10^{-4}\n",
    "$\n",
    "    - $\\psi_{ğŸ™‚}(3) = \\arg \\max\\limits_i \\{\\delta_i(2)a_{ij}\\} = ğŸ˜”$\n",
    "\n",
    "- $\\delta$ and $\\psi$ at state ğŸ˜” and T = 3\n",
    "    - $\\delta_{ğŸ˜”}(3) = \\max\\limits_i \\{\\delta_i(2)a_{ij}\\} b_j(o_t) = \\max \\begin{Bmatrix} 0 \\times 0.3, \\\\ 4.704 \\times 10^{-3} \\times 0.6 \\end{Bmatrix} \\times 0.6 = 1.69 \\times 10^{-3}$\n",
    "    - $\\psi_{ğŸ˜”}(3) = \\arg \\max\\limits_i \\{\\delta_i(2)a_{ij}\\} = ğŸ˜”$\n",
    "\n",
    "<br>\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"400\" width=\"400\">  -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.4 Viterbi conclusion\n",
    "- Choose the best final state: $q_t^* = \\arg \\max\\limits_i \\delta_i(t)$\n",
    "- Recursively choose the best previous state: $q_{t-1}^* = \\psi_{q_t^*}(t)$\n",
    "    - The most likely state sequence for the observation sequence ELFC is ğŸ™‚ğŸ™‚ğŸ˜”ğŸ˜”.\n",
    "- The probability of the state sequence is the probability of $q_t^*$\n",
    "    - $P(ğŸ™‚ğŸ™‚ğŸ˜”ğŸ˜”) = 1.69 \\times 10^{-3}$    \n",
    "    \n",
    "![](img/HMM_viterbi_conclusion_small.png)\n",
    "\n",
    "<!-- <img src=\"img/HMM_viterbi_conclusion.png\" height=\"600\" width=\"600\">  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.5 Viterbi with [ `hmmlearn`](https://hmmlearn.readthedocs.io) on our toy HMM\n",
    "\n",
    "![](img/HMM_example_small.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's get the optimal state sequence using Viterbi in our toy example.  \n",
    "- We assume that we already have the model, i.e., transition probabilities, emission probabilities, and initial state probabilities. \n",
    "- Our goal is to efficiently find the best state sequence for the given observation sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Initializing an HMM\n",
    "states = [\"Happy\", \"Sad\"]\n",
    "n_states = len(states)\n",
    "\n",
    "observations = [\"Learn\", \"Eat\", \"Cry\", \"Facebook\"]\n",
    "n_observations = len(observations)\n",
    "\n",
    "# Since we've discrete observations, we'll use `CategoricalHMM`\n",
    "model = hmm.CategoricalHMM(n_components=n_states)\n",
    "\n",
    "# Set the initial state probabilities\n",
    "model.startprob_ = np.array([0.8, 0.2])\n",
    "\n",
    "# Set the transition matrix\n",
    "model.transmat_ = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "\n",
    "# Set the emission probabilities of shape (n_components, n_symbols)\n",
    "model.emissionprob_ = np.array([[0.7, 0.2, 0.1, 0.0], [0.1, 0.1, 0.6, 0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_state_seq(model, observation_seq, states=states, symbols=observations):\n",
    "    logprob, state_seq = model.decode(observation_seq, algorithm=\"viterbi\")\n",
    "    o_seq = map(lambda x: symbols[x], observation_seq.T[0])\n",
    "    s_seq = map(lambda x: states[x], state_seq)\n",
    "    print(\"log probability of state sequence: \", logprob)\n",
    "    return pd.DataFrame(data=s_seq, index=o_seq, columns=[\"state sequence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log probability of state sequence:  -6.3809933159177925\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eat</th>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learn</th>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook</th>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cry</th>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         state sequence\n",
       "Eat               Happy\n",
       "Learn             Happy\n",
       "Facebook            Sad\n",
       "Cry                 Sad"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoding example\n",
    "toy_seq = np.array([[1], [0], [3], [2]])\n",
    "get_state_seq(model, toy_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- This is how you find the best state sequence that explains the observation sequence using the Viterbi algorithm!\n",
    "  \n",
    "- Much faster than the brute force approach of considering all possible state combinations, calculating probabilities for each of them and taking the one resulting in maximum probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## â“â“ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "**iClicker join link: https://join.iclicker.com/ZTLY**\n",
    "\n",
    "- (A) In Viterbi, $\\delta_i(t)$ is the probability of the best path (i.e., the path with highest probability) which accounts for the first $t$ observations and ending at state $i$.\n",
    "- (B) In Viterbi, suppose at $t-1$, state $i$ has the highest $\\delta_i(t-1)$ among all states. Then at time step $t$, the path from $i$ at $t-1$ is going to give us the highest $\\delta_j(t)$ for all states $j$ at time $t$.\n",
    "- (C) In Viterbi, the $\\psi_j(t)$ keeps track of the state from the previous time step which results in highest $\\delta_i(t-1)a_{ij}$ so that we can keep track of where we came from and we can recreate the path. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 4.1: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "\n",
    "- (A) True\n",
    "- (B) False. This will also depend upon the transition probabilities between states.\n",
    "- (C) True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. The backward algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In the last lecture we talked about supervised training of HMMs where we assumed that we had mapping between observation sequences and hidden state sequences. \n",
    "- In real life we rarely have such mapping available.  \n",
    "- For example, you can imagine how much manual effort it would be to come up with gold part-of-speech tag sequences on a large enough sample of text data, say Wikipedia, so that we have enough training data in order to learn initial state probabilities, transition probabilities, and emission probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question we want to answer**  \n",
    "\n",
    "- Given a large observation sequence (or a set of observation sequences) $O$ for training, but **not** the state sequence, how do we choose the \"best\" parameters $\\theta$ that explain the data $O$? \n",
    "\n",
    "- We want our parameters $\\theta$ to be set so that the available training data is maximally likely.\n",
    "  \n",
    "- We do this using the forward-backward algorithm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Recall: The forward algorithm**\n",
    "\n",
    "- Computer probability of a given observation sequence. \n",
    "\n",
    "- Given a model with parameters $\\theta = <\\pi, T, B>$, how do we efficiently compute the probability of a particular observation sequence $O$?\n",
    "\n",
    "- Example: What's the probability of the sequence below? \n",
    "\n",
    "![](img/HMM_example_activity_seq_small.png)\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_activity_seq.png\" height=\"400\" width=\"400\">     -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Three steps of the forward algorithm. \n",
    "\n",
    "- Initialization: Compute the $\\alpha$ values for nodes in the first column of the trellis $(t = 0)$.\n",
    "- Induction: Iteratively compute the $\\alpha$ values for nodes in the rest of the trellis $(1 \\leq t < T)$.\n",
    "- Conclusion: Sum over the $\\alpha$ values for nodes in the last column of the trellis $(t = T)$.\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Sum over all possible final states:\n",
    "  * $P(O;\\theta) = \\sum\\limits_{i=1}^{n}\\alpha_i(T-1)$\n",
    "  * $P(E,L,F,C) = \\alpha_ğŸ™‚(3) + \\alpha_ğŸ˜”(3) = 0.00023 + 0.00207 = 0.0023$ \n",
    "\n",
    "![](img/hmm_alpha_values_small.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/hmm_alpha_values.png\" height=\"700\" width=\"700\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What are we doing in the forward algorithm?**\n",
    "\n",
    "- In forward algorithm the point is to compute $P(O;\\theta)$. \n",
    "- For each state $i$, we calculated $\\alpha_i(0), \\alpha_i(1), \\alpha_i(2), ...$\n",
    "- The trellis was computed left to right and top to bottom.\n",
    "- The forward algorithm stores the probabilities of all possible 1-state sequences (from the start), to store all possible 2-state sequences (from the start), to store all possible 3-state sequences (from the start) and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2 The backward algorithm intuition\n",
    "\n",
    "- Less intuitively, we can also do that in reverse order, i.e., from  right to left and top to bottom. \n",
    "- We'll still deal with the same observation sequence which evolves forward in time but we will store temporary results in the backward direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In the $i^{th}$ node of the trellis at time $t$, we store the probability of starting in state $i$ at time $t$ then observing everything that comes thereafter. \n",
    "$$\\beta_{i}(t) = P(b_{t+1:T-1})$$\n",
    "\n",
    "- The trellis is computed **right-to-left** and **top-to-bottom**. \n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The backward algorithm: steps**\n",
    "\n",
    "Three steps of the backward procedure. \n",
    "\n",
    "- Initialization: Initialize $\\beta$ values for nodes in the last column of the trellis. \n",
    "$$\\beta_i(T-1) = 1$$\n",
    "- Induction: Iteratively compute the $\\beta$ values for nodes in the rest of the trellis $(1 \\leq t < T)$ as the probability of being in state $i$ at time $t$ and reading everything to follow.\n",
    "$$\\beta_i(t) = \\sum_{j=1}^N a_{ij}b_{j}(o_{t+1}) \\beta_j(t+1)$$\n",
    "- Conclusion: Sum over the $\\beta$ values for nodes in the first column of the trellis $(t = 0)$ (i.e., all initial states).\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3 The backward algorithm: Initialization $\\beta_ğŸ™‚(3)$ and $\\beta_ğŸ˜”(3)$\n",
    "\n",
    "- Initialize the nodes in the last column of the trellis $(T = 3)$.\n",
    "    * $\\beta_ğŸ™‚(3) = 1.0$\n",
    "    * $\\beta_ğŸ˜”(3) = 1.0$    \n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 2.4 The backward algorithm: Induction\n",
    "\n",
    "- Iteratively compute the nodes in the rest of the trellis $(1 \\leq t < T)$.\n",
    "-  To compute $\\beta_j(t)$ we can compute $\\beta_{i}(t+1)$ for all possible states $i$ and then use our knowledge of $a_{ij}$ and $b_j(o_{t+1})$ \n",
    "$$\\beta_i(t) = \\sum_{j=1}^N a_{ij}b_{j}(o_{t+1}) \\beta_j(t+1)$$\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The backward algorithm: Induction $\\beta_ğŸ™‚(2)$**\n",
    "\n",
    "$$\\beta_i(t) = \\sum_{j=1}^N a_{ij}b_{j}(o_{t+1}) \\beta_j(t+1)$$\n",
    "\n",
    "- Probability of being at state ğŸ™‚ at $t=2$ and observing everything to follow.  \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\beta_ğŸ™‚(2) = & a_{ğŸ™‚ğŸ™‚}b_ğŸ™‚(C)\\beta_ğŸ™‚(3) + a_{ğŸ™‚ğŸ˜”}b_ğŸ˜”(C)\\beta_ğŸ˜”(3)\\\\\n",
    "             = & 0.7 \\times 0.1 \\times 1.0 + 0.3 \\times 0.6 \\times 1.0\\\\ \n",
    "             = & 0.25& \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\">  -->\n",
    "<!-- </center>     -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The backward algorithm: Induction $\\beta_ğŸ˜”(2)$**\n",
    "\n",
    "$$\\beta_i(t) = \\sum_{j=1}^N a_{ij}b_{j}(o_{t+1}) \\beta_j(t+1)$$\n",
    "\n",
    "- Probability of being at state ğŸ˜” at $t=2$ and observing everything to follow.  \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\beta_ğŸ˜”(2) = & a_{ğŸ˜”ğŸ™‚}b_ğŸ™‚(C)\\beta_ğŸ™‚(3) + a_{ğŸ˜”ğŸ˜”}b_ğŸ˜”(C)\\beta_ğŸ˜”(3)\\\\\n",
    "             = & 0.4 \\times 0.1 \\times 1.0 + 0.6 \\times 0.6 \\times 1.0\\\\ \n",
    "             = & 0.4& \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\">  -->\n",
    "<!-- </center>     -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Carry out rest of the steps as home work.**\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.5 The backward algorithm: Conclusion\n",
    "\n",
    "- Sum over all possible initial states to get the probability of an observation sequence in the reverse direction. \n",
    "\n",
    "$$P(O;\\theta) = \\sum_{i=1}^{N} \\pi_i b_i(O_0)\\beta_i(0)$$\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We're not doing this just for fun. \n",
    "- We are going to use it for unsupervised HMM training! \n",
    "- In general, we can combine $\\alpha$ and $\\beta$ at any point in time to represent the probability of an entire sequence.  \n",
    "- This is going to be vital for training of unsupervised HMMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Baum-Welch (BW) algorithm (high-level idea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Introduction\n",
    "\n",
    "Given a large observation sequence (or a set of observation sequences) $O$ for training, but **not** the state sequence, how do we choose the \"best\" parameters $\\theta$ that explain the data $O$? \n",
    "\n",
    "We want our parameters $\\theta$ to be set so that the available training data is maximally likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can we use MLE?**\n",
    "- If the training data contained state sequences, we could simply do maximum likelihood estimation, as we did in the last lecture, to get transition probabilities and the emission probabilities.\n",
    "  \n",
    "- But when we are only given observations, we **cannot** count the following: \n",
    "    -  How often we move from $q_{i-1}$ to $q_i$ normalized by how often we move from $q_{i-1}$ to anything: \n",
    "      $p(q_i|q_{i-1}) = \\frac{Count(q_{i-1} q_i)}{Count(q_{i-1} \\text{ANY STATE })}$\n",
    "    - What's the proportion of $q_i$ emitting the observation $o_i$ .   \n",
    "      $p(o_i|q_{i}) = \\frac{Count(o_i \\text{ and } q_i)}{Count(q_{i})}$\n",
    "\n",
    "- In many cases, the mapping between hidden states and observations is unknown and so we can't use MLE.      \n",
    "- How to deal with the incomplete data?\n",
    "    - Use unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2 Iterative unsupervised approach \n",
    "\n",
    "- We guess the parameters and iteratively update them. \n",
    "- Unsupervised HMM training is done using a combination of the forward and the backward algorithms.  \n",
    "- The idea is that we can combine $\\alpha$ and $\\beta$ at any point in time to represent the probability of an entire observation sequence.  \n",
    "- The forward algorithm computes the $\\alpha$ values, which represent the probability of being in a particular state at a particular time, given the observation sequence up to that time. \n",
    "- The backward algorithm computes the $\\beta$ values, which represent the probability of observing the rest of the sequence after that time.\n",
    "- We define $\\gamma_i(t)$, which represents the probability of being in state $i$ at time $t$ given the entire observation sequence $O$. We calculate it by combining $\\alpha$ and $\\beta$ values calculated by the forward and backward algorithms.  \n",
    "- We define another probability $\\xi_{ij}(t)$ of landing in state $s_i$ at time $t$ and transitioning to state $s_j$ at time $t+1$ regardless of the previous states and future states given the observations.     \n",
    "- These probabilities are used to compute the expected sufficient statistics\n",
    "    - the expected number of times each state is visited\n",
    "    - the expected number of times each transition is made, given the observation sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Expectation maximization** \n",
    "\n",
    "- We will start with a randomly initialized model. \n",
    "- We use the model to calculate new $\\alpha_i(t), \\beta_i(t), \\gamma_i(t), \\xi_{ij}(t)$. \n",
    "- We update the model.  \n",
    "- We can do this iteratively until convergence or stopping condition. \n",
    "\n",
    "![](img/em.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/em.png\" height=\"700\" width=\"700\">        -->\n",
    "<!-- </center>    -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [AppendixB](AppendixB-BaumWelch.ipynb) for more details on Baum Welch algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Unsupervised learning of our toy problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we know how to do decoding and unsupervised learning of HMMs in Python, let's learn about how do they work. Let's try it out with `hmmlearn` before learning about the details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(42)\n",
    "n = 200\n",
    "t = 4000\n",
    "X, Z = model.sample(t)\n",
    "\n",
    "range_min=10\n",
    "range_max=30\n",
    "seqlens = [random.randint(range_min, range_max) for i in range(n)]\n",
    "seqlens[-1] += t - sum(seqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [2],\n",
       "       [3],\n",
       "       ...,\n",
       "       [2],\n",
       "       [1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's train an unsupervised HMM on these sampled sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CategoricalHMM(n_components=2, n_features=4,\n",
       "               random_state=RandomState(MT19937) at 0x14ACDFD40)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;CategoricalHMM<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>CategoricalHMM(n_components=2, n_features=4,\n",
       "               random_state=RandomState(MT19937) at 0x14ACDFD40)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "CategoricalHMM(n_components=2, n_features=4,\n",
       "               random_state=RandomState(MT19937) at 0x14ACDFD40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_states = 2\n",
    "unsup_model = hmm.CategoricalHMM(n_components=n_states, random_state=42)\n",
    "unsup_model.fit(X, seqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:\n",
      "['s0', 's1']\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"111pt\" height=\"133pt\"\n",
       " viewBox=\"0.00 0.00 111.22 132.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 128.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-128.5 107.22,-128.5 107.22,4 -4,4\"/>\n",
       "<!-- s0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>s0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27.47\" cy=\"-106.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27.47\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">s0</text>\n",
       "</g>\n",
       "<!-- s0&#45;&gt;s0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>s0&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52,-114.67C62.97,-115.45 72.47,-112.72 72.47,-106.5 72.47,-102.61 68.76,-100.09 63.3,-98.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.68,-95.45 53.51,-98.41 63.31,-102.44 63.68,-95.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"87.84\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">0.903</text>\n",
       "</g>\n",
       "<!-- s1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>s1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27.47\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27.47\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">s1</text>\n",
       "</g>\n",
       "<!-- s0&#45;&gt;s1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>s0&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M13.33,-90.68C8.69,-84.85 4.11,-77.81 1.72,-70.5 -0.57,-63.53 -0.57,-60.97 1.72,-54 2.95,-50.23 4.77,-46.53 6.87,-43.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"9.59,-45.26 12.46,-35.06 3.85,-41.24 9.59,-45.26\"/>\n",
       "<text text-anchor=\"middle\" x=\"17.84\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">0.097</text>\n",
       "</g>\n",
       "<!-- s1&#45;&gt;s0 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>s1&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M30.39,-36.04C31.24,-41.73 32.05,-48.12 32.47,-54 32.99,-61.31 32.99,-63.19 32.47,-70.5 32.31,-72.7 32.1,-74.98 31.85,-77.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"28.41,-76.59 30.59,-86.96 35.35,-77.5 28.41,-76.59\"/>\n",
       "<text text-anchor=\"middle\" x=\"47.84\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">0.999</text>\n",
       "</g>\n",
       "<!-- s1&#45;&gt;s1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>s1&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52,-26.17C62.97,-26.95 72.47,-24.22 72.47,-18 72.47,-14.11 68.76,-11.59 63.3,-10.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.68,-6.95 53.51,-9.91 63.31,-13.94 63.68,-6.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"87.84\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">0.001</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x148fddf40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_hmm(unsup_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53718145, 0.46281855])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsup_model.startprob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.03358928e-01, 9.66410721e-02],\n",
       "       [9.99095951e-01, 9.04049078e-04]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsup_model.transmat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.447484  , 0.15438547, 0.31955587, 0.07857466],\n",
       "       [0.5069931 , 0.12223317, 0.24067389, 0.13009984]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsup_model.emissionprob_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare it with our toy HMM\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"600\" width=\"600\">  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The model doesn't look very close to the real model from which we have sampled the sequences. \n",
    "- But it's unsupervised and with more data the probabilities would probably make more sense. \n",
    "- Also, note that it's an unsupervised model and it doesn't give you interpretation of the states. You have to do it on your own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final comments and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Hidden Markov Models (HMMs) provide a probabilistic framework to model sequences. \n",
    "- They are much more practical compared to Markov models and are widely used. \n",
    "- Speech recognition is a success story for HMMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Three fundamental questions for HMMs\n",
    "\n",
    "**Likelihood**: Given a model with parameters $\\theta = <\\pi, A, B>$, how do we efficiently compute the likelihood of a particular observation sequence $O$?\n",
    "\n",
    "**Decoding**\n",
    "Given an observation sequence $O$ and a model $\\theta$ how do we choose a state sequence $Q={q_0, q_1, \\dots q_T}$ that best explains the observation sequence?\n",
    "\n",
    "**Learning**\n",
    "Given a large observation sequence $O$ how do we choose the best parameters $\\theta$ that explain the data $O$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important ideas to know\n",
    "\n",
    "- The definition of an HMM\n",
    "- The conditional independence assumptions of an HMM\n",
    "- The purpose of the forward algorithm and the backward algorithm.\n",
    "    - How to compute $\\alpha_i(t)$ and $\\beta_i(t)$\n",
    "- The purpose of the Viterbi algorithm.\n",
    "    - How to compute $\\delta_i(t)$ and $\\psi_i(t)$\n",
    "- The purpose of the Baum-Welch algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuous HMMs\n",
    "\n",
    "- If the observations are drawn from a continuous space (e.g., speech), the probabilities must be continuous as well. \n",
    "- HMMs generalize to continuous probability distributions. \n",
    "- In the lab your observations are mfcc feature vectors for time frames which are continuous observations. \n",
    "- In `hmmlearn` you can use `GaussianHMM` or `GMMHMM` for continuous observations. \n",
    "\n",
    "![](img/continuous_hmms.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/continuous_hmms.png\" height=\"400\" width=\"400\">        -->\n",
    "<!-- </center>    -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important ideas to know \n",
    "\n",
    "Using `hmmlearn` \n",
    "- For unsupervised training of HMMs. \n",
    "- For likelihood (`model.score`)    \n",
    "- For decoding (`model.decode`)\n",
    "- For discrete observations (`MultinomialHMM`)\n",
    "- For continuous observations (`GaussianHMM` or `GMMHMM`)\n",
    "- For sequences with varying lengths.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some useful resources and links \n",
    "- [Frank Rudzicz's slides on HMM](http://www.cs.toronto.edu/~frank/csc401/lectures2020/5_HMMs.pdf) \n",
    "- [Andrew McCallum's slides on HMM](https://people.cs.umass.edu/~mccallum/courses/inlp2004a/lect10-hmm2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â“â“ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Exercise 4.2: More practice questions\n",
    "\n",
    "Discuss the following questions with your neighbour. \n",
    "\n",
    "Consider the sentence below:\n",
    "<blockquote>\n",
    "    Will the chair chair the meeting from this chair ?\n",
    "</blockquote>\n",
    "\n",
    "and a simple part-of-speech tagset: \n",
    "<blockquote>\n",
    "{noun, verb, determiner, preposition, punctuation}\n",
    "</blockquote>    \n",
    "\n",
    "The table below shows the possible assignments for words and part-of-speech tags. The symbol `x` denotes that the word and part-of-speech tag combination is possible. For instance, the word _chair_ is unlikely to be used as a determiner and so we do not have an `x` there. \n",
    "\n",
    "|    <i></i>    | Will    | the     | chair   | chair   | the     | meeting  | from    | this    | chair   | ?       |\n",
    "| ------------- | :-----: | :-----: | :-----: | :-----: | :----:  | :------: | :-----: | :-----: | :-----: | :----:  |\n",
    "| noun          | x       | x       |  x      | x       | x       | x        | <i></i> | <i></i> | x       | <i></i> |\n",
    "| verb          | x       | <i></i> |  x      | x       | <i></i> | x        | <i></i> | <i></i> | x       | <i></i> |\n",
    "| determiner    | <i></i> | x       | <i></i> | <i></i> | x       | <i></i>  | <i></i> | x       | <i></i> | <i></i> |\n",
    "| preposition   | <i></i> | <i></i> | <i></i> | <i></i> | <i></i> | <i></i>  | x       | <i></i> | <i></i> | <i></i> |\n",
    "| punctuation   | <i></i> | <i></i> | <i></i> | <i></i> | <i></i> | <i></i>  | <i></i> | <i></i> | <i></i> | x       |\n",
    "\n",
    "\n",
    "Given this information, answer the following questions: \n",
    "- (A) With this simple tagset of part-of-speech tags, how many possible part-of-speech tag sequences (i.e, hidden state sequences) are there for the given sentence (observation sequence)?\n",
    "- (B) Restricting to the possibilities shown above with `x`, how many possible part-of-speech tag sequences are there?\n",
    "- (C) Given an HMM with states as part-of-speech tags and observations as words, one way to decode the observation sequence is using the brute force method below. What is the time complexity of this method in terms of the number of states ($n$) and the length of the output sequence ($T$)? You may ignore constants.    \n",
    "    - enumerate all possible hidden state sequences (i.e., enumerate all solutions)\n",
    "    - for each hidden state sequence, calculate the probability of the observation sequence given the hidden state sequence (i.e., score each solution)\n",
    "    - pick the hidden state sequence which gives the highest probability for the observation sequence (i.e., pick the best solution)    \n",
    "\n",
    "- (D) If you decode the sequence using the Viterbi algorithm instead, what will be the time complexity in terms of the number of states ($n$) and the length of the output sequence ($T$)? You may ignore constants.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 4.2: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "- (A) 9.7 million! \n",
    "- (B) 128\n",
    "- (C) The time complexity of this approach is $\\mathcal{O}(N^T)$. \n",
    "- (D) $\\mathcal{O}(N^2T)$\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
