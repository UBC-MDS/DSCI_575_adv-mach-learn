
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Lecture 5: Introduction to Recurrent Neural Networks (RNNs) &#8212; DSCI 575 Advanced Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/notes/05_intro-to-RNNs';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="icon" href="../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Lecture 6: Introduction to self-attention and transformers" href="06_intro-to-transformers.html" />
    <link rel="prev" title="Lecture 4: More HMMs" href="04_More-HMMs.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/mds-hex-sticker.png" class="logo__image only-light" alt="DSCI 575 Advanced Machine Learning - Home"/>
    <script>document.write(`<img src="../../_static/mds-hex-sticker.png" class="logo__image only-dark" alt="DSCI 575 Advanced Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting ready</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="00_course-information.html">Course Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_Markov-models.html">Lecture 1: Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_LMs-text-preprocessing.html">Lecture 2: Applications of Markov Models and Text Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_HMMs-intro.html">Lecture 3: Introduction to Hidden Markov Models (HMMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_More-HMMs.html">Lecture 4: More HMMs</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Lecture 5: Introduction to Recurrent Neural Networks (RNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_intro-to-transformers.html">Lecture 6: Introduction to self-attention and transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_more-transformers.html">Lecture 7: More transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_llms-applications.html">Lecture 8: Applications of Large Language Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Class demos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../demos/transformers-recipe-generation.html">Recipe Generation using Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="AppendixA-BaumWelch.html">Baum-Welch (BW) algorithm</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Attribution</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../attribution.html">Attributions</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/UBC-MDS/DSCI_575_adv-mach-learn" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/notes/05_intro-to-RNNs.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture 5: Introduction to Recurrent Neural Networks (RNNs)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-plan-imports-lo">Lecture plan, imports, LO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-your-mood-today">What’s your mood today?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">1. Motivation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activity-feedforward-neural-networks-for-sentiment-analysis">1.1 Activity: Feedforward neural networks for sentiment analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-introduction">1.2 RNNs introduction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-details">2. RNN details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-presentations">2.1 RNN presentations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-sharing">2.2 Parameter sharing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass-in-rnns">3. Forward pass in RNNs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-new-state-h-t">3.1 Computing the new state <span class="math notranslate nohighlight">\(h_t\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-output-y-t">3.2 Computing the output <span class="math notranslate nohighlight">\(y_t\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-forward-pass-with-pytorch">3.3 RNN Forward pass with PyTorch</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-5-1-select-all-of-the-following-statements-which-are-true-iclicker">Exercise 5.1: Select all of the following statements which are <strong>True</strong> (iClicker)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-rnns">4. Training RNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-applications">5. RNN applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequence-labeling">5.1 Sequence labeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequence-classification">5.2 Sequence classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-generation">5.3 Text generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-captioning">5.4 Image captioning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stacked-and-bidirectional-rnn-architectures">6. Stacked and Bidirectional RNN architectures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stacked-rnns">6.1 Stacked RNNs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bidirectional-rnns">6.2 Bidirectional RNNs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-comments-and-summary">Final comments and summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-ideas-to-know">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coming-up">Coming up …</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resources">Resources</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><img alt="" src="../../_images/575_banner.png" /></p>
<section class="tex2jax_ignore mathjax_ignore" id="lecture-5-introduction-to-recurrent-neural-networks-rnns">
<h1>Lecture 5: Introduction to Recurrent Neural Networks (RNNs)<a class="headerlink" href="#lecture-5-introduction-to-recurrent-neural-networks-rnns" title="Link to this heading">#</a></h1>
<p>UBC Master of Data Science program, 2024-25</p>
<section id="lecture-plan-imports-lo">
<h2>Lecture plan, imports, LO<a class="headerlink" href="#lecture-plan-imports-lo" title="Link to this heading">#</a></h2>
<p><br><br></p>
</section>
<section id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">IPython</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span><span class="p">,</span> <span class="n">display</span>
</pre></div>
</div>
</div>
</div>
<p><br><br></p>
<section id="learning-outcomes">
<h3>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<p>From this lecture you will be able to</p>
<ul class="simple">
<li><p>Explain the motivation to use RNNs.</p></li>
<li><p>Explain how an RNN differs from a feed-forward neural network.</p></li>
<li><p>Explain three weight matrices in RNNs.</p></li>
<li><p>Explain parameter sharing in RNNs.</p></li>
<li><p>Explain how states and outputs are calculated in the forward pass of an RNN.</p></li>
<li><p>Explain the backward pass in RNNs at a high level.</p></li>
<li><p>Specify different architectures of RNNs and explain how these architectures are used in the context of NLP applications.</p></li>
<li><p>Broadly explain character-level text generation with RNNs.</p></li>
<li><p>Specify the shapes of weight matrices in RNNs.</p></li>
<li><p>Carry out forward pass with RNNs in <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code>.</p></li>
<li><p>Explain stacked RNNs and bidirectional RNNs and the difference between the two.</p></li>
</ul>
<p><br><br></p>
</section>
</section>
<section id="questions-for-you">
<h2>❓❓ Questions for you<a class="headerlink" href="#questions-for-you" title="Link to this heading">#</a></h2>
<section id="what-s-your-mood-today">
<h3>What’s your mood today?<a class="headerlink" href="#what-s-your-mood-today" title="Link to this heading">#</a></h3>
<p>Think about how you’re feeling right now. Pick one:</p>
<ul class="simple">
<li><p>(A) Happy 😊</p></li>
<li><p>(B) Sad 😞</p></li>
<li><p>(C) Not so simple. It’s a mix (e.g., maybe a bit happy, kind of excited, but also tired 😅)</p></li>
</ul>
<p><br><br><br><br></p>
<p>In HMMs, we were assuming discrete states (e.g., discrete set of moods).</p>
<p><img alt="" src="../../_images/HMM_example_small.png" /></p>
<p>But real human moods, like real-world states, are rarely that simple.</p>
<p>Today we’ll start exploring models that don’t force us to pick a single simple state, but instead allow us to represent subtle, continuous, and evolving internal states, much like how you actually feel when juggling multiple things.</p>
<p><br><br><br><br></p>
</section>
</section>
<section id="motivation">
<h2>1. Motivation<a class="headerlink" href="#motivation" title="Link to this heading">#</a></h2>
<p><strong>RNN-generated music!</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=dMhQalLBXIU">Magenta PerformanceRNN</a></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### An example of a state-of-the-art language model</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://www.youtube.com/embed/dMhQalLBXIU&quot;</span>
<span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">IFrame</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="500"
            height="300"
            src="https://www.youtube.com/embed/dMhQalLBXIU"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
<ul class="simple">
<li><p>Language is an inherently sequential phenomenon.</p></li>
<li><p>In lab2, you identified phonemes associated with a sequence of sound waves.</p></li>
<li><p>This temporal nature of language is reflected in the metaphors used to describe language</p>
<ul>
<li><p><em>flow of conversation</em>, <em>news feeds</em>, or <em>twitter streams</em></p></li>
</ul>
</li>
</ul>
<p>Beyond language, there are many examples of sequences in the wild.</p>
<ul class="simple">
<li><p>Financial markets</p></li>
<li><p>Medical signals such as ECGs</p></li>
<li><p>Biological sequences encoded in DNA</p></li>
<li><p>Patterns of climate and patterns of motion</p></li>
</ul>
<p><img alt="" src="../../_images/seqs-in-the-wild.png" /></p>
<p><a class="reference external" href="https://www.youtube.com/watch?v=ySEx_Bqxvvo">Source</a></p>
<p>Recall that in this course, our focus is on models for sequential data.</p>
<p>What properties do we want when we model sequences?</p>
<ul class="simple">
<li><p>[ ] Order matters</p></li>
<li><p>[ ] Variable sequence lengths</p></li>
<li><p>[ ] Capture long distance dependencies</p></li>
</ul>
<p>So far we have seen two models to process sequential data</p>
<ul class="simple">
<li><p>Markov models</p></li>
<li><p>And more flexible hidden Markov models</p></li>
</ul>
<p>How do these models perform on the above criteria?</p>
<p><br><br><br><br><br><br></p>
<ul class="simple">
<li><p>Recall the language modeling task, the task of predicting the next word given a sequence.</p></li>
<li><p>What’s the probability estimate of an upcoming word?</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(P(w_t|w_1,w_2,\dots,w_{t-1})\)</span></p></li>
</ul>
</li>
<li><p>When we used Markov models for this task, we made Markov assumption.</p>
<ul>
<li><p>Markov model: <span class="math notranslate nohighlight">\(P(w_t|w_1,w_2,\dots,w_{t-1}) \approx P(w_t|w_{t-1})\)</span></p></li>
<li><p>Markov model with more context: <span class="math notranslate nohighlight">\(P(w_t|w_1,w_2,\dots,w_{t-1}) \approx P(w_t|w_{t-2}, w_{t-1})\)</span></p></li>
</ul>
</li>
</ul>
<p>In lab 1, you generated text with Markov models which captured some temporal aspect when generating text.</p>
<p>With <span class="math notranslate nohighlight">\(n=1\)</span>, we considered one previous word as context when generating text.</p>
<p><img alt="" src="../../_images/bigram-ex_small.png" />
<br><br></p>
<p>With <span class="math notranslate nohighlight">\(n=2\)</span>, we considered two previous words as context when generating text.</p>
<p><img alt="" src="../../_images/trigram-ex_small.png" /></p>
<p><br><br></p>
<p>With <span class="math notranslate nohighlight">\(n=3\)</span>, we considered three previous words as context when generating text.</p>
<p><img alt="" src="../../_images/4-gram-ex_small.png" /></p>
<p>In 2006, Google <a class="reference external" href="https://research.google/blog/all-our-n-gram-are-belong-to-you/">released 5-grams</a> extracted from the internet.</p>
<blockquote>
<div><p>We processed 1,024,908,267,229 words of running text and are publishing the counts for all <strong>1,176,470,663 five-word sequences that appear at least 40 times</strong>. There are 13,588,391 unique words, after discarding words that appear less than 200 times.</p>
</div></blockquote>
<p>Imagine how big the transition matrix will be for 1,176,470,663 states!</p>
<ul class="simple">
<li><p>Markov models do not have memory beyond the previous 2, 3 or maximum <span class="math notranslate nohighlight">\(n\)</span> steps and when <span class="math notranslate nohighlight">\(n\)</span> becomes larger, there is sparsity problem.</p></li>
<li><p>Also, they have huge RAM requirements because you have to store all ngrams.</p></li>
<li><p>Overall, modeling of probabilities of sequences with Markov models doesn’t scale well.</p></li>
</ul>
<p>Would a Markov model able to predict a reasonable next word in the sequence below?</p>
<p><strong>I am studying data science at the University of British Columbia in Vancouver because I want to build a career in ___.</strong></p>
<p><br><br><br><br></p>
<p>Here are some completions given by ChatGPT for the same sequence.</p>
<blockquote>
<div><p>Provide four completions for the sequence
I am studying data science at the University of British Columbia in Vancouver because I want to build a career in</p>
</div></blockquote>
<ul class="simple">
<li><p><em>analytics and machine learning</em></p></li>
<li><p><em>health care analytics and research</em></p></li>
<li><p><em>environmental data analysis and climate change research</em></p></li>
<li><p><em>social media analytics and public opinion research</em></p></li>
</ul>
<p>How do large language models such as ChatGPT make such good predictions about next words?</p>
<ul class="simple">
<li><p>With neural architectures</p></li>
</ul>
<p>In the remaining lectures in this course, we will focus on <strong>neural sequence modeling</strong>.</p>
<p><br><br></p>
<section id="activity-feedforward-neural-networks-for-sentiment-analysis">
<h3>1.1 Activity: Feedforward neural networks for sentiment analysis<a class="headerlink" href="#activity-feedforward-neural-networks-for-sentiment-analysis" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Suppose you are performing sentiment analysis on movie reviews. Your goal is to predict whether a given movie review expresses a positive (👍) or negative (👎) sentiment. You are considering a feedforward neural network for this task.</p></li>
<li><p>Consider the following review:</p></li>
</ul>
<blockquote>
<div><p><strong>This movie was not boring at all. It was fascinating and thrilling from start to finish.</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>How would you encode the input features?</p></li>
<li><p>What would be the network architecture?</p></li>
<li><p>Reflect on the limitations of using a feedforward neural network for this task. What aspects of language might it struggle to capture?</p></li>
</ul>
<p><img alt="" src="../../_images/nn-7.png" /></p>
<!-- <img src="img/feedforwardNN.png" height="400" width="400">  -->
<p><br><br><br><br></p>
<p>It’s possible to use feedforward neural networks for sequence processing but they are not inherently designed to handle sequences because they lack ability to capture temporal dependencies.</p>
<ul class="simple">
<li><p>Reminder: In feed-forward neural networks,</p>
<ul>
<li><p>all connections flow forward (no loops)</p></li>
<li><p>each layer of hidden units is fully connected to the next</p></li>
</ul>
</li>
<li><p>We pass fixed sized vector representation of text (e.g., representation created with <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code>) as input.</p></li>
<li><p>We lose the temporal aspect of text in this representation.</p></li>
<li><p>Let’s simplify the presentation of the feed-forward network above.</p></li>
</ul>
<p><img alt="" src="../../_images/simplified_ffnn.png" /></p>
<p><br><br></p>
</section>
<section id="rnns-introduction">
<h3>1.2 RNNs introduction<a class="headerlink" href="#rnns-introduction" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>RNNs are a kind of neural network model which use hidden units to retain information over time.</strong></p></li>
<li><p>RNNs can help us with the limited memory problem of Markov models.</p></li>
<li><p>Unlike Markov models, this approach does not impose a fixed-length limit on this prior context; the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence.</p></li>
</ul>
<p><strong>RNN Activity</strong></p>
<p>RNN is like your brain reading a sentence word by word.</p>
<ul class="simple">
<li><p><strong>Input at each time step</strong>: The current word you read</p></li>
<li><p><strong>Hidden state</strong>: Your current mental understanding</p></li>
<li><p><strong>Output</strong>: Your interpretation, reaction, or prediction at that point in time</p></li>
</ul>
<p>Two rows of students:</p>
<ul class="simple">
<li><p>Front row = input layer (observations at each time step)</p></li>
<li><p>Back row = hidden state at each time step</p></li>
<li><p>Each column is a time step (0 through 4)</p></li>
<li><p>So we’ll have 4 front-row students: <span class="math notranslate nohighlight">\(x_0\)</span> to <span class="math notranslate nohighlight">\(x_3\)</span></p></li>
<li><p>And 4 back-row students: <span class="math notranslate nohighlight">\(h_0\)</span> to <span class="math notranslate nohighlight">\(h_3\)</span></p></li>
</ul>
<ol class="arabic simple">
<li><p>At time step 0:</p>
<ul class="simple">
<li><p>Front-row student <span class="math notranslate nohighlight">\(x_0\)</span> gets a word</p></li>
<li><p>They pass it to the back-row student behind them (<span class="math notranslate nohighlight">\(h_0\)</span>).</p></li>
</ul>
</li>
<li><p>At time step 1 (and beyond):</p>
<ul class="simple">
<li><p>The front-row student (e.g., <span class="math notranslate nohighlight">\(x_1\)</span>) gets a new word</p></li>
<li><p>The back-row student (e.g., <span class="math notranslate nohighlight">\(h_1\)</span>) receives:</p>
<ul>
<li><p>The current input from the front-row student (e.g., <span class="math notranslate nohighlight">\(x_1\)</span>)</p></li>
<li><p>Whatever “memory” is passed from the previous hidden state (e.g., <span class="math notranslate nohighlight">\(h_0\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(h_1\)</span> combines this (e.g., by writing a summary phrase or combining keywords).</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Repeat until time step 3 or 4.</p></li>
<li><p>Final time step: <span class="math notranslate nohighlight">\(h_3\)</span> summarizes what they remember (e.g., predicts next word, gives the “mood” of the sentence, etc.)</p></li>
</ol>
<p><br><br><br><br></p>
<ul class="simple">
<li><p>How can a temporal dimension be added to a feedforward neural network?</p></li>
<li><p>For word representation with a vector of size 4, a single feedforward neural network can be used for prediction.</p></li>
<li><p>For 2 words, two separate feedforward neural networks can be used together.</p></li>
<li><p>For 3 words, three separate feedforward neural networks can be used together.</p></li>
</ul>
<p><img alt="" src="../../_images/RNN-intro_small.png" /></p>
<!-- <img src="img/RNN-intro.png" height="400" width="400">  -->
<p>(Credit: <a class="reference external" href="http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf">Stanford CS224d slides</a>)</p>
<p>How to connect multiple feedforward networks?</p>
<ul class="simple">
<li><p><strong>Make connections between hidden layers</strong>.</p></li>
<li><p>The network typically consists of input, hidden layer, and output. The hidden layer is connected to itself for recurrent connections.</p></li>
<li><p>Sequences can be processed by presenting one element at a time to the network.</p></li>
</ul>
<p><br><br><br><br></p>
</section>
</section>
<section id="rnn-details">
<h2>2. RNN details<a class="headerlink" href="#rnn-details" title="Link to this heading">#</a></h2>
<section id="rnn-presentations">
<h3>2.1 RNN presentations<a class="headerlink" href="#rnn-presentations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Unrolled presentation</p></li>
</ul>
<p><img alt="" src="../../_images/RNN-intro_small.png" /></p>
<!-- <img src="img/RNN-intro.png" height="400" width="400">  -->
<ul class="simple">
<li><p>Recursive presentation</p></li>
</ul>
<p><img alt="" src="../../_images/RNN_recursive_2.png" /></p>
<!-- <img src="img/RNN_recursive_2.png" height="200" width="200">  -->
<!-- <center> -->
<!-- <img src="img/RNN_recursive_2.png" height="300" width="300">  -->
<!-- </center>      --><ul class="simple">
<li><p>The key distinction between non-recurrent and recurrent architectures is <strong>the inclusion of a new set of weights connecting the previous hidden layer to the current hidden layer</strong>.</p></li>
<li><p>The hidden layer from the previous time step acts as a form of “memory” that influences decisions made at later time steps.</p></li>
<li><p>These weights determine how the network incorporates the previous context when computing output for the current input.</p></li>
</ul>
<p><strong>RNN as a graphical model</strong></p>
<ul class="simple">
<li><p>RNNs can be visualized as a graphical model. The states below are the hidden layers in each time step.</p>
<ul>
<li><p>Somewhat similar to hidden Markov models (HMMs)</p></li>
<li><p>But a hidden state in an RNN is continuous valued, high dimensional, and much richer.</p></li>
</ul>
</li>
<li><p>Each state is a function of the previous state and the input.</p></li>
<li><p>A state contains information about the whole past sequence.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(h_t = g(x_t, x_{t-1}, \dots, x_2, x_1)\)</span></p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../../_images/RNN-dynamic-model_small.png" /></p>
<!-- <img src="img/RNN-dynamic-model.png" height="400" width="400">  --><ul class="simple">
<li><p>Adding a temporal dimension and the recursion make RNNs appear to be complex. But they are not all that different from standard feedforrward neural networks.</p></li>
<li><p>Given an input vector and the values for the hidden layer from the previous time step we are still performing standard feedforward calculations.</p></li>
<li><p>The most significant change lies in the new set of weights <span class="math notranslate nohighlight">\(U\)</span> that connect the hidden layer from the previous time step to the current hidden layer.</p></li>
<li><p>As with the other weights in the network, these connections are trained via a variant of backpropagation.</p></li>
</ul>
<p><img alt="" src="../../_images/RNN-as-FFNN_small.png" /></p>
<!-- <img src="img/RNN-as-FFNN.png" height="500" width="500">  -->
<p><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Source</a></p>
</section>
<section id="parameter-sharing">
<h3>2.2 Parameter sharing<a class="headerlink" href="#parameter-sharing" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What are the parameters of this model? There are three weight matrices.</p>
<ul>
<li><p>Input to hidden weight matrix: <span class="math notranslate nohighlight">\(W\)</span></p></li>
<li><p>Hidden to output weight matrix: <span class="math notranslate nohighlight">\(V\)</span></p></li>
<li><p>Hidden to hidden weight matrix: <span class="math notranslate nohighlight">\(U\)</span></p></li>
</ul>
</li>
<li><p>The key point in RNNs: <strong>All weights between time steps are shared.</strong></p>
<ul>
<li><p>This allows the model to learn patterns that are independent of their position in the sequence</p></li>
</ul>
</li>
</ul>
<p><strong>Dimensionality of different weight matrices</strong>
Lets consider an example:</p>
<ul class="simple">
<li><p>Suppose input vector <span class="math notranslate nohighlight">\(x_t\)</span> is of size 300 (i.e., <span class="math notranslate nohighlight">\(x_t \in \mathbb{R}^{300}\)</span>)</p></li>
<li><p>Suppose the hidden state vector is of size 100 (memory of the network) (i.e., <span class="math notranslate nohighlight">\(h_t \in \mathbb{R}^{100}\)</span>)</p></li>
<li><p>Suppose the output vector <span class="math notranslate nohighlight">\(y_t\)</span> is of size 60 (i.e., <span class="math notranslate nohighlight">\(y_t \in \mathbb{R}^{60}\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{100 \times 300}\)</span>, <span class="math notranslate nohighlight">\(V_{60\times 100}\)</span>, <span class="math notranslate nohighlight">\(U_{100\times 100}\)</span></p></li>
</ul>
<p><img alt="" src="../../_images/RNN-as-FFNN_small.png" /></p>
<!-- <img src="img/RNN-as-FFNN.png" height="500" width="500">  -->
<p><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Source</a></p>
<ul class="simple">
<li><p>Input size: Suppose <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{d_{in}}\)</span></p></li>
<li><p>Output size: Suppose <span class="math notranslate nohighlight">\(y \in \mathbb{R}^{d_{out}}\)</span></p></li>
<li><p>Hidden size: Suppose <span class="math notranslate nohighlight">\(h \in \mathbb{R}^{d_h}\)</span></p></li>
<li><p>Three kinds of weights: <span class="math notranslate nohighlight">\(W_{d_{h}\times d_{in}}\)</span>, <span class="math notranslate nohighlight">\(V_{d_{out}\times d_{h}}\)</span>, <span class="math notranslate nohighlight">\(U_{d_h\times d_h}\)</span></p></li>
</ul>
<blockquote>
<div><p>You may see transpose of these matrices in some notations.</p>
</div></blockquote>
<p><br><br><br><br></p>
</section>
</section>
<section id="forward-pass-in-rnns">
<h2>3. Forward pass in RNNs<a class="headerlink" href="#forward-pass-in-rnns" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The forward inference in RNNs is very similar to what you have seen with feedforward networks.</p></li>
<li><p>Given an input <span class="math notranslate nohighlight">\(x_t\)</span> at timestep <span class="math notranslate nohighlight">\(t\)</span>, how do we compute the new hidden state <span class="math notranslate nohighlight">\(h_{t}\)</span> and output <span class="math notranslate nohighlight">\(y_t\)</span>?</p></li>
</ul>
<p><img alt="" src="../../_images/RNN-dynamic-model_small.png" /></p>
<!-- <img src="img/RNN-dynamic-model.png" height="400" width="400">  --><section id="computing-the-new-state-h-t">
<h3>3.1 Computing the new state <span class="math notranslate nohighlight">\(h_t\)</span><a class="headerlink" href="#computing-the-new-state-h-t" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Multiply the input <span class="math notranslate nohighlight">\(x_t\)</span> with the weight matrix between input and hidden layer (<span class="math notranslate nohighlight">\(W\)</span>) and the state or the hidden layer from the previous time step <span class="math notranslate nohighlight">\(h_{t-1}\)</span> with the weight matrix between hidden layers (<span class="math notranslate nohighlight">\(U\)</span>).</p></li>
<li><p>Add these values together.</p></li>
<li><p>Add the bias vector and pass the result through a suitable activation function <span class="math notranslate nohighlight">\(g\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
h_t = g(U_{d_h \times d_h}(h_{t-1})_{d_h \times 1} + W_{d_{h} \times d_{in}} (x_t)_{d_{in} \times 1}  + b_1)\\
\end{split}\]</div>
<!-- <center> -->
<!-- <img src="img/RNN_dynamic_model.png" height="800" width="800">  -->
<!-- </center>     -->
<p><br><br></p>
</section>
<section id="computing-the-output-y-t">
<h3>3.2 Computing the output <span class="math notranslate nohighlight">\(y_t\)</span><a class="headerlink" href="#computing-the-output-y-t" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Once we have the value for the new state <span class="math notranslate nohighlight">\(h_t\)</span>, we can calculate the output vector <span class="math notranslate nohighlight">\(y_t\)</span> by multiplying <span class="math notranslate nohighlight">\(h_t\)</span> with the weight matrix <span class="math notranslate nohighlight">\(V\)</span> between the hidden layer and the output layer, adding the bias vector, and applying an appropriate activation function <span class="math notranslate nohighlight">\(f\)</span> the multiplication.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y_t = f(V_{d_{out} \times d_{h}} (h_t){_{d_h \times 1}} + b_2)
\]</div>
<!-- <img src="img/RNN-dynamic-model.png" height="400" width="400">  -->
<ul class="simple">
<li><p>Typically, we are interested in soft classification. So computing <span class="math notranslate nohighlight">\(y_t\)</span> involves a softmax computation which provides a probability distribution over the possible output classes.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y_t = \text{softmax}(Vh_t + b_2)
\]</div>
<p><br><br></p>
<p><strong>Summary</strong></p>
<p>So in the forward pass we compute the new state <span class="math notranslate nohighlight">\(h_t\)</span> and the output <span class="math notranslate nohighlight">\(y_t\)</span> for all time steps in a sequence, as shown below.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
h_t = g(Uh_{t-1} + Wx_t + b_1)\\
y_t = \text{softmax}(Vh_t + b_2)
\end{split}\]</div>
<p><img alt="" src="../../_images/RNN-dynamic-model_small.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN-dynamic-model.png" height="400" width="400">  -->
<!-- </center>     --><p><strong>Forward pass pseudo code</strong></p>
<p>We compute this for the full sequence.</p>
<ul class="simple">
<li><p>Given: <span class="math notranslate nohighlight">\(x\)</span>, network</p></li>
<li><p>Intialize <span class="math notranslate nohighlight">\(h_0\)</span></p></li>
<li><p>for <span class="math notranslate nohighlight">\(t\)</span> in 1 to length(input sequence <span class="math notranslate nohighlight">\(x\)</span>)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(h_t = g(Uh_{t-1} + Wx_t + b_1\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(y_t = \text{softmax}(Vh_t + b_2)\)</span></p></li>
</ul>
</li>
</ul>
<p>Note that the matrices <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(V\)</span> and <span class="math notranslate nohighlight">\(W\)</span> are <strong>shared across time</strong> and new values for <span class="math notranslate nohighlight">\(h_t\)</span> and <span class="math notranslate nohighlight">\(y_t\)</span> are calculated at each time step.</p>
<p><br><br></p>
</section>
<section id="rnn-forward-pass-with-pytorch">
<h3>3.3 RNN Forward pass with PyTorch<a class="headerlink" href="#rnn-forward-pass-with-pytorch" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>See the documentation <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.RNN.html">here</a>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">RNN</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ImportError</span><span class="g g-Whitespace">                               </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">torch</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">RNN</span>

<span class="n">File</span> <span class="o">~/</span><span class="n">miniforge3</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">jbook</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.12</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">torch</span><span class="o">/</span><span class="fm">__init__</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">367</span>
<span class="g g-Whitespace">    </span><span class="mi">365</span>     <span class="k">if</span> <span class="n">USE_GLOBAL_DEPS</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">366</span>         <span class="n">_load_global_deps</span><span class="p">()</span>
<span class="ne">--&gt; </span><span class="mi">367</span>     <span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="o">*</span>  <span class="c1"># noqa: F403</span>
<span class="g g-Whitespace">    </span><span class="mi">370</span> <span class="k">class</span> <span class="nc">SymInt</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">371</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">372</span><span class="sd">     Like an int (including magic methods), but redirects all operations on the</span>
<span class="g g-Whitespace">    </span><span class="mi">373</span><span class="sd">     wrapped node. This is used in particular to symbolically record operations</span>
<span class="g g-Whitespace">    </span><span class="mi">374</span><span class="sd">     in the symbolic shape workflow.</span>
<span class="g g-Whitespace">    </span><span class="mi">375</span><span class="sd">     &quot;&quot;&quot;</span>

<span class="ne">ImportError</span>: dlopen(/Users/kvarada/miniforge3/envs/jbook/lib/python3.12/site-packages/torch/_C.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libgfortran.5.dylib
  <span class="n">Referenced</span> <span class="n">from</span><span class="p">:</span> <span class="o">&lt;</span><span class="mi">0</span><span class="n">B9C315B</span><span class="o">-</span><span class="n">A1DD</span><span class="o">-</span><span class="mi">3527</span><span class="o">-</span><span class="mi">88</span><span class="n">DB</span><span class="o">-</span><span class="mi">4</span><span class="n">B90531D343F</span><span class="o">&gt;</span> <span class="o">/</span><span class="n">Users</span><span class="o">/</span><span class="n">kvarada</span><span class="o">/</span><span class="n">miniforge3</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">jbook</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">libopenblas</span><span class="mf">.0</span><span class="o">.</span><span class="n">dylib</span>
  <span class="n">Reason</span><span class="p">:</span> <span class="n">tried</span><span class="p">:</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">duplicate</span> <span class="n">LC_RPATH</span> <span class="s1">&#39;@loader_path&#39;</span><span class="p">),</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">duplicate</span> <span class="n">LC_RPATH</span> <span class="s1">&#39;@loader_path&#39;</span><span class="p">),</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">duplicate</span> <span class="n">LC_RPATH</span> <span class="s1">&#39;@loader_path&#39;</span><span class="p">),</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">duplicate</span> <span class="n">LC_RPATH</span> <span class="s1">&#39;@loader_path&#39;</span><span class="p">),</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">duplicate</span> <span class="n">LC_RPATH</span> <span class="s1">&#39;@loader_path&#39;</span><span class="p">),</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">duplicate</span> <span class="n">LC_RPATH</span> <span class="s1">&#39;@loader_path&#39;</span><span class="p">),</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">duplicate</span> <span class="n">LC_RPATH</span> <span class="s1">&#39;@loader_path&#39;</span><span class="p">),</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">duplicate</span> <span class="n">LC_RPATH</span> <span class="s1">&#39;@loader_path&#39;</span><span class="p">),</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/lib/python3.12/site-packages/torch/lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/lib/python3.12/site-packages/torch/../../../libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">duplicate</span> <span class="n">LC_RPATH</span> <span class="s1">&#39;@loader_path&#39;</span><span class="p">),</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/lib/python3.12/site-packages/torch/lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/lib/python3.12/site-packages/torch/../../../libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">duplicate</span> <span class="n">LC_RPATH</span> <span class="s1">&#39;@loader_path&#39;</span><span class="p">),</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/bin/../lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">duplicate</span> <span class="n">LC_RPATH</span> <span class="s1">&#39;@loader_path&#39;</span><span class="p">),</span> <span class="s1">&#39;/Users/kvarada/miniforge3/envs/jbook/bin/../lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">duplicate</span> <span class="n">LC_RPATH</span> <span class="s1">&#39;@loader_path&#39;</span><span class="p">),</span> <span class="s1">&#39;/usr/local/lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/usr/lib/libgfortran.5.dylib&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">,</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dyld</span> <span class="n">cache</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Creating an RNN object</strong></p>
<p>We are creating an RNN with</p>
<ul class="simple">
<li><p>only one hidden layer</p></li>
<li><p>input of size 20 (e.g., imagine word vectors of size 20)</p></li>
<li><p>hidden layer of size 10</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># input size, hidden_size, number of layers</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Input</strong></p>
<ul class="simple">
<li><p>The input is going to be sequences (e.g., sequences of words)</p></li>
<li><p>We need to provide the sequence length of the sequence and the size of each input vector.</p></li>
<li><p>For example, suppose you have the following sequence and you are representing each word with a 20-dimensional word vector, then your sequence length is going to be 5 and input size is going to be 20.</p></li>
</ul>
<blockquote>
<div><p>Cherry blossoms are beautiful .</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>  <span class="c1"># sequence length, input size</span>
<span class="n">inp</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.7449,  0.1666,  0.3951, -0.8870, -0.6183, -0.9801,  1.3899,  0.1448,
         -0.5490,  1.0835,  1.6662,  0.7660, -0.2617,  0.9894,  1.1245,  1.1661,
         -0.4772, -1.8159,  0.6064, -0.3212],
        [ 1.2073,  0.3854,  0.9935,  0.7480, -0.1274, -0.1711,  0.2687,  0.8226,
          0.5294, -0.0513,  0.4434,  1.0118, -0.0085, -0.5724,  0.4837,  0.6713,
          0.1026, -0.2679,  0.9887,  0.2314],
        [-0.6493, -0.4759,  0.5472, -0.7455,  1.9354,  0.3501, -0.6683,  0.8641,
          0.7480,  1.7375, -0.3868,  0.0245, -1.0831,  2.0118,  1.5609,  0.4658,
          0.3362, -1.2545, -0.0939, -0.9401],
        [-1.7787, -0.2415, -0.3304,  0.2118, -0.4295,  0.5924, -0.3078,  1.2190,
          0.2171, -0.5540,  1.6961,  0.1096, -0.5444, -0.3318, -1.4029, -0.7076,
          0.6535, -1.7345, -0.0667, -0.0352],
        [ 0.9936,  0.3977,  0.5135,  1.2352,  1.8197, -0.6011, -1.2933,  1.5456,
         -0.6196,  0.4122,  0.5576,  0.3847, -0.0681, -1.2007,  1.2829, -1.7951,
         -1.3169,  0.6187,  0.5400, -1.7226]])
</pre></div>
</div>
</div>
</div>
<p><strong>Initial hidden state</strong></p>
<ul class="simple">
<li><p>At the 0th time step, we do not have anything to remember. So we initialize the hidden state randomly or to 0.0.</p></li>
<li><p>Let’s initialize h0.</p></li>
<li><p>The shape of h0 is the number of hidden layers and hidden size.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># number of hidden layers, hidden size</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h0</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 2.2133, -1.4602, -0.9452, -1.6756, -0.9290, -0.2674, -0.2496, -1.5272,
          0.9818,  0.6087]])
</pre></div>
</div>
</div>
</div>
<p><strong>Calculating new hidden states and output</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch calculates the output and new hidden states for us for all time steps.</span>
<span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hn</span>  <span class="c1"># hidden state for the last time step in the sequence</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.2377, -0.7253,  0.6461,  0.5337, -0.0806, -0.1899, -0.5599, -0.1239,
         -0.3421, -0.9366]], grad_fn=&lt;SqueezeBackward1&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.7880, -0.8927,  0.8364, -0.9811, -0.0400,  0.9897, -0.3810, -0.7956,
         -0.9250, -0.9535],
        [ 0.2879,  0.0824,  0.5272, -0.3765, -0.0176,  0.7278, -0.2925, -0.1721,
         -0.5300, -0.8516],
        [-0.5859, -0.6065,  0.2953, -0.3474,  0.2902,  0.7199, -0.9498, -0.9228,
         -0.4398, -0.8357],
        [-0.0026, -0.3297,  0.7265, -0.7694,  0.5584,  0.7468,  0.4601, -0.7119,
         -0.9104, -0.9363],
        [-0.2377, -0.7253,  0.6461,  0.5337, -0.0806, -0.1899, -0.5599, -0.1239,
         -0.3421, -0.9366]], grad_fn=&lt;SqueezeBackward1&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span><span class="o">.</span><span class="n">shape</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 10])
</pre></div>
</div>
</div>
</div>
<p>By default, <code class="docutils literal notranslate"><span class="pre">tanh</span></code> activation function is used.</p>
<p><strong>Shapes of the weight matrices</strong></p>
<p>What would be the shapes of weight matrices?</p>
<ul class="simple">
<li><p>Input to hidden (<span class="math notranslate nohighlight">\(W\)</span>)</p></li>
<li><p>Hidden to hidden (<span class="math notranslate nohighlight">\(U\)</span>)</p></li>
</ul>
<p><br><br><br><br></p>
<p>Weight matrix <span class="math notranslate nohighlight">\(W\)</span> between input to hidden layer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 20])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rnn</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()[</span><span class="s2">&quot;weight_ih_l0&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># (hidden, input)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([10, 20])
</pre></div>
</div>
</div>
</div>
<p>Weight matrix <span class="math notranslate nohighlight">\(U\)</span> between hidden layer in time step <span class="math notranslate nohighlight">\(t-1\)</span> to hidden layer in time step <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h0</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 10])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rnn</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()[</span><span class="s2">&quot;weight_hh_l0&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># (hidden, hidden)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([10, 10])
</pre></div>
</div>
</div>
</div>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">rnn</span></code> above is calculating the output of the hidden layer at each time step but we are not calculating <span class="math notranslate nohighlight">\(y_t\)</span> in each time step <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p><br><br></p>
<p>Let’s define a simple RNN for a toy sentiment analysis task.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a toy dataset of sentences and their labels</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;I love machine learning and deep learning&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;I hate it when the Jupyter lab kernel dies on me&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>     
    <span class="p">(</span><span class="s2">&quot;Data cleaning is a tedious task&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;Hidden Markov models are so elegant&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>    
    <span class="p">(</span><span class="s2">&quot;Nothing is more exciting than uncovering hidden patterns in data&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;Debugging machine learning models can be frustrating&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;Overfitting is a common problem in machine learning models&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;It&#39;s rewarding to see your model perform well on unseen data&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;Dealing with missing data is annoying&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;I enjoy learning about neural models for sequence processing&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">]</span>

<span class="c1"># Tokenization and Vocabulary Creation</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="c1"># Tokenize sentences</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">word</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">)</span>

<span class="c1"># Create word to index mapping</span>
<span class="n">word_to_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">())}</span> <span class="c1"># Starting index from 1 for padding</span>
<span class="n">idx_to_word</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_to_idx</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_idx</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># +1 for padding token at index 0 </span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab_size</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>62
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># idx_to_word</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="kn">import</span> <span class="n">pad_sequence</span>

<span class="c1"># Convert sentences to integer sequences</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="p">[[</span><span class="n">word_to_idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span> <span class="k">for</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>

<span class="c1"># Pad sequences to have the same length and create tensors</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">sequences</span><span class="p">)</span>  <span class="c1"># Get max sequence length for padding</span>
<span class="n">padded_sequences</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">sequences</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">label</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">padded_sequences</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 1,  2,  3,  4,  5,  6,  4,  0,  0,  0,  0],
        [ 1,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16],
        [17, 18, 19, 20, 21, 22,  0,  0,  0,  0,  0],
        [23, 24, 25, 26, 27, 28,  0,  0,  0,  0,  0],
        [29, 19, 30, 31, 32, 33, 23, 34, 35, 17,  0],
        [36,  3,  4, 25, 37, 38, 39,  0,  0,  0,  0],
        [40, 19, 20, 41, 42, 35,  3,  4, 25,  0,  0],
        [43, 44, 45, 46, 47, 48, 49, 50, 15, 51, 17],
        [52, 53, 54, 17, 19, 55,  0,  0,  0,  0,  0],
        [ 1, 56,  4, 57, 58, 25, 59, 60, 61,  0,  0]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">SentimentRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">(</span><span class="n">SentimentRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span> <span class="c1"># the constructor of the parent nn.Module class</span>
        
        <span class="c1"># Define an embedding layer:</span>
        <span class="c1"># This layer will transform the input word indices into dense vectors of a specified size (embedding_dim)</span>
        <span class="c1"># vocab_size: the size of the vocabulary (number of unique words in your dataset + 1 for padding)</span>
        <span class="c1"># embedding_dim: the size of the embedding vector for each word</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        
        <span class="c1"># Define a simple RNN layer:</span>
        <span class="c1"># This layer processes the sequences of word embeddings and captures the sequential information</span>
        <span class="c1"># embedding_dim: the input size to the RNN (size of the word embeddings)</span>
        <span class="c1"># hidden_dim: the size of the RNN&#39;s hidden state</span>
        <span class="c1"># batch_first=True: specifies that the input and output tensors will be of shape (batch_size, seq_length, feature)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># Define a fully connected (linear) layer:</span>
        <span class="c1"># This layer maps the RNN&#39;s hidden state to the output classes (positive or negative sentiment in this case)</span>
        <span class="c1"># hidden_dim: the size of the RNN&#39;s hidden state (input features to this layer)</span>
        <span class="c1"># output_dim: the number of output classes (2 for binary classification)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="c1"># Forward pass through the embedding layer:</span>
        <span class="c1"># text: the input text sequences (batch of tokenized and indexed words)</span>
        <span class="c1"># embedded: the embedded representation of the input text</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        
        <span class="c1"># Forward pass through the RNN layer:</span>
        <span class="c1"># embedded: the sequences of embedded words</span>
        <span class="c1"># output: the output features from the RNN for each time step (we won&#39;t use this here)</span>
        <span class="c1"># hidden: the final hidden state from the RNN</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
        
        <span class="c1"># Assert that the last output of the RNN matches the final hidden state</span>
        <span class="c1"># This is just a sanity check and is not necessary for the model to function</span>
        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">hidden</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        
        <span class="c1"># Forward pass through the fully connected layer:</span>
        <span class="c1"># We use the final hidden state to predict the sentiment</span>
        <span class="c1"># hidden.squeeze(0): removes the first dimension of the hidden state to match the input shape expected by the linear layer</span>
        <span class="c1"># This operation is needed because the RNN layer outputs hidden states with a shape (num_layers, batch_size, hidden_dim),</span>
        <span class="c1"># but the linear layer expects inputs with a shape (batch_size, hidden_dim)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>


<span class="c1"># Create an instance of our SentimentRNN model, specifying the vocabulary size, embedding dimension,</span>
<span class="c1"># hidden dimension (size of the RNN&#39;s hidden state), and output dimension (number of classes).</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentimentRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>What are the parameters of the model?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchsummary</span> <span class="kn">import</span> <span class="n">summary</span>

<span class="n">summary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Embedding: 1-1                         6,200
├─RNN: 1-2                               29,440
├─Linear: 1-3                            258
=================================================================
Total params: 35,898
Trainable params: 35,898
Non-trainable params: 0
=================================================================
</pre></div>
</div>
</div>
</div>
<p>How are these parameters calculated?</p>
<ul class="simple">
<li><p>Embedding layer: vocab_size * embedding_dim = 62 * 100</p></li>
<li><p>RNN layer: (embedding_dim * hidden_dim) + hidden_dim + (hidden_dim * hidden_dim) + hidden_dim = (100 * 128) + 128 + (128 * 128) + 128 = 29,440</p></li>
<li><p>Linear layer: (hidden_dim * output_features) + output_features = (128 * 2) + 2 = 258</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">softmax</span>

<span class="c1"># Assuming a single batch for simplicity</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">padded_sequences</span><span class="p">)</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.3753, 0.6247],
        [0.3450, 0.6550],
        [0.3763, 0.6237],
        [0.3779, 0.6221],
        [0.3577, 0.6423],
        [0.3742, 0.6258],
        [0.4037, 0.5963],
        [0.5773, 0.4227],
        [0.3758, 0.6242],
        [0.4018, 0.5982]], grad_fn=&lt;SoftmaxBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>The probabilities won’t make sense because we have not trained the model.</p>
<p>The training loop will be very similar to that of feedforward neural networks. However, I won’t implement it for this toy example, as it won’t learn much from our tiny corpus.</p>
<p><br><br><br><br></p>
<p><img alt="" src="../../_images/eva-coffee.png" /></p>
<p><br><br><br><br></p>
</section>
</section>
<section id="id1">
<h2>❓❓ Questions for you<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<section id="exercise-5-1-select-all-of-the-following-statements-which-are-true-iclicker">
<h3>Exercise 5.1: Select all of the following statements which are <strong>True</strong> (iClicker)<a class="headerlink" href="#exercise-5-1-select-all-of-the-following-statements-which-are-true-iclicker" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>(A) RNNs pass along information between time steps through hidden layers.</p></li>
<li><p>(B) RNNs are appropriate only for text data.</p></li>
<li><p>(C) At each time step in an RNN, we use a unique hidden state (<code class="docutils literal notranslate"><span class="pre">h</span></code>), a unique input (<code class="docutils literal notranslate"><span class="pre">X</span></code>), but we reuse the same <code class="docutils literal notranslate"><span class="pre">U</span></code> matrix of weights.</p></li>
<li><p>(D) The number of parameters in an RNN language model would grow with the number of time steps.</p></li>
<li><p>(E) The hidden state at the current time step in an RNN depends only on the input data at the current time step and the hidden state from the previous time step.
<br><br><br><br></p></li>
</ul>
</section>
</section>
<section id="training-rnns">
<h2>4. Training RNNs<a class="headerlink" href="#training-rnns" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>RNN is a <strong>supervised machine learning model</strong>. Similar to feedforward networks, we’ll use a</p>
<ul>
<li><p>training set</p></li>
<li><p>a loss function</p></li>
<li><p>backpropagation to obtain the gradients needed to adjust the weights in these networks</p></li>
</ul>
</li>
<li><p>We have 3 sets of weights (and the corresponding bias terms) to update</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W \rightarrow \)</span> the weight matrix between input layer and hidden layer</p></li>
<li><p><span class="math notranslate nohighlight">\(U \rightarrow \)</span> the weight matrix between previous hidden layer to current hidden layer</p></li>
<li><p><span class="math notranslate nohighlight">\(V \rightarrow \)</span> the weight matrix between hidden layer and output layer</p></li>
</ul>
</li>
</ul>
<p>We want to assess the error occurring at time step <span class="math notranslate nohighlight">\(t\)</span>.</p>
<ul class="simple">
<li><p>To compute the loss function for the output at time <span class="math notranslate nohighlight">\(t\)</span> we need the hidden layer from time <span class="math notranslate nohighlight">\(t-1\)</span>.</p></li>
<li><p>The hidden layer at time <span class="math notranslate nohighlight">\(t\)</span> influences both the output at time <span class="math notranslate nohighlight">\(t\)</span> and the hidden layer at time <span class="math notranslate nohighlight">\(t+1\)</span>.</p></li>
</ul>
<p><img alt="" src="../../_images/RNN_loss.png" /></p>
<!-- <img src="img/RNN_loss.png" height="1500" width="1500">  -->
<p><a class="reference external" href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf">Credit</a></p>
<p><img alt="" src="../../_images/BPTT_small.png" /></p>
<ul class="simple">
<li><p>To assess the error occurring to <span class="math notranslate nohighlight">\(h_t\)</span>, we need to know its influence on both the current output and the ones that follow.</p></li>
<li><p>This is different than the usual backpropagation. We need to tailor backpropogation algorithm to this situation. In RNNs we use a generalized version of <strong>Backpropogation called Backpropogation Through Time (BPTT)</strong></p></li>
</ul>
<ul class="simple">
<li><p>The loss calculation depends upon the task and the architecture we are using.</p></li>
<li><p>The overall loss is the summation of losses at each time step.</p></li>
</ul>
<ul class="simple">
<li><p>See <a class="reference external" href="https://gist.github.com/karpathy/d4dee566867f8291f086">the code</a> for the above in ~112 lines of Python written by Andrej Karpathy. The code has only <code class="docutils literal notranslate"><span class="pre">numpy</span></code> dependency.</p></li>
</ul>
<p><br><br><br><br></p>
</section>
<section id="rnn-applications">
<h2>5. RNN applications<a class="headerlink" href="#rnn-applications" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>We have seen the basic RNN architecture below.</p></li>
</ul>
<p><img alt="" src="../../_images/RNN-intro_small.png" /></p>
<!-- <img src="img/RNN-intro.png" height="500" width="500">  -->
<ul class="simple">
<li><p>But a number of architectures are possible, which makes them a very rich family of models.</p></li>
</ul>
<ul class="simple">
<li><p>A number of possible RNN architectures</p></li>
</ul>
<p><img alt="" src="../../_images/RNN_architectures.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_architectures.png" height="1500" width="1500">  -->
<!-- </center>     -->
<p><a class="reference external" href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf">source</a></p>
<p>Let’s see how can we apply it to three different types of NLP tasks:</p>
<ul class="simple">
<li><p>Sequence labeling (e.g., POS tagging)</p></li>
<li><p>Sequence classification (e.g. sentiment analysis or text classification)</p></li>
<li><p>Text generation</p></li>
</ul>
<section id="sequence-labeling">
<h3>5.1 Sequence labeling<a class="headerlink" href="#sequence-labeling" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The task is to assign a label from a fixed set of labels to each element in the sequence.</p>
<ul>
<li><p>Part-of-speech tagging</p></li>
<li><p>Named entity recognition</p></li>
</ul>
</li>
<li><p>Many-to-many architecture</p></li>
<li><p>Inputs are usually pre-trained word embeddings and outputs are tag probabilities generated by a softmax layer over the given tagset.</p></li>
<li><p>The RNN block is an abstraction representing an unrolled simple RNN consisting of an input layer, hidden layer and output layer at each time step and shared weight matrices <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(W\)</span>, and <span class="math notranslate nohighlight">\(V\)</span>.</p></li>
</ul>
<p><img alt="" src="../../_images/RNN_seq_labeling.png" /></p>
<!-- <img src="img/RNN_seq_labeling.png" height="600" width="600">  -->
<p><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Source</a></p>
</section>
<section id="sequence-classification">
<h3>5.2 Sequence classification<a class="headerlink" href="#sequence-classification" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>We have done text classification such as sentiment analysis or spam identification before with traditional ML models, where we ignored the temporal nature of language.</p></li>
<li><p>These are actually sequence classification tasks where we want to map a sequence of text to a label from a small set of labels (e.g., positive, negative, neutral).</p></li>
<li><p>To apply RNNs in this setting, we take the text to be classified and pass one word at a time generating a new hidden layer at each time step. We can then take the hidden layer from the last time step, <span class="math notranslate nohighlight">\(h_n\)</span>, which has the compressed representation of the entire sequence. We pass this representation through a feedforward neural network which chooses a class via a softmax.</p></li>
<li><p>This is a many-to-one RNN architecture.</p></li>
</ul>
<p><img alt="" src="../../_images/RNN_classification.png" /></p>
<!-- <img src="img/RNN_classification.png" height="600" width="600">  -->
<p><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Source</a></p>
<ul class="simple">
<li><p>Similar to the sequence labeling example, we can also pass word embeddings as input.</p></li>
<li><p>Note that in this approach we do not have immediate outputs at each time step and we do not need to compute <span class="math notranslate nohighlight">\(y\)</span> at each time step. We only have an output at the last time step.</p></li>
<li><p>So there won’t be loss terms associated with each time step.</p></li>
<li><p>The loss function used to train the weights in the network is entirely based on the final text classification task.</p></li>
<li><p>We will compare the output of the softmax layer of the feed-forward classifier and the actual <span class="math notranslate nohighlight">\(y\)</span> to calculate the loss (e.g., cross-entropy loss) and this loss will drive the training.</p></li>
<li><p>The error signal is backpropagated all the way through the weights in the feed-forward classifier, through its input, which is the hidden layer output of the last time step, through the three sets of RNN weights: <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(V\)</span>, and <span class="math notranslate nohighlight">\(W\)</span>.</p></li>
</ul>
</section>
<section id="text-generation">
<h3>5.3 Text generation<a class="headerlink" href="#text-generation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The idea is similar to text generation with Markov models.</p></li>
<li><p>We start with a seed. We then continue to sample words conditioned on our previous choices until we reach a pre-determined desired length of a sequence or end-of-sequence token is generated.</p></li>
<li><p>In the context of RNNs</p>
<ul>
<li><p>We start with a seed. In the example below, we are starting with a special beginning of sequence token &lt;s&gt;.</p></li>
<li><p>We use embedding representation of this token and pass it to the RNN.</p></li>
<li><p>We sample a word in the output from the softmax distribution.</p></li>
<li><p>We use this sampled word as the input in the next time step and then sample the next word in the same fashion.</p></li>
<li><p>We continue this until the fixed length limit or the end of the sentence marker is reached.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../../_images/RNN_generation_small.png" /></p>
<!-- <img src="img/RNN_generation.png" height="600" width="600">  -->
<ul class="simple">
<li><p>The same idea can be used for music generation.</p></li>
</ul>
<p><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Source</a></p>
<p>You can find a toy example of of RNN text generation with PyTorch in <a class="reference internal" href="#AppendixC-toy-RNN.ipynb"><span class="xref myst">AppendixC</span></a>.</p>
<p><br><br></p>
</section>
<section id="image-captioning">
<h3>5.4 Image captioning<a class="headerlink" href="#image-captioning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The same idea can be used for more complicated applications such as machine translation, summarization, or image captioning.</p></li>
<li><p>The idea is to prime the generation component with an appropriate context.</p></li>
<li><p>For example, in image captioning we can prime the generation component with a meaningful  representation of an image given by the last layer in CNNs.</p></li>
<li><p>You’ll work on this application in the lab next week.</p></li>
</ul>
<p><img alt="" src="../../_images/image_captioning_small.png" /></p>
<!-- <center> -->
<!-- <img src="img/image_captioning.png" width="1000" height="1000"> -->
<!-- </center> -->
<p><a class="reference external" href="https://cs.stanford.edu/people/karpathy/sfmltalk.pdf">Source</a></p>
<p><br><br><br><br></p>
</section>
</section>
<section id="stacked-and-bidirectional-rnn-architectures">
<h2>6. Stacked and Bidirectional RNN architectures<a class="headerlink" href="#stacked-and-bidirectional-rnn-architectures" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>We have seen a simple RNN with one hidden layer.</p></li>
<li><p>But RNNs are quite flexible.</p></li>
<li><p>Two common ways to create complex networks by combining RNNs are:</p>
<ul>
<li><p>Stacked RNNs</p></li>
<li><p>Bidirectional RNNs</p></li>
</ul>
</li>
</ul>
<section id="stacked-rnns">
<h3>6.1 Stacked RNNs<a class="headerlink" href="#stacked-rnns" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In the examples thus far, the input of RNNs was a sequence of word or character embeddings. We were passing the output of the RNN layer to the output layer and the outputs have been vectors useful for predicting next words, tags, or sequence labels.</p></li>
</ul>
<p><img alt="" src="../../_images/RNN_seq_labeling.png" /></p>
<!-- <img src="img/RNN_seq_labeling.png" height="600" width="600">  -->
<p><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Source</a></p>
<ul class="simple">
<li><p>But nothing prevents us from using <strong>the sequence of outputs from one RNN as an input sequence to another one</strong>.</p></li>
<li><p>These are called <strong>stacked RNNs</strong> which consist of multiple networks where the output of one layer serves as the input to a subsequent layer.</p></li>
</ul>
<p><img alt="" src="../../_images/RNN_stacked.png" /></p>
<!-- <img src="img/RNN_stacked.png" height="600" width="600">  -->
<p><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Source</a></p>
<ul class="simple">
<li><p>Stacked RNNs generally outperform single-layer networks.</p></li>
<li><p>The network learns a different level of abstraction at each layer.</p></li>
<li><p>You can optimize your network for number of layers for your specific application and dataset.</p></li>
<li><p>But remember that more layers means higher training cost.</p></li>
</ul>
<p><br><br></p>
</section>
<section id="bidirectional-rnns">
<h3>6.2 Bidirectional RNNs<a class="headerlink" href="#bidirectional-rnns" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The RNN uses information from the prior context to make predictions at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p>But in many applications (e.g., POS tagging) we do have access to the entire input sequence and knowing the context on the right of time <span class="math notranslate nohighlight">\(t\)</span> can be useful.</p></li>
<li><p>For example, suppose you are doing POS tagging and you are at the token <strong>Teddy</strong> in the sequence. It will be useful to know the right context in order to make the decision on whether it should be tagged as a <em>noun</em> or a <em>proper noun</em>.</p></li>
</ul>
<blockquote>
<div><p>He said , “ <strong>Teddy</strong> Roosevelt was a great president ! “<br></p>
</div></blockquote>
<blockquote>
<div><p>He said , “ <strong>Teddy</strong> bears are on sale ! “</p>
</div></blockquote>
<p><br><br><br><br></p>
<ul class="simple">
<li><p>How can we use the words on the right of time step <span class="math notranslate nohighlight">\(t\)</span> as context?</p></li>
<li><p>In the left-to-right RNN, the hidden state at time <span class="math notranslate nohighlight">\(t\)</span> represents everything the network knows about the sequence up to that point.</p></li>
<li><p>Suppose <span class="math notranslate nohighlight">\(h_t^f\)</span> denotes a hidden state at time <span class="math notranslate nohighlight">\(t\)</span> representing everything the network has gleaned from the sequence so far.
$<span class="math notranslate nohighlight">\(h_t^f = RNN_{forward}(x_1, x_2, \dots, x_t) \)</span>$</p></li>
<li><p>We can also train the network in the reverse direction, from right to left, to take advantage of the right context.</p></li>
<li><p>With this approach the hidden state at time <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(h_t^b\)</span> represents all the information we have learned about the sequence from time <span class="math notranslate nohighlight">\(t\)</span> to the end of the sequence.
$<span class="math notranslate nohighlight">\(h_t^b = RNN_{backward}(x_t, x_{t+1}, \dots, x_n) \)</span>$</p></li>
<li><p>(Somewhat similar to the <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> values in the forward and backward algorithms in HMMs.)</p></li>
</ul>
<p>A <strong>bidirectional RNN</strong> combines two independent RNNs:</p>
<ul class="simple">
<li><p>One where the input is processed from the start to the end</p></li>
<li><p>The other from the end to the start.</p></li>
<li><p>Each RNN will result in some representation of the input.</p></li>
<li><p>We then combine the two representations computed by two independent RNNs into a single vector which captures both the left and right contexts of an input at each point in time.</p></li>
<li><p>We can combine vectors by</p>
<ul>
<li><p>Concatenating them, as shown in the picture below or</p></li>
<li><p>Element-wise addition</p></li>
<li><p>Element-wise multiplication</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../../_images/bidirectional_seq_labeling.png" /></p>
<!-- <img src="img/bidirectional_seq_labeling.png" height="600" width="600">  -->
<p><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Source</a></p>
<ul class="simple">
<li><p>You can also use bidirectional RNNs for sequence classification.</p></li>
<li><p>Recall that in sequence classification we pass the final hidden state of the RNN as input to a subsequent feedforward classifier.</p></li>
<li><p>The problem with this approach is that the final hidden state reflects more information about the end of the sequence than its beginning.</p></li>
<li><p>Bidirectional RNNs provide a simple solution to this problem. We can create a final hidden state by combining hidden states of forward and backward passes so that the hidden state reflects information about both the beginning and end of the sequence.</p></li>
</ul>
<p><img alt="" src="../../_images/bidirectional_classification.png" /></p>
<!-- <img src="img/bidirectional_classification.png" height="600" width="600">  -->
<p><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Source</a></p>
<p><br><br><br><br></p>
</section>
</section>
<section id="final-comments-and-summary">
<h2>Final comments and summary<a class="headerlink" href="#final-comments-and-summary" title="Link to this heading">#</a></h2>
<section id="important-ideas-to-know">
<h3>Important ideas to know<a class="headerlink" href="#important-ideas-to-know" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>RNNs are supervised neural network models to process sequential data.</p></li>
<li><p>The intuition is to put multiple feed-forward networks together and making connections between hidden layers.</p></li>
<li><p>They have feedback connections in their structure to “remember” previous inputs, when reading in a sequence.</p></li>
<li><p>In simple RNNs sequences are processed one element at a time. The output of each neural unit at time <span class="math notranslate nohighlight">\(t\)</span> is based on the current input at <span class="math notranslate nohighlight">\(t\)</span> and the hidden layer at time <span class="math notranslate nohighlight">\(t-1\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>RNNs share parameters across different time steps, making them efficient for modeling sequences.</p></li>
<li><p>They are trained using a generalized version of backpropagation called Backpropagation Through Time (BPTT).</p></li>
<li><p>In practice, we often use truncated BPTT, where we update the network using smaller chunks of the sequence.</p></li>
<li><p>There are many possible RNN architectures.</p></li>
<li><p>Standard RNNs struggle with long-distance dependencies due to issues like vanishing gradients.</p></li>
<li><p>To address this, more sophisticated variants such as <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html">LSTMs</a> and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html">GRUs</a> are commonly used. These models follow the same general idea as RNNs but include additional components to better manage memory and information flow.</p></li>
<li><p>PyTorch provides built-in implementations of <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html">LSTMs</a> and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html">GRUs</a>, which you can use directly instead of the basic RNN.</p></li>
</ul>
</section>
<section id="coming-up">
<h3>Coming up …<a class="headerlink" href="#coming-up" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Intuition of transformers</p></li>
</ul>
</section>
<section id="resources">
<h3>Resources<a class="headerlink" href="#resources" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Sequence processing with Recurrent Neural Networks</a> (The notes above are heavily based on this resource.)</p></li>
<li><p><a class="reference external" href="https://axon.cs.byu.edu/Dan/678/papers/Recurrent/Werbos.pdf">Backpropagation Through Time: What it does and how to do it</a></p></li>
<li><p><a class="reference external" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></p></li>
<li><p><a class="reference external" href="https://www.coursera.org/lecture/nlp-sequence-models/recurrent-neural-network-model-ftkzt">Coursera: NLP sequence models</a></p></li>
<li><p><a class="reference external" href="https://gist.github.com/karpathy/d4dee566867f8291f086#file-min-char-rnn-py-L112">RNN code in 112 lines of Python</a></p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-env-575-py"
        },
        kernelOptions: {
            name: "conda-env-575-py",
            path: "./lectures/notes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-env-575-py'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="04_More-HMMs.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Lecture 4: More HMMs</p>
      </div>
    </a>
    <a class="right-next"
       href="06_intro-to-transformers.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 6: Introduction to self-attention and transformers</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-plan-imports-lo">Lecture plan, imports, LO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-your-mood-today">What’s your mood today?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">1. Motivation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activity-feedforward-neural-networks-for-sentiment-analysis">1.1 Activity: Feedforward neural networks for sentiment analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-introduction">1.2 RNNs introduction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-details">2. RNN details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-presentations">2.1 RNN presentations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-sharing">2.2 Parameter sharing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass-in-rnns">3. Forward pass in RNNs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-new-state-h-t">3.1 Computing the new state <span class="math notranslate nohighlight">\(h_t\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-output-y-t">3.2 Computing the output <span class="math notranslate nohighlight">\(y_t\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-forward-pass-with-pytorch">3.3 RNN Forward pass with PyTorch</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-5-1-select-all-of-the-following-statements-which-are-true-iclicker">Exercise 5.1: Select all of the following statements which are <strong>True</strong> (iClicker)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-rnns">4. Training RNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-applications">5. RNN applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequence-labeling">5.1 Sequence labeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequence-classification">5.2 Sequence classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-generation">5.3 Text generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-captioning">5.4 Image captioning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stacked-and-bidirectional-rnn-architectures">6. Stacked and Bidirectional RNN architectures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stacked-rnns">6.1 Stacked RNNs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bidirectional-rnns">6.2 Bidirectional RNNs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-comments-and-summary">Final comments and summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-ideas-to-know">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coming-up">Coming up …</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resources">Resources</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>