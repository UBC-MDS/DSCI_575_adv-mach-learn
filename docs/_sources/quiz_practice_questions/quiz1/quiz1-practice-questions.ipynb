{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 575 - Quiz 1 practice questions\n",
    "\n",
    "Note that these are just sample questions for you to get an idea about what to expect in the quiz. They are neither meant to cover all the topics we have covered in the last four lectures nor meant to be indicative of the number of questions in the actual quiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Let's say P(sunny today | sunny yesterday) = 0.8 and P(sunny today | cloudy yesterday) = 0.4. What is the transition matrix? State your assumptions as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 1.1c: V's Solutions! \n",
    ":class: tip, dropdown\n",
    "\n",
    "|               | sunny  | cloudy |\n",
    "| ------------- |:---------:| -----:|\n",
    "| sunny         | 0.8       | 0.2   |\n",
    "| cloudy        | 0.4       | 0.6   |\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "![](Markov_ex.png)\n",
    "<!-- <img src=\"img/Markov_ex.png\" height=\"500\" width=\"500\">  -->\n",
    "\n",
    "Does a stationary distribution exist for this chain? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solution \n",
    ":class: tip, dropdown\n",
    "\n",
    "Yes. Stationary distribution exists for this chain because all transitions are positive (>0). \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "For sentence segmentation in English, why is simple splitting on typical sentence ending markers (e.g., `.', `?', `!') not enough? Give an example where such approach would fail.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "These markers are ambiguous in English and the sentence segmentation would fail on texts containing abbreviations such as Dr. or  Ph.D.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Select all the statements which are true\n",
    "\n",
    "- [ ] In a realistic corpus, a word-based Markov model with `n=2` typically has a larger state space than a character-based model with the same `n` value.\n",
    "- [ ] The stationary distribution of a Markov chain is a probability distribution that remains unchanged over time, regardless of the initial state. \n",
    "- [ ] Higher perplexity of a language model on a test set indicates better accuracy in predicting the next words on the unseen test set.\n",
    "- [ ] In PageRank the webpage or node with the most incoming links gets the highest ranking.\n",
    "- [ ] The Viterbi algorithm can be used to determine the best hidden state sequence corresponding to a given observation sequence.\n",
    "- [ ] In the context of HMMs, like the $\\alpha$ values in the forward algorithm, the $\\delta$ values in the Viterbi algorithm represent the probability of being in state $i$ at time $t$ and observing the observations up to the current time step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "- [x] In a realistic corpus, a word-based Markov model with `n=2` typically has a larger state space than a character-based model with the same `n` value.\n",
    "- [x] The stationary distribution of a Markov chain is a probability distribution that remains unchanged over time, regardless of the initial state. \n",
    "- [ ] Higher perplexity of a language model on a test set indicates better accuracy in predicting the next words on the unseen test set.\n",
    "- [ ] In PageRank the webpage or node with the most incoming links gets the highest ranking.\n",
    "- [x] The Viterbi algorithm can be used to determine the best hidden state sequence corresponding to a given observation sequence.\n",
    "- [ ] In the context of HMMs, like the \\(\\alpha\\) values in the forward algorithm, the \\(\\delta\\) values in the Viterbi algorithm represent the probability of being in state \\(i\\) at time \\(t\\) and observing the observations up to the current time step. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "State an advantage and a disadvantage of a word-based language model over a character-based language model in text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "(Many possibilities)\n",
    "- Advantages: The generated text would have valid words even for smaller values of `n`. \n",
    "- Disadvantage: The state space and memory requirements can go up quite quickly.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Below is a hidden Markov model that relates numbers of ice creams eaten by Jason to the weather which we saw in class. \n",
    "\n",
    "![](ice-cream-hmm.png)\n",
    "\n",
    "<!-- <img src=\"img/ice-cream-hmm.png\" height=\"600\" width=\"600\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/A.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 \n",
    "\n",
    "Assume the observation sequence 3, 3, 2, 1 and a corresponding hidden state sequence HOT HOT HOT COLD for time steps t0, t1, t2, t3, respectively. Assuming HMM assumptions, what would be P(3 at t1 | HOT at t0, HOT at t1, 3 at t0)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "P(3 at t1) $\\approx$ P(3 | HOT) = 0.4 because according to the HMM assumptions, the probability of any observation at a given time step only depends upon the hidden state at that time step and nothing else. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2\n",
    "\n",
    "Given this HMM, what method will you use to calculate P(3, 3, 2, 1) efficiently? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "The forward method which uses dynamic programming to efficiently compute probability of an observation sequence given HMM parameters (time complexity = $\\mathcal{O}(n^2T)$, where $n$ is the number of hidden states and $T$ is the number of time steps).  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Should stopwords be removed when training an n-gram language model for text generation in order to focus on learning transitions between more meaningful words? Provide a brief explanation to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "No, stopwords should not be removed. Although they may seem less \"meaningful\", stopwords play a vital role in the structure and grammar of language. An n-gram model trained without them would fail to learn important syntactic patterns, leading to ungrammatical and unnatural text generation.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Suppose you have two language models A and B. Model A has a perplexity of 50 on a test set, and Model B has a perplexity of 200. Which model performs better, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solution\n",
    ":class: tip, dropdown\n",
    "Model A performs better. Lower perplexity indicates that the model is less \"surprised\" by the test data, i.e., it assigns higher probabilities to the observed sequences and therefore predicts more accurately.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 \n",
    "\n",
    "Which of the following best describes the effect of increasing the temperature parameter in temperature sampling?\n",
    "\n",
    "- (A) It makes the model more deterministic by favouring the highest-probability tokens.\n",
    "- (B) It increases randomness by flattening the probability distribution over possible next tokens.\n",
    "- (C) It limits the output to only the top-k tokens.\n",
    "- (D) It prevents the repetition of any previously seen token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solution\n",
    ":class: tip, dropdown\n",
    "- B\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 \n",
    "\n",
    "In lab 2, you trained HMMs using hmmlearn's GaussianHMM because the observations were continuous and not discrete. What exactly was an observation in this problem? Be concise and clear.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solution\n",
    ":class: tip, dropdown\n",
    "Each observation was a 15-dimensional MFCC feature vector corresponding to the audio in a time frame. \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
