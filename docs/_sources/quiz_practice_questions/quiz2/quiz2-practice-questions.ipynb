{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 575 - Quiz 2 practice questions\n",
    "\n",
    "Please note that these are sample questions intended to give you an idea of what to expect in the quiz. They do not cover all the topics we have covered in the last four lectures, nor do they indicate the exact number of questions in the actual quiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Which of the following are **true** about Recurrent Neural Networks (RNNs)?\n",
    "\n",
    "- (A) RNNs can process inputs of varying lengths.\n",
    "      \n",
    "- (B) RNNs can capture long-term dependencies without any issues.\n",
    "- (C) RNNs maintain a hidden state that carries information from previous steps.\n",
    "- (D) RNNs are well-suited for parallelization during training.\n",
    "- (E) RNNs are suitable for sequential data such as text or speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solutions! \n",
    ":class: tip, dropdown\n",
    "- A, C, E\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: \n",
    "\n",
    "Match each architecture to its most common application.\n",
    "\n",
    "| Architecture           | Application                         |\n",
    "|------------------------|-------------------------------------|\n",
    "| A. Decoder-only        | 1. Text classification              |\n",
    "| B. Encoder-only        | 2. Text generation (e.g., GPT)      |\n",
    "| C. Encoder-Decoder     | 3. Machine translation              |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solutions! \n",
    ":class: tip, dropdown\n",
    "- A: 2  \n",
    "- B: 1  \n",
    "- C: 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Give one similarity and one difference between RNNs and Transformers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solutions! \n",
    ":class: tip, dropdown\n",
    "\n",
    "(Many possible answers)\n",
    "\n",
    "Both RNNs and transformer models handle sequential data with variable length inputs and outputs. One key difference between them is in their architecture. RNNs use recurrent connections to capture long-distance dependencies, while transformers employ self-attention mechanisms. RNNs often suffer from the problem of vanishing gradients, whereas transformers use self-attention to effectively capture local and global dependencies, including long-distance dependencies. Another advantage of transformers is that they can be parallelized, while RNNs cannot.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Consider a toy transformer model where \n",
    "- the sequence length is 20\n",
    "- each token is represented by a 512-dimensional embedding (`d_model` in Pytorch) \n",
    "- the model uses multihead self-attention with 4 attention heads, each head with its own set of trainable parameters to transform input embeddings into queries, keys, and values \n",
    "\n",
    "\n",
    "| Component                | Shape                               |\n",
    "|--------------------------|-------------------------------------|\n",
    "| A. $W^Q$ (Query matrix)         | 1. (512, 512)                       |\n",
    "| B. $W^K$ (Key matrix)           | 2. (512, 512)                       |\n",
    "| C. $Q$ (Query vectors)   | 3. (batch_size, 20, 20)             |\n",
    "| D. Attention scores      | 4. (batch_size, 20, 512)            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solutions! \n",
    ":class: tip, dropdown\n",
    "\n",
    "- A: 1  \n",
    "- B: 2  \n",
    "- C: 4  \n",
    "- D: 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "- Name two differences between a single-head and multihead attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solutions! \n",
    ":class: tip, dropdown\n",
    "\n",
    "(Many possible answers)\n",
    "1. Multihead attention is capable of capturing diverse parallel relations among input elements.   \n",
    "2. Multi-head attention typically requires more parameters compared to single-head attention. \n",
    "3. If $d$ is the input dimension, in a single-head attention layer, the output after weighted summation is a vector of dimension $d$. However, in a multi-head attention layer, there will be $h$ $d$-dimensional vectors, where $h$ is the number of heads. These vectors need to be concatenated and passed through a linear layer to obtain a $d$-dimensional representation.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "You're designing a system to identify which customer reviews are expressing **fear**, **joy**, or **sadness**, but you don't have any labeled training data. \n",
    "\n",
    "Which tool or approach is best suited?\n",
    "\n",
    "- (A) Fine-tuning a BERT model\n",
    "  \n",
    "- (B) Using `zero-shot-classification` with a pretrained model like `bart-large-mnli`  \n",
    "- (C) Training an RNN from scratch  \n",
    "- (D) Using a summarization pipeline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solutions! \n",
    ":class: tip, dropdown\n",
    "\n",
    "- B\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 7\n",
    "\n",
    "Consider the language modeling task. Given the text below, you want to predict the word followed by the word in. \n",
    "\n",
    "> She was born in Italy. She spent most of her childhood and teenage years there. She even worked as a school teacher there. She moved to North America only when she was 30. That's why she is so fluent in  ____. \n",
    "\n",
    "Would a Markov model of language (i.e., an n-gram language model) be a reasonable choice in this language modeling task to successfully predict the word Italian? Why or why not? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solutions! \n",
    ":class: tip, dropdown\n",
    "\n",
    "No. The relevant word Italy occurs around 40+ words away from the word to be predicted. If we train a Markov model with such a large n, the model will be very sparse; it's very unlikely that the exact sequence of the 40 words above occurs more than once in the corpus. Large n also means higher RAM requirements.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Suppose you are using a basic (vanilla) RNN layer with the following:\n",
    "\n",
    "- Input size = 100 (i.e., each input token is a 100-dimensional vector)  \n",
    "- Hidden size = 64 (i.e., the hidden state has 64 units)  \n",
    "- Sequence length = 10  \n",
    "- Batch size = 32  \n",
    "\n",
    "What are the shapes of the following matrices?\n",
    "\n",
    "Match each weight matrix with its shape:\n",
    "\n",
    "| Matrix                   | Shape                                  |\n",
    "|--------------------------|-----------------------------------------|\n",
    "| A. `W_ih` (input to hidden) | 1. (64, 64)                            |\n",
    "| B. `W_hh` (hidden to hidden) | 2. (64, 1)                           |\n",
    "| C. `b_hh` (bias between hidden to hidden) | 3. (64, 100)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} V's Solutions! \n",
    ":class: tip, dropdown\n",
    "\n",
    "- A: 3  \n",
    "- B: 1  \n",
    "- C: 2  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Other than these sample questions, there are many questions in the lecture notes which you can go through to assess your understanding of the material. There won't be tedious calculations in the quiz. Make sure you understand the dimensionality of different weight matrices in the context of RNNs and transformers._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
