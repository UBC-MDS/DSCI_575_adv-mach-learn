{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49fecb8b-833e-4f1d-979c-29de9ebc3e37",
   "metadata": {},
   "source": [
    "## Class Demo: Recipe generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82734cd-af1f-481d-8609-5519ffaa8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from urllib.request import urlopen\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc90cc6-d9bc-4747-bc91-ed5dcc065c47",
   "metadata": {},
   "source": [
    "This is a demo for recipe generation using PyTorch and Transformers. \n",
    "For the purpose of this demo, we'll sample 10_000 recipe titles from the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765fad1d-0cc9-4d88-a4d8-357a767fc072",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_recipes_df = pd.read_csv(\"../data/RAW_recipes.csv\")\n",
    "orig_recipes_df = orig_recipes_df.dropna()\n",
    "recipes_df = orig_recipes_df.sample(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7839bff9-07e0-4e3d-be61-2aba64b9fefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>tags</th>\n",
       "      <th>nutrition</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>steps</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>n_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166255</th>\n",
       "      <td>provencal tomato stuffed bell peppers</td>\n",
       "      <td>97801</td>\n",
       "      <td>45</td>\n",
       "      <td>4470</td>\n",
       "      <td>2004-08-14</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[91.4, 10.0, 19.0, 12.0, 2.0, 4.0, 2.0]</td>\n",
       "      <td>7</td>\n",
       "      <td>['preheat oven to 375f', 'lightly brush an 8\" ...</td>\n",
       "      <td>this recipe is from the cooking club of americ...</td>\n",
       "      <td>['olive oil', 'red peppers', 'roma tomatoes', ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369</th>\n",
       "      <td>apple cabbage ravioli in a savory herb bacon b...</td>\n",
       "      <td>437558</td>\n",
       "      <td>40</td>\n",
       "      <td>1329782</td>\n",
       "      <td>2010-09-17</td>\n",
       "      <td>['bacon', 'weeknight', '60-minutes-or-less', '...</td>\n",
       "      <td>[314.2, 36.0, 28.0, 33.0, 23.0, 53.0, 4.0]</td>\n",
       "      <td>34</td>\n",
       "      <td>['apple cabbage filling -- in a large saute pa...</td>\n",
       "      <td>these are so good and just reminds me of fall....</td>\n",
       "      <td>['green cabbage', 'onion', 'golden delicious a...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154755</th>\n",
       "      <td>paula deen s uncle bubba s wings</td>\n",
       "      <td>407501</td>\n",
       "      <td>35</td>\n",
       "      <td>339260</td>\n",
       "      <td>2010-01-09</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'prepar...</td>\n",
       "      <td>[1310.1, 159.0, 8.0, 204.0, 170.0, 199.0, 1.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>['prepare the wings: combine the hot sauce , c...</td>\n",
       "      <td>paula's younger brother, earl \"bubba\" hiers, i...</td>\n",
       "      <td>['hot sauce', 'cajun seasoning', 'cayenne pepp...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176351</th>\n",
       "      <td>roasted tomatoes onions and zucchini</td>\n",
       "      <td>95881</td>\n",
       "      <td>35</td>\n",
       "      <td>52282</td>\n",
       "      <td>2004-07-19</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[121.5, 16.0, 17.0, 0.0, 3.0, 7.0, 2.0]</td>\n",
       "      <td>6</td>\n",
       "      <td>['preheat oven to 375f', 'arrange the zucchini...</td>\n",
       "      <td>roasting vegetables brings out their sweetness...</td>\n",
       "      <td>['zucchini', 'roma tomatoes', 'vidalia onion',...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206171</th>\n",
       "      <td>sweet onion burgers</td>\n",
       "      <td>81346</td>\n",
       "      <td>25</td>\n",
       "      <td>116939</td>\n",
       "      <td>2004-01-17</td>\n",
       "      <td>['30-minutes-or-less', 'time-to-make', 'main-i...</td>\n",
       "      <td>[365.3, 23.0, 25.0, 21.0, 54.0, 31.0, 9.0]</td>\n",
       "      <td>18</td>\n",
       "      <td>['for onions , tear a large piece of heavy dut...</td>\n",
       "      <td>what a wondewrful tasting hamburger to make fo...</td>\n",
       "      <td>['sweet onion', 'butter', 'dry mustard', 'hone...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14257</th>\n",
       "      <td>baked garlic corn on the cob</td>\n",
       "      <td>87650</td>\n",
       "      <td>35</td>\n",
       "      <td>30534</td>\n",
       "      <td>2004-03-28</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[255.3, 13.0, 22.0, 1.0, 12.0, 6.0, 15.0]</td>\n",
       "      <td>4</td>\n",
       "      <td>['preheat oven to 350f', 'add oil and garlic t...</td>\n",
       "      <td>corn on the cob baked with olive oil and garlic!</td>\n",
       "      <td>['olive oil', 'garlic', 'corn', 'salt and pepp...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25498</th>\n",
       "      <td>blue cheese potato cakes</td>\n",
       "      <td>93207</td>\n",
       "      <td>25</td>\n",
       "      <td>51011</td>\n",
       "      <td>2004-06-11</td>\n",
       "      <td>['30-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[159.7, 15.0, 5.0, 13.0, 7.0, 13.0, 4.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>['first , combine the potatoes , bread crumbs ...</td>\n",
       "      <td>i have found a new love... blue cheese! i have...</td>\n",
       "      <td>['mashed potatoes', 'panko breadcrumbs', 'blue...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183702</th>\n",
       "      <td>seitan and mushroom stroganoff  vegan</td>\n",
       "      <td>238251</td>\n",
       "      <td>40</td>\n",
       "      <td>385678</td>\n",
       "      <td>2007-07-02</td>\n",
       "      <td>['lactose', '60-minutes-or-less', 'time-to-mak...</td>\n",
       "      <td>[109.0, 9.0, 7.0, 31.0, 10.0, 3.0, 3.0]</td>\n",
       "      <td>12</td>\n",
       "      <td>['gravy: stir cornstarch and soy sauce togethe...</td>\n",
       "      <td>this is the best vegetarian or vegan stroganof...</td>\n",
       "      <td>['cornstarch', 'soy sauce', 'vegan chicken bro...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29587</th>\n",
       "      <td>broccoli with dijon sauce</td>\n",
       "      <td>351792</td>\n",
       "      <td>9</td>\n",
       "      <td>17803</td>\n",
       "      <td>2009-01-26</td>\n",
       "      <td>['15-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[63.8, 4.0, 8.0, 3.0, 9.0, 1.0, 2.0]</td>\n",
       "      <td>5</td>\n",
       "      <td>['simmer broccoli , covered , in water for 3 t...</td>\n",
       "      <td>i received a lot of side dish recipes from our...</td>\n",
       "      <td>['frozen broccoli florets', 'water', 'dijon mu...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126138</th>\n",
       "      <td>low carb peanut butter cup cheesecake squares</td>\n",
       "      <td>119728</td>\n",
       "      <td>140</td>\n",
       "      <td>134663</td>\n",
       "      <td>2005-04-29</td>\n",
       "      <td>['time-to-make', 'course', 'main-ingredient', ...</td>\n",
       "      <td>[396.8, 56.0, 29.0, 9.0, 19.0, 86.0, 4.0]</td>\n",
       "      <td>14</td>\n",
       "      <td>['preheat oven to 350f', 'for crust , in mediu...</td>\n",
       "      <td>there is quite a bit of cooling time involved,...</td>\n",
       "      <td>['peanut butter', 'splenda sugar substitute', ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name      id  minutes  \\\n",
       "166255              provencal tomato stuffed bell peppers   97801       45   \n",
       "6369    apple cabbage ravioli in a savory herb bacon b...  437558       40   \n",
       "154755                   paula deen s uncle bubba s wings  407501       35   \n",
       "176351               roasted tomatoes onions and zucchini   95881       35   \n",
       "206171                                sweet onion burgers   81346       25   \n",
       "...                                                   ...     ...      ...   \n",
       "14257                        baked garlic corn on the cob   87650       35   \n",
       "25498                            blue cheese potato cakes   93207       25   \n",
       "183702              seitan and mushroom stroganoff  vegan  238251       40   \n",
       "29587                           broccoli with dijon sauce  351792        9   \n",
       "126138      low carb peanut butter cup cheesecake squares  119728      140   \n",
       "\n",
       "        contributor_id   submitted  \\\n",
       "166255            4470  2004-08-14   \n",
       "6369           1329782  2010-09-17   \n",
       "154755          339260  2010-01-09   \n",
       "176351           52282  2004-07-19   \n",
       "206171          116939  2004-01-17   \n",
       "...                ...         ...   \n",
       "14257            30534  2004-03-28   \n",
       "25498            51011  2004-06-11   \n",
       "183702          385678  2007-07-02   \n",
       "29587            17803  2009-01-26   \n",
       "126138          134663  2005-04-29   \n",
       "\n",
       "                                                     tags  \\\n",
       "166255  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
       "6369    ['bacon', 'weeknight', '60-minutes-or-less', '...   \n",
       "154755  ['60-minutes-or-less', 'time-to-make', 'prepar...   \n",
       "176351  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
       "206171  ['30-minutes-or-less', 'time-to-make', 'main-i...   \n",
       "...                                                   ...   \n",
       "14257   ['60-minutes-or-less', 'time-to-make', 'course...   \n",
       "25498   ['30-minutes-or-less', 'time-to-make', 'course...   \n",
       "183702  ['lactose', '60-minutes-or-less', 'time-to-mak...   \n",
       "29587   ['15-minutes-or-less', 'time-to-make', 'course...   \n",
       "126138  ['time-to-make', 'course', 'main-ingredient', ...   \n",
       "\n",
       "                                             nutrition  n_steps  \\\n",
       "166255         [91.4, 10.0, 19.0, 12.0, 2.0, 4.0, 2.0]        7   \n",
       "6369        [314.2, 36.0, 28.0, 33.0, 23.0, 53.0, 4.0]       34   \n",
       "154755  [1310.1, 159.0, 8.0, 204.0, 170.0, 199.0, 1.0]        9   \n",
       "176351         [121.5, 16.0, 17.0, 0.0, 3.0, 7.0, 2.0]        6   \n",
       "206171      [365.3, 23.0, 25.0, 21.0, 54.0, 31.0, 9.0]       18   \n",
       "...                                                ...      ...   \n",
       "14257        [255.3, 13.0, 22.0, 1.0, 12.0, 6.0, 15.0]        4   \n",
       "25498         [159.7, 15.0, 5.0, 13.0, 7.0, 13.0, 4.0]        8   \n",
       "183702         [109.0, 9.0, 7.0, 31.0, 10.0, 3.0, 3.0]       12   \n",
       "29587             [63.8, 4.0, 8.0, 3.0, 9.0, 1.0, 2.0]        5   \n",
       "126138       [396.8, 56.0, 29.0, 9.0, 19.0, 86.0, 4.0]       14   \n",
       "\n",
       "                                                    steps  \\\n",
       "166255  ['preheat oven to 375f', 'lightly brush an 8\" ...   \n",
       "6369    ['apple cabbage filling -- in a large saute pa...   \n",
       "154755  ['prepare the wings: combine the hot sauce , c...   \n",
       "176351  ['preheat oven to 375f', 'arrange the zucchini...   \n",
       "206171  ['for onions , tear a large piece of heavy dut...   \n",
       "...                                                   ...   \n",
       "14257   ['preheat oven to 350f', 'add oil and garlic t...   \n",
       "25498   ['first , combine the potatoes , bread crumbs ...   \n",
       "183702  ['gravy: stir cornstarch and soy sauce togethe...   \n",
       "29587   ['simmer broccoli , covered , in water for 3 t...   \n",
       "126138  ['preheat oven to 350f', 'for crust , in mediu...   \n",
       "\n",
       "                                              description  \\\n",
       "166255  this recipe is from the cooking club of americ...   \n",
       "6369    these are so good and just reminds me of fall....   \n",
       "154755  paula's younger brother, earl \"bubba\" hiers, i...   \n",
       "176351  roasting vegetables brings out their sweetness...   \n",
       "206171  what a wondewrful tasting hamburger to make fo...   \n",
       "...                                                   ...   \n",
       "14257    corn on the cob baked with olive oil and garlic!   \n",
       "25498   i have found a new love... blue cheese! i have...   \n",
       "183702  this is the best vegetarian or vegan stroganof...   \n",
       "29587   i received a lot of side dish recipes from our...   \n",
       "126138  there is quite a bit of cooling time involved,...   \n",
       "\n",
       "                                              ingredients  n_ingredients  \n",
       "166255  ['olive oil', 'red peppers', 'roma tomatoes', ...              7  \n",
       "6369    ['green cabbage', 'onion', 'golden delicious a...             15  \n",
       "154755  ['hot sauce', 'cajun seasoning', 'cayenne pepp...              9  \n",
       "176351  ['zucchini', 'roma tomatoes', 'vidalia onion',...              7  \n",
       "206171  ['sweet onion', 'butter', 'dry mustard', 'hone...             11  \n",
       "...                                                   ...            ...  \n",
       "14257   ['olive oil', 'garlic', 'corn', 'salt and pepp...              4  \n",
       "25498   ['mashed potatoes', 'panko breadcrumbs', 'blue...              5  \n",
       "183702  ['cornstarch', 'soy sauce', 'vegan chicken bro...             13  \n",
       "29587   ['frozen broccoli florets', 'water', 'dijon mu...              6  \n",
       "126138  ['peanut butter', 'splenda sugar substitute', ...              9  \n",
       "\n",
       "[10000 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf1f1886-8a5c-42c0-97c6-43561ced1dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Set the appropriate device depending upon your hardware. \n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65696451-317e-4e86-8bcd-5fb27120018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = recipes_df['name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93d5db3f-e718-4c8f-bbae-26efe1290950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TokenizerWrapper():\n",
    "    \"\"\"\n",
    "    A wrapper class for the AutoTokenizer to handle tokenization and provide\n",
    "    custom token-vocabulary mappings. T\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"bert-base-cased\"):        \n",
    "        \"\"\"\n",
    "        Initializes the TokenizerWrapper with a specified model.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # The wrapper class creates a token-to-vocab mapping\n",
    "        # Let's keep the ids corresponding to special tokens.  \n",
    "        # 0 --> [PAD], 101 --> [CLS], 102 --> [SEP]  \n",
    "        self.token_id_to_vocab_id = {0: 0, 101: 1, 102: 2}\n",
    "        self.vocab_id_to_token_id = {0: 0, 1: 101, 2:102}\n",
    "        self.vocab_id = len(self.vocab_id_to_token_id)\n",
    "        self.padding_len = None \n",
    "\n",
    "    def build_dictionary(self, list_of_recipes: list):\n",
    "        \"\"\"\n",
    "        Processes a list of captions to build and update the vocabulary based on the tokens found in the captions.\n",
    "        This function also finds the maximum length of the tokenized captions to set the padding length.\n",
    "    \n",
    "        \"\"\"\n",
    "        # Tokenize all recipes to find the unique tokens and the maximum length\n",
    "        tokenized_outputs = self.tokenizer(list_of_recipes, add_special_tokens=False)\n",
    "        all_token_ids = set(token for sublist in tokenized_outputs.input_ids for token in sublist)\n",
    "    \n",
    "        # Update the custom token-vocabulary mapping\n",
    "        for token_id in all_token_ids:\n",
    "            if token_id not in self.token_id_to_vocab_id:\n",
    "                self.token_id_to_vocab_id[token_id] = self.vocab_id\n",
    "                self.vocab_id_to_token_id[self.vocab_id] = token_id\n",
    "                self.vocab_id += 1\n",
    "    \n",
    "        # Set the padding length to the length of the longest tokenized recipe\n",
    "        self.padding_len = max(len(tokens) for tokens in tokenized_outputs.input_ids)\n",
    "    \n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        \"\"\"\n",
    "        Returns the size of the custom vocabulary.\n",
    "        \"\"\"\n",
    "        assert len(self.token_id_to_vocab_id) == len(self.vocab_id_to_token_id)\n",
    "        return len(self.token_id_to_vocab_id)\n",
    "\n",
    "\n",
    "    def tokenize(self, text: str) -> list:\n",
    "        \"\"\"\n",
    "        Tokenizes a text string into custom vocabulary IDs, using the built dictionary. \n",
    "        Requires the dictionary to be built first.\n",
    "    \n",
    "        Parameters:\n",
    "            text (str): The text to tokenize.\n",
    "    \n",
    "        Returns:\n",
    "            list of int: A list of custom vocabulary IDs corresponding to the text tokens.\n",
    "        \"\"\"\n",
    "        assert self.padding_len is not None, 'Call build_dictionary first.'\n",
    "        # Tokenize the text with the maximum length set to the previously found maximum padding length\n",
    "        \n",
    "        tokenized_output = self.tokenizer(text, add_special_tokens=False, padding='max_length', max_length=self.padding_len, truncation=True)\n",
    "        return [self.token_id_to_vocab_id.get(token_id, 0)  # Default to [PAD] if token_id is not found\n",
    "                for token_id in tokenized_output.input_ids]\n",
    "        \n",
    "    \n",
    "    def decode(self, vocab_list: list) -> str:\n",
    "        \"\"\"\n",
    "        Decodes a list of custom vocabulary IDs back into the original text string.\n",
    "\n",
    "        Parameters:\n",
    "            vocab_list (list of int): A list of custom vocabulary IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded text string.\n",
    "        \"\"\"        \n",
    "        token_list = [self.vocab_id_to_token_id[vocab_id] for vocab_id in vocab_list]\n",
    "        decoded_string = self.tokenizer.decode(token_list, skip_special_tokens=True)\n",
    "        return decoded_string.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab21b95b-135d-4b98-ad63-7e48e9b89ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dictionary for our tokenizer  \n",
    "from tqdm import tqdm, trange \n",
    "tokenizer_wrapper = TokenizerWrapper()\n",
    "tokenizer_wrapper.build_dictionary(recipes_df[\"name\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57a5e6de-63b0-438f-be17-bb8b61e3c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: weight watchers raspberry zinger cake\n",
      "Tokens: [1451, 1443, 658, 82, 1120, 783, 3010, 90, 3421, 1359, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded caption: weight watchers raspberry zinger cake\n"
     ]
    }
   ],
   "source": [
    "recipe_tokens = tokenizer_wrapper.tokenize(recipes_df['name'].iloc[10])\n",
    "decoeded_recipe = tokenizer_wrapper.decode(recipe_tokens)\n",
    "print('Caption:', recipes_df['name'].iloc[10])\n",
    "print('Tokens:', recipe_tokens)\n",
    "print('Decoded caption:', decoeded_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd9a66e-40f9-4dd4-a581-b7f7804ea1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3603"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer_wrapper.get_vocab_size()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405676b-1c97-4c37-a71e-a07866a3956b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dbb5e-5213-416d-92f1-1a838316b28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d127ebca-f8f7-4397-ac11-33336966e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(data_df, tokenizer_wrapper):    \n",
    "    dataset = []\n",
    "    for row_id in trange(len(data_df)):\n",
    "        reicpe_tokens = torch.tensor(tokenizer_wrapper.tokenize(data_df['name'].iloc[row_id]))  # SOLUTION\n",
    "        dataset.append({'token': reicpe_tokens})\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52cda2-aec7-4e96-b653-b0115b86f646",
   "metadata": {},
   "source": [
    "Let's create train and test datasets by calling `build_data` on train and test splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54299ff0-a919-4453-84c7-c3b4b417fd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/8000 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|████████████████████████████████████| 8000/8000 [00:00<00:00, 21730.57it/s]\n",
      "100%|█████████████████████████████████████| 2000/2000 [00:00<00:00, 9832.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(recipes_df, test_size=0.2, random_state=123)\n",
    "train_data = build_data(train_df, tokenizer_wrapper)\n",
    "test_data = build_data(test_df, tokenizer_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "140dabb0-d937-49c8-aa19-11cb67b4dcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocab size is 3603.\n"
     ]
    }
   ],
   "source": [
    "# Get the dimension of the image feature\n",
    "vocab_size = tokenizer_wrapper.get_vocab_size()\n",
    "print(f'The vocab size is {vocab_size}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3afe702f-0349-44ce-82f1-4f4f5d4f5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchDataset():\n",
    "    def __init__(self, data, pad_vocab_id=0):\n",
    "        self.data = data\n",
    "        self.pad_tensor = torch.tensor([pad_vocab_id])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        # Retrieve the next sequence of tokens from the current index\n",
    "        # by excluding the first token of the current sequence and appending a padding token at the end.        \n",
    "        target_sequence = torch.cat([self.data[ind]['token'][1:], self.pad_tensor]) # SOLUTION\n",
    "        return self.data[ind]['token'], target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d993084-a56d-45fe-8438-48d6f88c6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PytorchDataset(train_data)\n",
    "test_dataset = PytorchDataset(test_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f735587-d16d-4e5d-aaf9-0ba8bf29b533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 23])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's get a batch of data from DataLoader\n",
    "train_text, train_target = next(iter(train_dataloader))\n",
    "train_text = train_text.to(device)\n",
    "train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "103cff6c-b65c-4c5f-bf05-b1ddc3efbe9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  66, 3256, 1840,  797,  438,  444, 2589,  392, 2312,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a019a4e-38b1-4d93-9a06-9b9da5c9faed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3256, 1840,  797,  438,  444, 2589,  392, 2312,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bd67dac-e5e7-4a68-9b05-3f901673b1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'balsamic soy glazed chicken wings'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_wrapper.decode(train_text[11].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "149533c9-778d-443e-8080-723c95bfb8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##alsamic soy glazed chicken wings'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_wrapper.decode(train_target[11].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02094388-1b47-4f8e-9218-8167425fe48d",
   "metadata": {},
   "source": [
    "This is called autoregressive training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "426427fd-d3b9-4484-b3da-db0cfedaa6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PositionalEncoding model is already defined for you.  Do not change this class.\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f03c05e-435e-45a4-a86f-7944414edfee",
   "metadata": {},
   "source": [
    "**PyTorch [TransformerDecoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html)**\n",
    "\n",
    "- Encoder decoder models (Sequence to sequence models) \n",
    "- Decoder only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9be74fc-4b38-4f60-bc2b-45d5818314d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeGenerator(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, num_layers, vocab_size, device, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the RecipeGenerator which uses a transformer decoder architecture\n",
    "        for generating image captions.\n",
    "\n",
    "        Parameters:\n",
    "            d_model (int): The number of expected features in the encoder/decoder inputs.\n",
    "            n_heads (int): The number of heads in the multiheadattention models.\n",
    "            num_layers (int): The number of sub-decoder-layers in the transformer.\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "            device (torch.device): The device on which the model will be trained.\n",
    "            dropout (float): The dropout value used in PositionalEncoding and TransformerDecoderLayer.\n",
    "        \"\"\"        \n",
    "        super(RecipeGenerator, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "        # Positional Encoding to add position information to input embeddings\n",
    "        self.pos_encoding = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
    "\n",
    "        self.TransformerDecoder = nn.TransformerDecoder(\n",
    "            decoder_layer=nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads, dropout=dropout), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Embedding layer for converting input text tokens into vectors\n",
    "        self.text_embedding = nn.Embedding(vocab_size , d_model)\n",
    "\n",
    "        # Final linear layer to map the output of the transformer decoder to vocabulary size        \n",
    "        self.linear_layer = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # END SOLUTION\n",
    "\n",
    "        # Initialize the weights of the model\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights of the model to small random values.\n",
    "        \"\"\"\n",
    "        initrange = 0.1\n",
    "        # BEGIN SOLUTION\n",
    "        self.text_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear_layer.bias.data.zero_()\n",
    "        self.linear_layer.weight.data.uniform_(-initrange, initrange)\n",
    "        # END SOLUTION\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Get the embeded input\n",
    "        encoded_text = self.embed_text(text)        \n",
    "\n",
    "        # Get transformer output\n",
    "        transformer_output = self.decode(encoded_text)\n",
    "\n",
    "        # Final linear layer (unembedding layer)\n",
    "        return self.linear_layer(transformer_output)\n",
    "    \n",
    "    def embed_text(self, text):\n",
    "        embedding = self.text_embedding(text) * math.sqrt(self.d_model)\n",
    "        return self.pos_encoding(embedding.permute(1, 0, 2))\n",
    "    \n",
    "    def decode(self, encoded_text):\n",
    "        # Get the length of the sequences to be decoeded. This is needed to generate the causal masks\n",
    "        seq_len = encoded_text.size(0)\n",
    "        causal_mask = self.generate_mask(seq_len)\n",
    "        dummy_memory = torch.zeros_like(encoded_text)\n",
    "        return self.TransformerDecoder(tgt=encoded_text, memory=dummy_memory, tgt_mask=causal_mask)\n",
    "    \n",
    "    def generate_mask(self, size):\n",
    "        mask = torch.triu(torch.ones(size, size, device=self.device), 1)\n",
    "        return mask.float().masked_fill(mask == 1, float('-inf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fb26974-ebd5-45f0-95ec-9389cfa1ef8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "size = 10\n",
    "mask = torch.triu(torch.ones(size, size), 1)\n",
    "mask.float().masked_fill(mask == 1, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aec3e665-1988-4c7a-8337-41a81d0a792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try your model. \n",
    "# Define the hyperparameters and initalize the model. Feel free to change these hyperparameters. \n",
    "d_model = 256 \n",
    "n_heads = 4\n",
    "num_layers = 8\n",
    "model = RecipeGenerator(d_model=d_model, n_heads=n_heads, num_layers=num_layers, vocab_size=vocab_size, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca2660f0-5eae-463e-9ea2-48f090ac5bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 924, 1971, 1396,  ...,    0,    0,    0],\n",
       "        [ 610, 1944, 1539,  ...,    0,    0,    0],\n",
       "        [  78,  586,  612,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [3133, 1681, 2202,  ...,    0,    0,    0],\n",
       "        [  80, 1543, 2620,  ...,    0,    0,    0],\n",
       "        [2219, 2861, 2476,  ...,    0,    0,    0]], device='mps:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e55dbf9a-f866-40e7-a934-a4a48ef0de90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 64, 3603])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass inputs to your model\n",
    "output = model(train_text)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39544a5d-b332-4cff-807c-04d3a2eb08ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3603"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76417d4a-6e75-4c60-9fa7-433798366f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 23])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1c1cc1a-765f-4ed4-89ff-d900cd5a25a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 64, 3603])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed75fdb-733e-4b83-a5d3-43345551cef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52a533ea-631e-4fc6-a24a-14ec898fff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, train_dataloader, test_dataloader, epochs=5, patience=5, clip_norm=1.0):\n",
    "    train_losses, test_losses = [], []\n",
    "    consec_increases, verbose = 0, True\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for train_text, target_seq in train_dataloader:\n",
    "            train_text, target_seq = train_text.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_text).permute(1, 2, 0)  # Ensure output is in correct shape for loss calculation\n",
    "            loss = criterion(output, target_seq)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for test_text, target_seq in test_dataloader:\n",
    "                test_text, target_seq = test_text.to(device), target_seq.to(device)\n",
    "                output = model(test_text).permute(1, 2, 0)\n",
    "                test_loss += criterion(output, target_seq).item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_dataloader))\n",
    "        test_losses.append(test_loss / len(test_dataloader))\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}: Train Loss {train_losses[-1]:.4f}, Test Loss {test_losses[-1]:.4f}\")\n",
    "\n",
    "        if epoch > 0 and test_losses[-1] > test_losses[-2] * (1 + 1e-5):\n",
    "            consec_increases += 1\n",
    "        else:\n",
    "            consec_increases = 0\n",
    "\n",
    "        if consec_increases >= patience:\n",
    "            print(f\"Stopped early at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    return train_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "895144a0-8d32-4841-b284-6f059a8f78a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 7.4726, Test Loss 6.9161\n",
      "Epoch 2: Train Loss 6.4946, Test Loss 6.0254\n",
      "Epoch 3: Train Loss 5.7545, Test Loss 5.4912\n",
      "Epoch 4: Train Loss 5.2850, Test Loss 5.1488\n",
      "Epoch 5: Train Loss 4.9662, Test Loss 4.9200\n",
      "Epoch 6: Train Loss 4.7159, Test Loss 4.7522\n",
      "Epoch 7: Train Loss 4.5264, Test Loss 4.6318\n",
      "Epoch 8: Train Loss 4.3603, Test Loss 4.5241\n",
      "Epoch 9: Train Loss 4.2203, Test Loss 4.4399\n",
      "Epoch 10: Train Loss 4.0927, Test Loss 4.3667\n",
      "Epoch 11: Train Loss 3.9791, Test Loss 4.3158\n",
      "Epoch 12: Train Loss 3.8782, Test Loss 4.2743\n",
      "Epoch 13: Train Loss 3.7861, Test Loss 4.2237\n",
      "Epoch 14: Train Loss 3.7068, Test Loss 4.1924\n",
      "Epoch 15: Train Loss 3.6249, Test Loss 4.1640\n",
      "Epoch 16: Train Loss 3.5487, Test Loss 4.1357\n",
      "Epoch 17: Train Loss 3.4737, Test Loss 4.1140\n",
      "Epoch 18: Train Loss 3.4064, Test Loss 4.0946\n",
      "Epoch 19: Train Loss 3.3407, Test Loss 4.0771\n",
      "Epoch 20: Train Loss 3.2798, Test Loss 4.0684\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer and the loss function. Feel free to change the hyperparameters. \n",
    "\n",
    "num_epoch = 20\n",
    "clip_norm = 1.0\n",
    "lr = 5e-5\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0) # Ignore the padding index\n",
    "train_losses, test_losses = trainer(model, criterion, optimizer,train_dataloader, test_dataloader, epochs= num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6eb1aa4e-8584-4dc3-893c-c35a53b3b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, device, max_recipe_length=39, seed = 10, end_vocab=2):\n",
    "    \"\"\"\n",
    "    Generates a recipe for an image using the specified model and device.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The trained model used for generating captions.\n",
    "        device (torch.device): The device (e.g., CPU or GPU) to which tensors will be sent for model execution.\n",
    "        max_caption_length (int, optional): The maximum length of the generated caption. Defaults to 100.\n",
    "        start_vocab (int, optional): The vocabulary index used to signify the start of a caption. Defaults to 1.\n",
    "        end_vocab (int, optional): The vocabulary index used to signify the end of a caption. Defaults to 2.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An array containing the sequence of vocabulary indices representing the generated caption.\n",
    "        \n",
    "    \"\"\"    \n",
    "    context = torch.tensor([[seed]]).to(device)\n",
    "    for _ in range(max_recipe_length):\n",
    "        logits = model(context)[-1]\n",
    "        probabilities = torch.softmax(logits, dim=-1).flatten(start_dim=1)\n",
    "        next_vocab = torch.multinomial(probabilities, num_samples=1)\n",
    "        context = torch.cat([context, next_vocab], dim=1)\n",
    "        if next_vocab.item() == end_vocab:\n",
    "            break\n",
    "    return context.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "229871ab-e2dd-460e-9f00-3a8d74f27f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = generate_recipe(model, device, max_recipe_length=20, seed = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f308b243-b184-45cb-91e2-cd22f67e4d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingredientrbet with smoked salmon and lemon garlic sauce for frostingcrambs in syrup asparagu'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_recipe = tokenizer_wrapper.decode(recipe)\n",
    "generated_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb6fd9-8d11-4d52-854e-7ce07feb88b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
