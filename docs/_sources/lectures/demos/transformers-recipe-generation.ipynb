{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f8a77f",
   "metadata": {},
   "source": [
    "# Recipe Generation using Transformers \n",
    "\n",
    "This notebook demonstrates how to build a Transformer-based model to generate recipe titles. You'll learn about tokenization, preparing datasets, building and training the model, and generating new text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc6546-ef29-4503-8bb0-7a5e7a92eb07",
   "metadata": {},
   "source": [
    "![](../img/eva-guacamole-bowl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fecb8b-833e-4f1d-979c-29de9ebc3e37",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82734cd-af1f-481d-8609-5519ffaa8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from urllib.request import urlopen\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc90cc6-d9bc-4747-bc91-ed5dcc065c47",
   "metadata": {},
   "source": [
    "This is a demo for recipe generation using PyTorch and Transformers. \n",
    "For the purpose of this demo, we'll sample 10_000 recipe titles from the corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81df36ee-1880-4a87-8e58-12dd647dd011",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765fad1d-0cc9-4d88-a4d8-357a767fc072",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_recipes_df = pd.read_csv(\"../data/RAW_recipes.csv\")\n",
    "orig_recipes_df = orig_recipes_df.dropna()\n",
    "recipes_df = orig_recipes_df.sample(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7839bff9-07e0-4e3d-be61-2aba64b9fefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>tags</th>\n",
       "      <th>nutrition</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>steps</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>n_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217580</th>\n",
       "      <td>tuna stuff</td>\n",
       "      <td>18264</td>\n",
       "      <td>20</td>\n",
       "      <td>29110</td>\n",
       "      <td>2002-01-28</td>\n",
       "      <td>['30-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[280.2, 14.0, 23.0, 25.0, 29.0, 18.0, 11.0]</td>\n",
       "      <td>7</td>\n",
       "      <td>['boil noodles', 'drain and return to pan', 'a...</td>\n",
       "      <td>when i first saw this i thought it sounded so ...</td>\n",
       "      <td>['macaroni and cheese mix', 'corn', 'tuna', 'b...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190826</th>\n",
       "      <td>soetkoekies   sweet wine and spice south afric...</td>\n",
       "      <td>309794</td>\n",
       "      <td>30</td>\n",
       "      <td>539686</td>\n",
       "      <td>2008-06-17</td>\n",
       "      <td>['30-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[99.0, 4.0, 36.0, 3.0, 3.0, 5.0, 5.0]</td>\n",
       "      <td>16</td>\n",
       "      <td>['preheat the oven to 350 degrees', 'spray two...</td>\n",
       "      <td>i got this from a very old (1970) african cook...</td>\n",
       "      <td>['butter', 'all-purpose flour', 'cooking spray...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21969</th>\n",
       "      <td>berry fruit dip</td>\n",
       "      <td>181343</td>\n",
       "      <td>65</td>\n",
       "      <td>341355</td>\n",
       "      <td>2006-08-10</td>\n",
       "      <td>['time-to-make', 'course', 'main-ingredient', ...</td>\n",
       "      <td>[134.0, 0.0, 75.0, 4.0, 15.0, 1.0, 8.0]</td>\n",
       "      <td>3</td>\n",
       "      <td>['combine yogurt , orange rind , orange juice ...</td>\n",
       "      <td>berries, orange, and a touch of almond flavori...</td>\n",
       "      <td>['non-fat strawberry yogurt', 'orange rind', '...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189302</th>\n",
       "      <td>slow cooked  texas style beef brisket</td>\n",
       "      <td>485907</td>\n",
       "      <td>1515</td>\n",
       "      <td>4439</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>['time-to-make', 'course', 'main-ingredient', ...</td>\n",
       "      <td>[316.3, 20.0, 31.0, 17.0, 76.0, 23.0, 2.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>['place the beef brisket in a large slow cooke...</td>\n",
       "      <td>this is a unique method of making lush, succul...</td>\n",
       "      <td>['beef brisket', 'strong black coffee', 'ketch...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215708</th>\n",
       "      <td>tortellini salad and basil dressing</td>\n",
       "      <td>93075</td>\n",
       "      <td>160</td>\n",
       "      <td>133174</td>\n",
       "      <td>2004-06-10</td>\n",
       "      <td>['time-to-make', 'course', 'main-ingredient', ...</td>\n",
       "      <td>[246.2, 7.0, 19.0, 10.0, 23.0, 11.0, 13.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>['in a small bowl whisk together basil , pecti...</td>\n",
       "      <td>this salad is so pretty. perfect for a ladies ...</td>\n",
       "      <td>['fresh basil', 'powdered fruit pectin', 'dijo...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127459</th>\n",
       "      <td>lucky leprechaun smoothie</td>\n",
       "      <td>405618</td>\n",
       "      <td>5</td>\n",
       "      <td>628076</td>\n",
       "      <td>2009-12-29</td>\n",
       "      <td>['15-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[155.4, 4.0, 93.0, 5.0, 20.0, 8.0, 7.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>['combine all ingredients in a shaking contain...</td>\n",
       "      <td>this recipe came from studio 5 - who could res...</td>\n",
       "      <td>['low-fat vanilla yogurt', 'low-fat milk', 'in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86899</th>\n",
       "      <td>fresh fruit pudding milk mixer</td>\n",
       "      <td>407115</td>\n",
       "      <td>15</td>\n",
       "      <td>57042</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>['weeknight', '15-minutes-or-less', 'time-to-m...</td>\n",
       "      <td>[79.3, 3.0, 34.0, 2.0, 8.0, 7.0, 3.0]</td>\n",
       "      <td>4</td>\n",
       "      <td>['place all ingredients in blender container',...</td>\n",
       "      <td>i found this chemung county dairy princess rec...</td>\n",
       "      <td>['2% low-fat milk', 'vanilla flavor instant pu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111516</th>\n",
       "      <td>inside out pizza dilla margerita</td>\n",
       "      <td>205925</td>\n",
       "      <td>45</td>\n",
       "      <td>37779</td>\n",
       "      <td>2007-01-17</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[583.0, 51.0, 20.0, 47.0, 62.0, 82.0, 12.0]</td>\n",
       "      <td>16</td>\n",
       "      <td>['heat a skillet over medium heat', 'add olive...</td>\n",
       "      <td>rachael ray</td>\n",
       "      <td>['extra virgin olive oil', 'garlic cloves', 'r...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112585</th>\n",
       "      <td>italian eggplant  aubergine  crepes</td>\n",
       "      <td>21508</td>\n",
       "      <td>120</td>\n",
       "      <td>15718</td>\n",
       "      <td>2002-03-05</td>\n",
       "      <td>['weeknight', 'time-to-make', 'course', 'main-...</td>\n",
       "      <td>[228.7, 18.0, 27.0, 17.0, 22.0, 23.0, 6.0]</td>\n",
       "      <td>21</td>\n",
       "      <td>['cut eggplant lengthwise into thin slices', '...</td>\n",
       "      <td>delicious italian/ mediterranean-style eggplan...</td>\n",
       "      <td>['eggplant', 'seasoned flour', 'olive oil', 'p...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133045</th>\n",
       "      <td>mediterranean herb baked chicken</td>\n",
       "      <td>112720</td>\n",
       "      <td>540</td>\n",
       "      <td>73836</td>\n",
       "      <td>2005-03-05</td>\n",
       "      <td>['time-to-make', 'course', 'main-ingredient', ...</td>\n",
       "      <td>[467.8, 25.0, 12.0, 30.0, 138.0, 19.0, 2.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>['combine the parsley , cilantro , garlic , cu...</td>\n",
       "      <td>the selection of spices used in this dish crea...</td>\n",
       "      <td>['fresh parsley', 'fresh cilantro', 'fresh cil...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name      id  minutes  \\\n",
       "217580                                         tuna stuff   18264       20   \n",
       "190826  soetkoekies   sweet wine and spice south afric...  309794       30   \n",
       "21969                                     berry fruit dip  181343       65   \n",
       "189302              slow cooked  texas style beef brisket  485907     1515   \n",
       "215708                tortellini salad and basil dressing   93075      160   \n",
       "...                                                   ...     ...      ...   \n",
       "127459                          lucky leprechaun smoothie  405618        5   \n",
       "86899                      fresh fruit pudding milk mixer  407115       15   \n",
       "111516                   inside out pizza dilla margerita  205925       45   \n",
       "112585                italian eggplant  aubergine  crepes   21508      120   \n",
       "133045                   mediterranean herb baked chicken  112720      540   \n",
       "\n",
       "        contributor_id   submitted  \\\n",
       "217580           29110  2002-01-28   \n",
       "190826          539686  2008-06-17   \n",
       "21969           341355  2006-08-10   \n",
       "189302            4439  2012-08-24   \n",
       "215708          133174  2004-06-10   \n",
       "...                ...         ...   \n",
       "127459          628076  2009-12-29   \n",
       "86899            57042  2010-01-05   \n",
       "111516           37779  2007-01-17   \n",
       "112585           15718  2002-03-05   \n",
       "133045           73836  2005-03-05   \n",
       "\n",
       "                                                     tags  \\\n",
       "217580  ['30-minutes-or-less', 'time-to-make', 'course...   \n",
       "190826  ['30-minutes-or-less', 'time-to-make', 'course...   \n",
       "21969   ['time-to-make', 'course', 'main-ingredient', ...   \n",
       "189302  ['time-to-make', 'course', 'main-ingredient', ...   \n",
       "215708  ['time-to-make', 'course', 'main-ingredient', ...   \n",
       "...                                                   ...   \n",
       "127459  ['15-minutes-or-less', 'time-to-make', 'course...   \n",
       "86899   ['weeknight', '15-minutes-or-less', 'time-to-m...   \n",
       "111516  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
       "112585  ['weeknight', 'time-to-make', 'course', 'main-...   \n",
       "133045  ['time-to-make', 'course', 'main-ingredient', ...   \n",
       "\n",
       "                                          nutrition  n_steps  \\\n",
       "217580  [280.2, 14.0, 23.0, 25.0, 29.0, 18.0, 11.0]        7   \n",
       "190826        [99.0, 4.0, 36.0, 3.0, 3.0, 5.0, 5.0]       16   \n",
       "21969       [134.0, 0.0, 75.0, 4.0, 15.0, 1.0, 8.0]        3   \n",
       "189302   [316.3, 20.0, 31.0, 17.0, 76.0, 23.0, 2.0]        8   \n",
       "215708   [246.2, 7.0, 19.0, 10.0, 23.0, 11.0, 13.0]       11   \n",
       "...                                             ...      ...   \n",
       "127459      [155.4, 4.0, 93.0, 5.0, 20.0, 8.0, 7.0]        1   \n",
       "86899         [79.3, 3.0, 34.0, 2.0, 8.0, 7.0, 3.0]        4   \n",
       "111516  [583.0, 51.0, 20.0, 47.0, 62.0, 82.0, 12.0]       16   \n",
       "112585   [228.7, 18.0, 27.0, 17.0, 22.0, 23.0, 6.0]       21   \n",
       "133045  [467.8, 25.0, 12.0, 30.0, 138.0, 19.0, 2.0]        9   \n",
       "\n",
       "                                                    steps  \\\n",
       "217580  ['boil noodles', 'drain and return to pan', 'a...   \n",
       "190826  ['preheat the oven to 350 degrees', 'spray two...   \n",
       "21969   ['combine yogurt , orange rind , orange juice ...   \n",
       "189302  ['place the beef brisket in a large slow cooke...   \n",
       "215708  ['in a small bowl whisk together basil , pecti...   \n",
       "...                                                   ...   \n",
       "127459  ['combine all ingredients in a shaking contain...   \n",
       "86899   ['place all ingredients in blender container',...   \n",
       "111516  ['heat a skillet over medium heat', 'add olive...   \n",
       "112585  ['cut eggplant lengthwise into thin slices', '...   \n",
       "133045  ['combine the parsley , cilantro , garlic , cu...   \n",
       "\n",
       "                                              description  \\\n",
       "217580  when i first saw this i thought it sounded so ...   \n",
       "190826  i got this from a very old (1970) african cook...   \n",
       "21969   berries, orange, and a touch of almond flavori...   \n",
       "189302  this is a unique method of making lush, succul...   \n",
       "215708  this salad is so pretty. perfect for a ladies ...   \n",
       "...                                                   ...   \n",
       "127459  this recipe came from studio 5 - who could res...   \n",
       "86899   i found this chemung county dairy princess rec...   \n",
       "111516                                        rachael ray   \n",
       "112585  delicious italian/ mediterranean-style eggplan...   \n",
       "133045  the selection of spices used in this dish crea...   \n",
       "\n",
       "                                              ingredients  n_ingredients  \n",
       "217580  ['macaroni and cheese mix', 'corn', 'tuna', 'b...              6  \n",
       "190826  ['butter', 'all-purpose flour', 'cooking spray...             14  \n",
       "21969   ['non-fat strawberry yogurt', 'orange rind', '...              4  \n",
       "189302  ['beef brisket', 'strong black coffee', 'ketch...              8  \n",
       "215708  ['fresh basil', 'powdered fruit pectin', 'dijo...             15  \n",
       "...                                                   ...            ...  \n",
       "127459  ['low-fat vanilla yogurt', 'low-fat milk', 'in...              4  \n",
       "86899   ['2% low-fat milk', 'vanilla flavor instant pu...              4  \n",
       "111516  ['extra virgin olive oil', 'garlic cloves', 'r...             10  \n",
       "112585  ['eggplant', 'seasoned flour', 'olive oil', 'p...             22  \n",
       "133045  ['fresh parsley', 'fresh cilantro', 'fresh cil...             16  \n",
       "\n",
       "[10000 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf1f1886-8a5c-42c0-97c6-43561ced1dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Set the appropriate device depending upon your hardware. \n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65696451-317e-4e86-8bcd-5fb27120018c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tuna stuff',\n",
       " 'soetkoekies   sweet wine and spice south african cookies',\n",
       " 'berry fruit dip',\n",
       " 'slow cooked  texas style beef brisket',\n",
       " 'tortellini salad and basil dressing',\n",
       " 'brussels sprouts and carrots',\n",
       " 'camarones en chile salsa   shrimp in chili gravy',\n",
       " 'sirloin burgers with blue cheese mayo and sherry vidalia onions',\n",
       " 'turnips and greens',\n",
       " 'bacon wrapped parmesan breadsticks']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes = recipes_df['name'].tolist()\n",
    "recipes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc1084-b0ee-49a7-95ae-d99c0ed432b4",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59990e90",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Let's start with tokenization. \n",
    "\n",
    "- We create a tokenizer wrapper to convert recipe names into tokens using a pre-trained language model (like BERT) that knows lots of words and subwords. But for our specific dataset (say, a bunch of recipe descriptions), we only need a much smaller dictionary, just the words (tokens) that actually show up in our dataset.\n",
    "\n",
    "So this code helps us:\n",
    "- Use the tokenizer from a big pre-trained model.\n",
    "  \n",
    "- Go through our dataset and extract just the tokens we need.\n",
    "- Build a mini vocabulary just for our data.\n",
    "- Be able to tokenize and decode texts using this mini vocab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc22e44f-937b-450f-83ce-27855c41eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tqdm import trange\n",
    "\n",
    "class TokenizerWrapper:\n",
    "    \"\"\"Wraps AutoTokenizer with a custom vocabulary mapping.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"bert-base-cased\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Initialize mappings with special tokens: [PAD] -> 0, [CLS] -> 1, [SEP] -> 2\n",
    "        self.token_id_to_vocab_id = {0: 0, 101: 1, 102: 2}\n",
    "        self.vocab_id_to_token_id = {0: 0, 1: 101, 2: 102}\n",
    "        \n",
    "        self.vocab_id = 3  # Start after special tokens\n",
    "        self.padding_len = None\n",
    "\n",
    "    def build_dictionary(self, recipes: list[str]):\n",
    "        \"\"\"Builds vocabulary from a list of recipes and sets padding length.\"\"\"\n",
    "        tokenized = self.tokenizer(recipes, padding='longest').input_ids\n",
    "        self.padding_len = len(tokenized[0])\n",
    "\n",
    "        for tokens in tokenized:\n",
    "            for token_id in tokens:\n",
    "                if token_id not in self.token_id_to_vocab_id:\n",
    "                    self.token_id_to_vocab_id[token_id] = self.vocab_id\n",
    "                    self.vocab_id_to_token_id[self.vocab_id] = token_id\n",
    "                    self.vocab_id += 1\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        \"\"\"Returns the size of the custom vocabulary.\"\"\"\n",
    "        assert len(self.token_id_to_vocab_id) == len(self.vocab_id_to_token_id)\n",
    "        return self.vocab_id\n",
    "\n",
    "    def tokenize(self, text: str) -> list[int]:\n",
    "        \"\"\"Tokenizes text using custom vocabulary (requires build_dictionary first).\"\"\"\n",
    "        assert self.padding_len is not None, \"Call build_dictionary() before tokenizing.\"\n",
    "        token_ids = self.tokenizer(text, padding='max_length', max_length=self.padding_len).input_ids\n",
    "        return [self.token_id_to_vocab_id[token_id] for token_id in token_ids]\n",
    "\n",
    "    def decode(self, vocab_ids: list[int]) -> str:\n",
    "        \"\"\"Decodes a list of custom vocab IDs into a string.\"\"\"\n",
    "        token_ids = [self.vocab_id_to_token_id[vocab_id] for vocab_id in vocab_ids]\n",
    "        # decoded_string = self.tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "        decoded_string = self.tokenizer.decode(token_ids, skip_special_tokens=False)\n",
    "        return decoded_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab21b95b-135d-4b98-ad63-7e48e9b89ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dictionary for our tokenizer  \n",
    "from tqdm import tqdm, trange \n",
    "tokenizer_wrapper = TokenizerWrapper()\n",
    "tokenizer_wrapper.build_dictionary(recipes_df[\"name\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57a5e6de-63b0-438f-be17-bb8b61e3c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe: roast teriyaki broccoli\n",
      "Tokens: [1, 90, 91, 28, 92, 93, 33, 94, 95, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded recipe: [CLS] roast teriyaki broccoli [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "recipe_tokens = tokenizer_wrapper.tokenize(recipes_df['name'].iloc[10])\n",
    "decoeded_recipe = tokenizer_wrapper.decode(recipe_tokens)\n",
    "print('Recipe:', recipes_df['name'].iloc[10])\n",
    "print('Tokens:', recipe_tokens)\n",
    "print('Decoded recipe:', decoeded_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dd9a66e-40f9-4dd4-a581-b7f7804ea1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3699"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer_wrapper.get_vocab_size()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffea745-80a7-4f0e-806a-d5bbb86ee5f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e793f-7632-41ce-b988-d5479d760fb0",
   "metadata": {},
   "source": [
    "- Shouldn't we just have a few meaningful indices above? What's going on? \n",
    "  \n",
    "- Why might we want to build a smaller custom vocabulary from our dataset instead of using the full vocabulary from a large pre-trained model?\n",
    "  \n",
    "- What do you think the impact would be on memory usage?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba0c3c-2356-4ee6-a80d-0808798e7962",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc63cc68",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "We split the dataset into training and test sets and convert each recipe name into a token sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d127ebca-f8f7-4397-ac11-33336966e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(data_df, tokenizer_wrapper):    \n",
    "    dataset = []\n",
    "    for row_id in trange(len(data_df)):\n",
    "        reicpe_tokens = torch.tensor(tokenizer_wrapper.tokenize(data_df['name'].iloc[row_id]))  \n",
    "        dataset.append({'token': reicpe_tokens})\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52cda2-aec7-4e96-b653-b0115b86f646",
   "metadata": {},
   "source": [
    "Let's create train and test datasets by calling `build_data` on train and test splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54299ff0-a919-4453-84c7-c3b4b417fd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/8000 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|███████████████████████████████████| 8000/8000 [00:00<00:00, 19073.31it/s]\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 23614.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(recipes_df, test_size=0.2, random_state=123)\n",
    "train_data = build_data(train_df, tokenizer_wrapper)\n",
    "test_data = build_data(test_df, tokenizer_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83184fe3-becf-4248-bec3-7c24966c167c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': tensor([   1,  304,  110,  342, 1229,    2,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0])},\n",
       " {'token': tensor([  1,  54,  61, 161,  48, 251,  69, 443,   2,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])},\n",
       " {'token': tensor([   1,  588,  665,  788, 1095,  831,   40, 1027,  405, 1120,  106,   31,\n",
       "             2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0])},\n",
       " {'token': tensor([   1,   99,  198,  336,  223, 1316,    2,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0])},\n",
       " {'token': tensor([   1, 1273,   59,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0])}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f5adf-b91c-4544-b544-20c300e68dd1",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578fe39",
   "metadata": {},
   "source": [
    "### Custom PyTorch dataset and batching\n",
    "\n",
    "- We define a `PytorchDataset` class to provide input-target token sequences for autoregressive training.\n",
    "\n",
    "- We prepare the input and target such that the model predicts the next token given previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3afe702f-0349-44ce-82f1-4f4f5d4f5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchDataset():\n",
    "    def __init__(self, data, pad_vocab_id=0):\n",
    "        self.data = data\n",
    "        self.pad_tensor = torch.tensor([pad_vocab_id])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        # Retrieve the next sequence of tokens from the current index\n",
    "        # by excluding the first token of the current sequence and appending a padding token at the end.        \n",
    "        target_sequence = torch.cat([self.data[ind]['token'][1:], self.pad_tensor])\n",
    "        return self.data[ind]['token'], target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d993084-a56d-45fe-8438-48d6f88c6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PytorchDataset(train_data)\n",
    "test_dataset = PytorchDataset(test_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5852b1b0-4511-412c-b4c1-4ccfa8db47d0",
   "metadata": {},
   "source": [
    "Now let's get a batch of data from DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f735587-d16d-4e5d-aaf9-0ba8bf29b533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 25])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text, train_target = next(iter(train_dataloader))\n",
    "train_text = train_text.to(device)\n",
    "train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "103cff6c-b65c-4c5f-bf05-b1ddc3efbe9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,   48,  267,  645,  113,  968, 1491, 1897,    2,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0], device='mps:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a019a4e-38b1-4d93-9a06-9b9da5c9faed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  48,  267,  645,  113,  968, 1491, 1897,    2,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bd67dac-e5e7-4a68-9b05-3f901673b1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] carrot apple chicken nuggets [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_wrapper.decode(train_text[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "149533c9-778d-443e-8080-723c95bfb8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carrot apple chicken nuggets [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_wrapper.decode(train_target[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02094388-1b47-4f8e-9218-8167425fe48d",
   "metadata": {},
   "source": [
    "The target is shifted one position to the left for autoregressive training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc436d81-2b85-457c-89da-c235f2a9a097",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a24cdcb",
   "metadata": {},
   "source": [
    "## Transformer Decoder Model\n",
    "\n",
    "We are now ready to define a transformer-based decoder-only model with positional encoding to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a33199-7710-4700-ab44-75a3b67506c9",
   "metadata": {},
   "source": [
    "Let's begin with positional encoding. Transformers don't have any built-in notion of word order (unlike RNNs), so we need to explicitly tell the model the position of each word in the sequence.\n",
    "\n",
    "In the interest of time, we won't dive deep into the math, but we'll use a standard implementation inspired by the [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) paper.\n",
    "\n",
    "The code below adds these position signals to token embeddings so the model can learn not just what the tokens are, but where they appear in the sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "426427fd-d3b9-4484-b3da-db0cfedaa6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PositionalEncoding model is already defined for you. Do not change this class.\n",
    "# We'll use this class in this exercise as well as the next exercise. \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements sinusoidal positional encoding as described in \"Attention is All You Need\".\n",
    "\n",
    "    Args:\n",
    "        d_model (int): Dimension of the embedding space.\n",
    "        dropout (float): Dropout rate after adding positional encodings.\n",
    "        max_len (int): Maximum length of supported input sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a (max_len, 1) position tensor: [[0], [1], ..., [max_len-1]]\n",
    "        positions = torch.arange(max_len).unsqueeze(1)\n",
    "\n",
    "        # Compute the scaling terms for each dimension (even indices only)\n",
    "        scale_factors = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Initialize the positional encoding matrix with shape (max_len, 1, d_model)\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(positions * scale_factors)  # Apply sine to even indices\n",
    "        pe[:, 0, 1::2] = torch.cos(positions * scale_factors)  # Apply cosine to odd indices\n",
    "\n",
    "        # Register as buffer (not a trainable parameter)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Adds positional encoding to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (seq_len, batch_size, d_model)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor with positional encoding added.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(0)\n",
    "        x = x + self.pe[:seq_len]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3dd46c-481a-49f1-9030-c3d4c73627b7",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f03c05e-435e-45a4-a86f-7944414edfee",
   "metadata": {},
   "source": [
    "### Model architecture \n",
    "\n",
    "Now we're ready to define our model architecture!\n",
    "It's going to include several key components that work together to generate text one token at a time:\n",
    "- `nn.Embedding layer`: turns token IDs into dense vector representations.\n",
    "- `PositionalEncoding`:  adds information about the position of each token in the sequence.\n",
    "- `TransformerDecoder`: the core of the model that processes the input using attention mechanisms.\n",
    "- Causal mask: ensures the model only attends to earlier positions when generating text, so it doesn't \"peek ahead\".\n",
    "- Output layer (`nn.Linear`): maps decoder outputs to vocab logits so we can predict the next token.\n",
    "  \n",
    "- Weight initialization: helps the model start training with reasonable values instead of random chaos.\n",
    "\n",
    "We'll walk through each part step by step in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9be74fc-4b38-4f60-bc2b-45d5818314d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeGenerator(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, num_layers, vocab_size, device, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the RecipeGenerator which uses a transformer decoder architecture\n",
    "        for generating recipes.\n",
    "\n",
    "        Parameters:\n",
    "            d_model (int): The number of expected features in the encoder/decoder inputs.\n",
    "            n_heads (int): The number of heads in the multiheadattention models.\n",
    "            num_layers (int): The number of sub-decoder-layers in the transformer.\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "            device (torch.device): The device on which the model will be trained.\n",
    "            dropout (float): The dropout value used in PositionalEncoding and TransformerDecoderLayer.\n",
    "        \"\"\"        \n",
    "        super(RecipeGenerator, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "        # Embedding layer for converting input text tokens into vectors\n",
    "        self.text_embedding = nn.Embedding(vocab_size , d_model)\n",
    "        \n",
    "        # Positional Encoding to add position information to input embeddings\n",
    "        self.pos_encoding = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
    "\n",
    "        # Define the Transformer decoder\n",
    "        decoder_layer=nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads, dropout=dropout)\n",
    "        self.TransformerDecoder = nn.TransformerDecoder(\n",
    "            decoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Final linear layer to map the output of the transformer decoder to vocabulary size        \n",
    "        self.linear_layer = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "\n",
    "        # Initialize the weights of the model\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights of the model to small random values.\n",
    "        \"\"\"\n",
    "        initrange = 0.1\n",
    "        self.text_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear_layer.bias.data.zero_()\n",
    "        self.linear_layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Get the embeded input\n",
    "        encoded_text = self.embed_text(text)        \n",
    "\n",
    "        # Get transformer output\n",
    "        transformer_output = self.decode(encoded_text)\n",
    "\n",
    "        # Final linear layer (unembedding layer)\n",
    "        return self.linear_layer(transformer_output)\n",
    "    \n",
    "    def embed_text(self, text):\n",
    "        embedding = self.text_embedding(text) * math.sqrt(self.d_model)\n",
    "        return self.pos_encoding(embedding.permute(1, 0, 2))\n",
    "    \n",
    "    def decode(self, encoded_text):\n",
    "        # Get the length of the sequences to be decoeded. This is needed to generate the causal masks\n",
    "        seq_len = encoded_text.size(0)\n",
    "        causal_mask = self.generate_mask(seq_len)\n",
    "        dummy_memory = torch.zeros_like(encoded_text)\n",
    "        return self.TransformerDecoder(tgt=encoded_text, memory=dummy_memory, tgt_mask=causal_mask)\n",
    "    \n",
    "    def generate_mask(self, size):\n",
    "        mask = torch.triu(torch.ones(size, size, device=self.device), 1)\n",
    "        return mask.float().masked_fill(mask == 1, float('-inf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fb26974-ebd5-45f0-95ec-9389cfa1ef8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "size = 10\n",
    "mask = torch.triu(torch.ones(size, size), 1)\n",
    "mask.float().masked_fill(mask == 1, float('-inf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c28d39-8795-45c5-9c71-29ba33577310",
   "metadata": {},
   "source": [
    "Let's instantiate our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d798f850-322c-4ecc-b975-059c2bb7247e",
   "metadata": {},
   "source": [
    "Let's instantiate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aec3e665-1988-4c7a-8337-41a81d0a792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the hyperparameters and initalize the model. Feel free to change these hyperparameters. \n",
    "d_model = 256 \n",
    "n_heads = 4\n",
    "num_layers = 8\n",
    "model = RecipeGenerator(d_model=d_model, n_heads=n_heads, num_layers=num_layers, vocab_size=vocab_size, device=device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666d874-2d32-4387-a490-d5706a26c477",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af84a7",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "We define the loss function and optimizer and train the model using cross-entropy loss while applying gradient clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca2660f0-5eae-463e-9ea2-48f090ac5bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,   48,  267,  ...,    0,    0,    0],\n",
       "        [   1,   56, 1135,  ...,    0,    0,    0],\n",
       "        [   1,  142,  488,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   1,  693,  970,  ...,    0,    0,    0],\n",
       "        [   1,  684,  685,  ...,    0,    0,    0],\n",
       "        [   1,   14,  427,  ...,    0,    0,    0]], device='mps:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ccc0c9c-ca40-4d5b-b36a-718fb9f2a462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 25])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e55dbf9a-f866-40e7-a934-a4a48ef0de90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 64, 3699])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass inputs to your model\n",
    "output = model(train_text)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39544a5d-b332-4cff-807c-04d3a2eb08ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3699"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd85513-c419-486a-b212-e0e6e3c1ee4a",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52a533ea-631e-4fc6-a24a-14ec898fff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(\n",
    "    model, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    train_dataloader, \n",
    "    test_dataloader, \n",
    "    epochs=5, \n",
    "    patience=5, \n",
    "    clip_norm=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the transformer model over multiple epochs using the provided dataloaders.\n",
    "\n",
    "    Args:\n",
    "        model: The Transformer model to train.\n",
    "        criterion: Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer: Optimizer (e.g., Adam).\n",
    "        train_dataloader: DataLoader for training data.\n",
    "        test_dataloader: DataLoader for validation data.\n",
    "        epochs: Number of training epochs.\n",
    "        patience: Early stopping patience – stop if validation loss increases `patience` times in a row.\n",
    "        clip_norm: Maximum norm for gradient clipping to avoid exploding gradients.\n",
    "\n",
    "    Returns:\n",
    "        train_losses: List of average training losses for each epoch.\n",
    "        test_losses: List of average test losses for each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch_inputs, batch_targets in train_dataloader:\n",
    "            # Move inputs and targets to the correct device (GPU or CPU)\n",
    "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(batch_inputs)  # shape: (seq_len, batch_size, vocab_size)\n",
    "            predictions = predictions.permute(1, 2, 0)  # shape: (batch_size, vocab_size, seq_len)\n",
    "\n",
    "            loss = criterion(predictions, batch_targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_inputs, batch_targets in test_dataloader:\n",
    "                batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "\n",
    "                predictions = model(batch_inputs).permute(1, 2, 0)\n",
    "                loss = criterion(predictions, batch_targets)\n",
    "\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Test Loss = {avg_test_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if epoch > 0 and avg_test_loss > test_losses[-2] * (1 + 1e-5):\n",
    "            early_stopping_counter += 1\n",
    "        else:\n",
    "            early_stopping_counter = 0\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "895144a0-8d32-4841-b284-6f059a8f78a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 6.9585, Test Loss = 6.3770\n",
      "Epoch 2: Train Loss = 6.0016, Test Loss = 5.6101\n",
      "Epoch 3: Train Loss = 5.3564, Test Loss = 5.1428\n",
      "Epoch 4: Train Loss = 4.9545, Test Loss = 4.8622\n",
      "Epoch 5: Train Loss = 4.6845, Test Loss = 4.6780\n",
      "Epoch 6: Train Loss = 4.4856, Test Loss = 4.5558\n",
      "Epoch 7: Train Loss = 4.3282, Test Loss = 4.4594\n",
      "Epoch 8: Train Loss = 4.1998, Test Loss = 4.3868\n",
      "Epoch 9: Train Loss = 4.0888, Test Loss = 4.3136\n",
      "Epoch 10: Train Loss = 3.9922, Test Loss = 4.2542\n",
      "Epoch 11: Train Loss = 3.9038, Test Loss = 4.2114\n",
      "Epoch 12: Train Loss = 3.8230, Test Loss = 4.1818\n",
      "Epoch 13: Train Loss = 3.7493, Test Loss = 4.1433\n",
      "Epoch 14: Train Loss = 3.6828, Test Loss = 4.1118\n",
      "Epoch 15: Train Loss = 3.6258, Test Loss = 4.0892\n",
      "Epoch 16: Train Loss = 3.5637, Test Loss = 4.0803\n",
      "Epoch 17: Train Loss = 3.5074, Test Loss = 4.0499\n",
      "Epoch 18: Train Loss = 3.4555, Test Loss = 4.0367\n",
      "Epoch 19: Train Loss = 3.4072, Test Loss = 4.0278\n",
      "Epoch 20: Train Loss = 3.3545, Test Loss = 4.0141\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer and the loss function. Feel free to change the hyperparameters. \n",
    "\n",
    "num_epoch = 20\n",
    "clip_norm = 1.0\n",
    "lr = 5e-5\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0) # Ignore the padding index\n",
    "train_losses, test_losses = trainer(model, criterion, optimizer,train_dataloader, test_dataloader, epochs= num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28fd406-8e6a-407d-937b-35daf7bada26",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19154a47",
   "metadata": {},
   "source": [
    "## Recipe Generation\n",
    "\n",
    "We generate a new recipe by sampling tokens one by one from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6eb1aa4e-8584-4dc3-893c-c35a53b3b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, device, max_recipe_length=39, seed=[206], end_vocab=2):\n",
    "    \"\"\"\n",
    "    Generates a recipe for an image using the specified model and device.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The trained model used for generating tokens.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        max_recipe_length (int): Maximum number of tokens to generate.\n",
    "        seed (list[int]): A list of one or more token IDs to start generation with.\n",
    "        end_vocab (int): Token ID that indicates the end of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 1D array of token IDs representing the generated recipe.\n",
    "    \"\"\"\n",
    "    # Ensure seed is a list and convert to tensor of shape [1, len(seed)]\n",
    "    context = torch.tensor([seed], device=device)\n",
    "\n",
    "    # Generate tokens until max length or end token is reached\n",
    "    for _ in range(max_recipe_length - len(seed)):  # subtract len(seed) to cap total length\n",
    "        logits = model(context)[-1]  # Get logits for the last position\n",
    "        probabilities = torch.softmax(logits, dim=-1).flatten(start_dim=1)\n",
    "        next_vocab = torch.multinomial(probabilities, num_samples=1)\n",
    "        context = torch.cat([context, next_vocab], dim=1)\n",
    "        if next_vocab.item() == end_vocab:\n",
    "            break\n",
    "\n",
    "    return context.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "229871ab-e2dd-460e-9f00-3a8d74f27f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = generate_recipe(model, device, max_recipe_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f308b243-b184-45cb-91e2-cd22f67e4d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chocolate chip chocolate frosting [SEP]'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_recipe = tokenizer_wrapper.decode(recipe)\n",
    "generated_recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5a61e-ef6a-4356-90b2-cf228e35a764",
   "metadata": {},
   "source": [
    "The generation quality might not be great but the purpose here is to demonstrate different components involved in text generation using transformers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec7f02-dcec-40c2-856b-9b27a1f9237b",
   "metadata": {},
   "source": [
    "![](../img/eva-chocolate.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
