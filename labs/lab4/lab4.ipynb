{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvpAmw1VzuBo"
   },
   "source": [
    "# DSCI 575 - Advanced Machine Learning\n",
    "\n",
    "# Lab 4: Long Short Term Memory networks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VSWy0GKJwwO"
   },
   "source": [
    "## Table of contents\n",
    "- [Submission guidelines](#sg)\n",
    "- [Learning outcomes](#lo)\n",
    "- [Exercise 1: Text generation with LSTMs](#1)\n",
    "- [Exercise 2: Text classification with LSTMs](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQYUxpVDJwwR"
   },
   "source": [
    "## Submission guidelines <a name=\"sg\"></a>\n",
    "\n",
    "#### Tidy submission\n",
    "rubric={mechanics:2}\n",
    "- To submit this assignment, submit this jupyter notebook with your answers embedded.\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/).\n",
    "- Use proper English, spelling, and grammar throughout your submission.\n",
    "\n",
    "#### Code quality and writing\n",
    "- These rubrics will be assessed on a question-by-question basis and are included in individual question rubrics below where appropriate.\n",
    "- See the [quality rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_quality.md) and [writing rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_writing.md) as a guide to what we are looking for.\n",
    "- Refer to [Python PEP 8 Style Guide](https://www.python.org/dev/peps/pep-0008/) for coding style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 911,
     "status": "ok",
     "timestamp": 1587394290546,
     "user": {
      "displayName": "varada kolhatkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQrcDFm59pZ859zTkQgrMPppBvkUHXzKN9D8_qxw=s64",
      "userId": "02873324325420823018"
     },
     "user_tz": 420
    },
    "id": "TVFXo-LUJwwU",
    "outputId": "bfbccc0f-78f0-4d02-d5c4-7efd1ec204f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kvarada/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/kvarada/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/kvarada/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as npr\n",
    "import os, sys, re\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCkNF6u4Jwwb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LS7PHfhlJwwg"
   },
   "source": [
    "## Learning outcomes <a name=\"lo\"></a>\n",
    "\n",
    "In this lab, we will carry out two tasks using Long Short Term Memory networks (LSTMs). \n",
    "\n",
    "1. Text generation\n",
    "2. Text classification\n",
    "\n",
    "After working on this lab, you will be able to \n",
    "\n",
    "- explain text generation using LSTMs \n",
    "- implement, debug, and explain text classification using LSTMs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TgIRoq8dzuB9"
   },
   "source": [
    "## Exercise 1: Text generation with LSTMs <a name=\"1\"></a>\n",
    "\n",
    "In lab 2 you generated text using a Markov model. A weakness of this approach is that the model cannot maintain long-term state. For example, imagine you wanted to model/generate a sentence like\n",
    "\n",
    "> Hello, my name is Alex (but really it's Alexandra, I just go by Alex for short).\n",
    "\n",
    "Well, that's tricky: how does the model to remember to close the parentheses? If you're using $n=10$ characters, then when it comes time to close the parentheses you've completely forgotten that you've opened them! And yet, on the other hand, it's completely infeasible to use $n=50$ or $n=100$ for the Markov model, as you'll just have one example of each training sequence. We're going to address this conundrum using LSTMs, which are a type of Recurrent Neural Network (RNN). The [long-short term memory](https://en.wikipedia.org/wiki/Long_short-term_memory) (LSTM) model allows us to use large $n$ in a practical way.\n",
    "\n",
    "LSTMs take long to train. So for this exercise, I am going to provide you with the weights of a trained network, and you will use the weights to generate random text.\n",
    "\n",
    "**Some things you should know:**\n",
    "\n",
    "- We're doing everything with characters here, not words. \n",
    "- With the LSTM, the data matrix `X` is now 3-dimensional instead of 2-dimensional. The dimensions are \n",
    "  1. number training examples\n",
    "  2. sequence length (this is the $n$ of the $n$-grams) \n",
    "  3. number of features, which in this case is a 1-hot encoding of the $d$ characters in our vocabulary\n",
    "- My model uses $n=100$, with 3 LSTM layers and 50% dropout. \n",
    "- My model only considers the $d=100$ most common characters, which captures the vast majority of all characters appearing in the corpus. There are over 4000 unique characters in the corpus but most appear very few times; including them would slow things down a lot, and would be unnecessary. (It's just a coincidence that $n=d$ here, there's no reason they need to be the same number. In hindsight I wish I used $n=50$ to speed everything up 2x.)\n",
    "\n",
    "**Attributions:** the model architecture is inspired by [this blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (which uses Torch rather than Keras). The Keras code is loosely based on code from the book [Deep Learning with Python](https://machinelearningmastery.com/deep-learning-with-python2/), with permission from the author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QDANi5DNzuB_"
   },
   "source": [
    "**To get going:**\n",
    "\n",
    "0. Highly recommended: read [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "1. Download the [weights](https://github.ubc.ca/MDS-2018-19/datasets/blob/master/data/LSTM-weights-wiki1MB.hdf5) and the [raw Wikipedia data](https://github.ubc.ca/MDS-2018-19/datasets/blob/master/data/wiki1MB.txt) from the [datasets repo](https://github.ubc.ca/ubc-mds-2016/datasets) and place them in the `data` directory. (The reason we also need the raw data is twofold: (1) is so that we can seed the generator with a random sequence of real data, and (2) so that we can extract the \"vocabulary\" of the top $d$ words, and then re-generate the mapping from these words to integers for the one-hot encoding.)\n",
    "2. Run the code :\n",
    "\n",
    "```\n",
    "python textgen.py gen \"data/wiki1MB.txt\" --weightsfile=\"data/LSTM-weights-wiki1MB.hdf5\"\n",
    "```\n",
    "\n",
    "**Please do not push your `data` folder.**\n",
    "\n",
    "You should be able to do this on your local machine using a CPU. It's pretty cool that a model trained on a GPU can then be run with no modifications on a CPU, thanks to TensorFlow. The code to train the network is all there too; you can look at it if you're interested, but you don't have to do anything with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i453J8DPzuCA"
   },
   "source": [
    "**Troubleshooting:**\n",
    "\n",
    "If you have problems, you may need to get yourself the same version of Keras and TensorFlow used to train the model. You can achieve this with\n",
    "\n",
    "```\n",
    "pip install keras==2.0.2\n",
    "pip install tensorflow==1.0.0\n",
    "```\n",
    "\n",
    "Beyond this, you may occasionally encounter the error described [here](http://stackoverflow.com/questions/40560795/tensorflow-attributeerror-nonetype-object-has-no-attribute-tf-deletestatus) and [here](https://github.com/tensorflow/tensorflow/issues/3388). If it happens to you: don't panic, and just try re-running the code. It seems like this bug [was fixed in tensorflow 1.1.0](https://github.com/tensorflow/tensorflow/commit/90d964f3382faf30d291aa3fbdb509844e1f042a).\n",
    "\n",
    "If you're getting an \"out of memory\" error, open textgen.py and change the line\n",
    "\n",
    "```\n",
    "n_examples = len(text) - seq_length\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```\n",
    "n_examples = 10000\n",
    "```\n",
    "\n",
    "This will use 100x less memory. It would mess up training but for generating sequences it is fine; the only consequence is that the seed text will be chosen from within the first 1% of the data file (no big deal). There are more proper fixes but this is quick and easy for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tflTvcjLzuCB"
   },
   "source": [
    "### 1(a) Generate text using LSTMs \n",
    "rubric={reasoning:4} \n",
    "\n",
    "1. Play around with the code. \n",
    "2. Paste some sample output along with the seed here in this notebook so the TAs know what you got. \n",
    "3. Comment on the behaviour of the LSTM model as compared to the Markov model from lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s6UaXXe1zuCD"
   },
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svkg1cZIzuCF"
   },
   "source": [
    "### 1(b) Temperature\n",
    "rubric={reasoning:4}\n",
    "\n",
    "At this point you've hopefully had success generating some text. Unfortunately, you won't be able to try different input files as the network is already trained (unless you want to try training a new one!). However, you can play with the `temperature` parameter, described in the context of this project [here](https://github.com/karpathy/char-rnn#sampling) and more generally softmax with temperature [here](https://en.wikipedia.org/wiki/Softmax_function). Let's define temperature, $T$, in a simpler (and hopefully mathematically equivalent) way than usual: \n",
    "\n",
    "- we take all our class probabilities (of different next characters) outputted by the model, and raise them to the power of $1/T$\n",
    "- then we re-normalize the probabilities so that they sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUQG5dZkzuCG"
   },
   "outputs": [],
   "source": [
    "def temper(p,T):\n",
    "    p = p**(1/T)\n",
    "    return p/np.sum(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmMrEY3ZzuCL"
   },
   "source": [
    "(Note that this does nothing when $T=1$.)\n",
    "\n",
    "To build an intuition for this, let's look at an example that has nothing to do with machine learning. Consider the (discrete) probability distribution below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3RWe_9WzuCN",
    "outputId": "6c5fadf2-fcd2-4418-ab4a-6c533c9c8915"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs = np.array([0.2, 0.5, 0.1, 0.1, 0.1])\n",
    "plt.bar(range(5), probs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TdV4m0k7zuCS"
   },
   "source": [
    "Now we can see what happens at $T=0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-Vr5VOfzuCV",
    "outputId": "e656cadf-f5ce-437a-c86f-55fb06b17450"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(5), temper(probs, T=0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Izt27NDOzuCf"
   },
   "source": [
    "And $T=0.25$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7O5Kf-8FzuCg",
    "outputId": "00c2509a-d1a4-4d10-fbdb-242a02306d30"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(5), temper(probs, T=0.25));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XV9R90uJzuCk"
   },
   "source": [
    "As you can see, a lower temperature _makes the probability distribution more extreme_. If you went all the way to $T=0$ then you'd have probability 1 of the most likely outcome and 0 for all others; in other words, at each iteration you'd just predict the argmax of the probability distribution rather than sampling from it (this would then be a deterministic model, not a random one). And if $T=1$ then we're just sampling from the original distribution, which is what you did with the Markov model. \n",
    "\n",
    "The take-home message is: lower temperatures cause the model to output text that is more just memorized from the training data, whereas higher temperatures allow \"crazier\" or \"more creative\" outputs than might be more fun but also might make less sense.\n",
    "\n",
    "Some optional notes: \n",
    "\n",
    "- Yes, this is related to temperature in physics. Boltzmann distribution, etc.\n",
    "- This is why softmax is called softmax. When we set $T=0$ we get the \"hard\" max of the numbers because we just pick out the biggest one. Setting $T>0$ makes it the \"soft\" max.\n",
    "- You could have taken this same notion of temperature and apply it to your Markov model from before, but that's not part of the assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OeYfSbkAzuCl"
   },
   "source": [
    "**Your task**\n",
    "\n",
    "1. Play around with different temperature values. You can do that by passing the temperature argument to the script (`-t TEMPERATURE`). You can look up all the possible arguments of the script with: \n",
    "\n",
    "    `python textgen.py --help`\n",
    "\n",
    "2. Paste your output for different temperatures here in the notebook.  \n",
    "3. Discuss how temperature affects the output, qualitatively by inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFJ43BkUzuCm"
   },
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3MOOz9VVzuCn",
    "outputId": "cbba48b6-8b84-4e47-ef26-d6bad4355370"
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RsZ9IrdVzuCt"
   },
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SmDdZa2zuCu",
    "outputId": "28b4e52b-3246-4085-b878-81d3265d7f10"
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5S5DcsvzuC1"
   },
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9vpmiwo1zuC2",
    "outputId": "2706534e-33e5-47f1-f659-62dd61bdce28"
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uM04KDYpzuC9"
   },
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "khOAGelLzuC_"
   },
   "source": [
    "## Exercise 2: Text classification with LSTMs <a name=\"2\"></a>\n",
    "\n",
    "In this exercise, we will train long short term memory models (LSTMs) for text classification. In particular, we'll be classifying \"toxic\" comments. \n",
    "\n",
    "Some background on the problem: One of the key challenges facing online communities, from social media to the comment sections of news sites, is content moderation. Content moderation is often a task of filtering out, i.e., deleting toxic and abusive content. (I think in DSCI 541 you talked a bit about the documentary [The Cleaners](https://www.youtube.com/watch?v=JA1DxRdT2hA).) Written abusive content can be found on online commenting platforms and there is a growing interest in developing automation to help filter and organize online comments for both moderators and readers. \n",
    "\n",
    "In this exercise we'll develop a deep learning model to identify \"toxic\" comments. We will be using [Toxic Comment Classification Challenge data](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) from Kaggle. You can download it [here](https://www.kaggle.com/c/8076/download-all). **Sorry for the offensive language in the toxic comments; it's the reality of such platforms ðŸ˜”. If you are sensitive to such language try not to look at the examples where `toxic` label is 1.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQ9D2uHr7AVM"
   },
   "source": [
    "LSTMs are expensive to train and for this exercise, I recommend that you work on the cloud using [Google colab](https://colab.research.google.com/notebooks/welcome.ipynb). This will allow you to train on a GPU and assess the benefits of training neural networks on GPUs.\n",
    "\n",
    "You will follow the steps below.\n",
    "\n",
    "- Save this Jupyter notebook so that it contains your latest work. Also push it to GitHub to be safe.\n",
    "- Go to [Google colab](https://colab.research.google.com/). \n",
    "- Make an account if you don't have one.\n",
    "- Select \"UPLOAD\" and upload this notebook itself, lab4.ipynb, which you just saved.\n",
    "- Runtime --> change runtime type --> Select GPU.\n",
    "- SUGGESTION: once you're done all your work on Colab, you can download the notebook from Colab and overwrite your local version. That way any work you did on Colab won't be lost. (They allow working directly off a notebook on GitHub, but that feature won't work for us since we're using github.ubc.ca.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g81fGdRnzuDB"
   },
   "source": [
    "### 2(a) Data and preprocessing \n",
    "rubric={reasoning:2, mechanics:2}\n",
    "\n",
    "Download the data from [here](https://www.kaggle.com/c/8076/download-all). Unzip the file. We'll be using only `train.csv`. You will need to upload it to your Google drive. I am providing starter code to read the data CSV below. Replace the path with your downloaded path. The dataset has 159,571 labeled comments. Also, In general, on online commenting platforms, the number of toxic comments is much smaller than non-toxic comments as can be seen in the value counts below. It's not recommended in general, but for this assignment I am extracting a small subset of comments with equal number of toxic and non-toxic comments. The final corpus is stored in `small_toxicity_df`. Note: Feel free to experiment with a larger sample. \n",
    "\n",
    "Your tasks: \n",
    "1. Preprocess `comment_text`. You can carry out preprocessing you think is appropriate for this problem, using the preprocessing library of your choice. \n",
    "2. Create train test splits for `small_toxicity_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22633,
     "status": "ok",
     "timestamp": 1587392899211,
     "user": {
      "displayName": "varada kolhatkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQrcDFm59pZ859zTkQgrMPppBvkUHXzKN9D8_qxw=s64",
      "userId": "02873324325420823018"
     },
     "user_tz": 420
    },
    "id": "XFU6vXdiz_nd",
    "outputId": "3d94c092-51e7-4a49-ed98-d51f0563c617"
   },
   "outputs": [],
   "source": [
    "# You will need to upload your downloaded datafile to Google drive and mount the drive as follows. \n",
    "# Read files from Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4016,
     "status": "ok",
     "timestamp": 1587394311251,
     "user": {
      "displayName": "varada kolhatkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQrcDFm59pZ859zTkQgrMPppBvkUHXzKN9D8_qxw=s64",
      "userId": "02873324325420823018"
     },
     "user_tz": 420
    },
    "id": "e4WF2oHszuDE",
    "outputId": "7dd0ba9d-1d2a-476c-8fe1-ee8ad805b589"
   },
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "# Data loading and preprocessing\n",
    "df = pd.read_csv('/content/gdrive/My Drive/<YOUR-DATA-PATH>', encoding = \"ISO-8859-1\")\n",
    "print('Number of comments', df.shape)\n",
    "print(df['toxic'].value_counts())\n",
    "\n",
    "# Create a smaller and balanced corpus of toxic and non-toxic comments\n",
    "# Probably not a great idea to create a balanced sample here. \n",
    "# But we'll do it for this lab.\n",
    "# You are welcome to think about it and share your thoughts. \n",
    "toxic_df = df[df['toxic'] == 1]\n",
    "non_toxic_df = df[df['toxic'] == 0]\n",
    "small_toxic_df = toxic_df.sample(n = 10000)\n",
    "small_non_toxic_df = non_toxic_df.sample(n = 10000)\n",
    "small_toxicity_df = pd.concat([small_non_toxic_df, small_toxic_df])\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1000,
     "status": "ok",
     "timestamp": 1587394313492,
     "user": {
      "displayName": "varada kolhatkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQrcDFm59pZ859zTkQgrMPppBvkUHXzKN9D8_qxw=s64",
      "userId": "02873324325420823018"
     },
     "user_tz": 420
    },
    "id": "DfZ-Ynix6lgI",
    "outputId": "fe55d0ea-e7d5-44ad-9bbd-97f81d0935fc"
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5pJswNDzuDS"
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xlrx5XXBzuDv"
   },
   "source": [
    "### 2(b) Baseline system\n",
    "rubric={accuracy:2,reasoning:1}\n",
    "\n",
    "We will train logistic regression with [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) as our baseline. \n",
    "\n",
    "- Get the sparse bag-of-words representation of the comments using [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "- Train [scikit-learn's LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier. \n",
    "- Report the accuracies on the train and test portions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3XGU-2EzuDw"
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4272,
     "status": "ok",
     "timestamp": 1587397174910,
     "user": {
      "displayName": "varada kolhatkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQrcDFm59pZ859zTkQgrMPppBvkUHXzKN9D8_qxw=s64",
      "userId": "02873324325420823018"
     },
     "user_tz": 420
    },
    "id": "wA_AiRulzuD1",
    "outputId": "780997b7-3c0c-4ffb-b583-296dc7a232e3"
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3V_DufP9CFuP"
   },
   "source": [
    "### YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDaFK0x8zuEA"
   },
   "source": [
    "### 2(c) Text classification with LSTMs\n",
    "rubric={reasoning:5}\n",
    "\n",
    "The starter code below for the class `DLTextClassifier` has methods for data preparation, building network, training, evaluating, and predicting. \n",
    "\n",
    "Your tasks: \n",
    "1. Read the provided methods and write the missing docstrings. \n",
    "2. We saw that in text generation our `X` which was input to the network was a 3-D tensor and `y` was a 2-D matrix. Comment on the dimensions of `X` and `y` in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1YX7M2qwzuEB"
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1L0zm6zWXfc"
   },
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "class DLTextClassifier():\n",
    "    def __init__(self, \n",
    "                 embedding_dimension = 200,\n",
    "                 max_features = 20000, \n",
    "                 maxlen = 80):\n",
    "        \"\"\"\n",
    "        Instantiate the DLTextClassifer.              \n",
    "        Parameters:\n",
    "        ------------------        \n",
    "        embedding_dimension : (int)\n",
    "            size of your word embedding vector\n",
    "        max_features: (int)\n",
    "            max number of words to keep in the vocabulary\n",
    "        maxlen : (int)\n",
    "            sequence length\n",
    "        \"\"\"\n",
    "        # We'll be using an embedding layer and pass a vector of\n",
    "        # size embedding dimension instead of one-hot-encoding. \n",
    "        self.embedding_dimension = embedding_dimension\n",
    "\n",
    "        # Only consider max_features features to prevent from \n",
    "        self.max_features = max_features\n",
    "        \n",
    "        # Sequence length\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "        # Create the tokenizer. We'll be using Keras Tokenizer here.         \n",
    "        self.tokenizer = Tokenizer(num_words=self.max_features, \n",
    "                             filters='! #$% ()*+,-./:; = ?@[\\\\]^_`{|}~\\t\\n>\"<')\n",
    "        \n",
    "        # Store word_index\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "                \n",
    "    def prepare_data(self, corpus, mode = 'train'):\n",
    "        \"\"\"              \n",
    "        YOUR DOCSTRING HERE.        \n",
    "        \"\"\"\n",
    "        if mode == 'train': \n",
    "            # fit the tokenizer on the documents\n",
    "            self.tokenizer.fit_on_texts(corpus)\n",
    "        \n",
    "        # integer encode documents\n",
    "        encoded_corpus = self.tokenizer.texts_to_sequences(corpus)        \n",
    "        print('len of encoded docs: ', len(encoded_corpus))\n",
    "        return self.pad_sequences(encoded_corpus)\n",
    "\n",
    "    def pad_sequences(self, data):\n",
    "        \"\"\"\n",
    "        YOUR DOCSTRING HERE.\n",
    "        \"\"\"\n",
    "        print('Pad sequences (samples x time)')\n",
    "        padded_data = sequence.pad_sequences(data, maxlen=self.maxlen)\n",
    "        print('Padded data shape:', padded_data.shape)\n",
    "        return padded_data    \n",
    "          \n",
    "    def build_network(self, layer_size = 256, dropout_amount=0.5):\n",
    "        \"\"\"\n",
    "        Given layer_size and dropout_amount, build an LSTM network\n",
    "        using Keras and tensorflow and print summary of the model. \n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        layer_size : int\n",
    "          The number of units to be passed in the LSTM layer\n",
    "        dropout_amount : float\n",
    "          the dropout amount to be passed in the Dropout layer. \n",
    "\n",
    "        Return\n",
    "        -----------\n",
    "        None\n",
    "          print the summary of the model \n",
    "        \"\"\"      \n",
    "        # YOUR CODE HERE \n",
    "\n",
    "                \n",
    "    def fit(self, \n",
    "              X_train, y_train,\n",
    "              batch_size =32, \n",
    "              epochs = 10, \n",
    "              save_path='/content/gdrive/My Drive/<YOUR MODEL PATH>'):\n",
    "        \"\"\"        \n",
    "        Given the parameters train a deep learning model and save and return it.  \n",
    "        \n",
    "        Parameters\n",
    "        -------------\n",
    "        X_train : (list) \n",
    "          the X values of the train split \n",
    "        y_train : (list) \n",
    "          the y values of the train split \n",
    "        batch_size : (int) \n",
    "          the batch_size for the training\n",
    "        epochs : (int) \n",
    "          the number of epochs for training \n",
    "        save_path : (str) the path to save the model\n",
    "        \n",
    "        Return\n",
    "        -------------\n",
    "          the trained model\n",
    "        \"\"\"      \n",
    "        # YOUR CODE HERE \n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Given a model and X_test, y_test, evaluates the model and prints \n",
    "        the accuracy\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X_test -- Array of text data\n",
    "        y_test -- labels for every text\n",
    "        \n",
    "        Return:    \n",
    "        -----------    \n",
    "        None                \n",
    "        \"\"\"        \n",
    "        X_test_padded = self.prepare_data(X_test, mode='test')        \n",
    "        score, acc = self.model.evaluate(X_test_padded, y_test)\n",
    "        print('Accuracy: ', acc)\n",
    "\n",
    "\n",
    "    def predict(self, texts):\n",
    "        \"\"\"\n",
    "        Return predicted labels for a list of texts.        \n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        \n",
    "        texts : (list) \n",
    "          a list of text comments\n",
    "\n",
    "        Return:\n",
    "        --------        \n",
    "          a list of prediction scores        \n",
    "        \"\"\"        \n",
    "        padded_sequences = self.prepare_data(texts, mode = 'test')\n",
    "        return self.model.predict(padded_sequences)\n",
    "### END STARTER CODE    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_th1KIXU24U"
   },
   "source": [
    "### 2(d) Build LSTM network\n",
    "rubric={accuracy:4,reasoning:4}\n",
    "\n",
    "In this exercise you will build an LSTM network . Complete the method `build_LSTM` in the class above. In particular, your tasks are: \n",
    "\n",
    "1. Add Keras layers to build an LSTM. You may create as many layers of LSTMs as you want. \n",
    "2. Compile and store the model in `self.model` and print the summary of the model. \n",
    "3. Create a class object and check whether your code is working so far. \n",
    "4. Briefly explain your network in words. Comment on the input and output dimensions of each layer in the network. How many total parameters are there? What is the effect of having an embedding layer on the number of parameters? \n",
    "\n",
    "**Hint**: You may refer to [Keras examples for text classification using LSTM](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) when you build layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kxy6SM5bjVMQ"
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_gXxGtdElIk8"
   },
   "source": [
    "### 2(e) Training LSTMs\n",
    "rubric={accuracy:3,reasoning:2}\n",
    "\n",
    "Train a deep learning model. In particular, \n",
    "\n",
    "1. Complete the class method `fit`. You'll have to prepare the data first using `prepare_data` class method (which is already provided) and then `fit` the model. \n",
    "3. Call the `fit` method and experiment with different hyperparameters of your choice. Note your observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 601094,
     "status": "ok",
     "timestamp": 1587395954736,
     "user": {
      "displayName": "varada kolhatkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQrcDFm59pZ859zTkQgrMPppBvkUHXzKN9D8_qxw=s64",
      "userId": "02873324325420823018"
     },
     "user_tz": 420
    },
    "id": "axBlrqbF4c4Y",
    "outputId": "eb762ff7-e364-418e-ff9d-06eae8a4c28c"
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WYfY3NnUmcn7"
   },
   "source": [
    "### 2(f) Evaluating LSTM performance\n",
    "\n",
    "rubric={accuracy:2,reasoning:4}\n",
    "\n",
    "1. Call the class method `evaluate` to evaluate the model on the test set. \n",
    "2. Compare your results with the results in 2(b). \n",
    "3. Create some fake comments and get and examine model predictions on these comment using the `predict` method of the class. \n",
    "4. Note your observations. Do you think it would be reasonable to use your system in real-life applications to moderate online content?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3947,
     "status": "ok",
     "timestamp": 1587396168206,
     "user": {
      "displayName": "varada kolhatkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQrcDFm59pZ859zTkQgrMPppBvkUHXzKN9D8_qxw=s64",
      "userId": "02873324325420823018"
     },
     "user_tz": 420
    },
    "id": "tiOLGF0Sf0gU",
    "outputId": "e49406a1-5a97-4a36-83f2-b30aa34ad65a"
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5512,
     "status": "ok",
     "timestamp": 1587396176185,
     "user": {
      "displayName": "varada kolhatkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQrcDFm59pZ859zTkQgrMPppBvkUHXzKN9D8_qxw=s64",
      "userId": "02873324325420823018"
     },
     "user_tz": 420
    },
    "id": "GQzgrrY4aAN-",
    "outputId": "a5551eb8-1aef-4ae2-f1e8-c1f70c09e04b"
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mIDUuuyAJwyd"
   },
   "source": [
    "### (optional) 2(g): Train GRUs and bidirectional LSTMs\n",
    "rubric={reasoning:1}\n",
    "\n",
    "- Train [GRUs](https://keras.io/layers/recurrent/#gru) and [bidirectional LSTMs](https://keras.io/layers/wrappers/#bidirectional)\n",
    "- Compare your results with results in 2(e). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xlpR4ESAzuEI"
   },
   "source": [
    "### (optional) 2(h): Use pre-trained embeddings \n",
    "rubric={reasoning:1}\n",
    "\n",
    "- Currently you are using an embedding layer, where you are training your word embeddings, a word representation of size `embedding_size`. Try using pre-trained embeddings instead. \n",
    "- Compare your results with the results in 2(e). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
