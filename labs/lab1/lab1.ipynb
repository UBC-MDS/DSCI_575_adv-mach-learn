{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 575 - Advanced Machine Learning\n",
    "\n",
    "# Lab 1: Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm \n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "# pip install ipython-autotime\n",
    "import autotime\n",
    "\n",
    "from gensim.models import Word2Vec,FastText\n",
    "\n",
    "from preprocessing import MyPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- [Submission guidelines](#sg)\n",
    "- [Learning outcomes](#lo)\n",
    "- [Exercise 0: Warm up](#0)\n",
    "- [Exercise 1: Word meaning representation using co-occurrence matrix](#1)\n",
    "- [Exercise 2: Word embeddings (dense word representations)](#2)\n",
    "- [Exercise 3: Pre-trained word embeddings](#3)\n",
    "- [Exercise 4: Product recommendation using Word2Vec](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission guidelines <a name=\"sg\"></a>\n",
    "\n",
    "#### Tidy submission\n",
    "rubric={mechanics:3}\n",
    "- To submit this assignment, submit this jupyter notebook with your answers embedded.\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/).\n",
    "- Use proper English, spelling, and grammar throughout your submission.\n",
    "\n",
    "#### Code quality and writing\n",
    "- These rubrics will be assessed on a question-by-question basis and are included in individual question rubrics below where appropriate.\n",
    "- See the [quality rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_quality.md) and [writing rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_writing.md) as a guide to what we are looking for.\n",
    "- Refer to [Python PEP 8 Style Guide](https://www.python.org/dev/peps/pep-0008/) for coding style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning outcomes <a name=\"lo\"></a>\n",
    "\n",
    "After working on this lab, you will be able to\n",
    "\n",
    "- Find cosine similarity between words using sparse word representation\n",
    "- Train your own dense embeddings using Word2Vec and fastText algorithms\n",
    "- Explain how unknown words are handled in Word2Vec vs. fastText\n",
    "- Use pre-trained word embeddings\n",
    "- Use word2vec algorithm for product recommendation \n",
    "\n",
    "\n",
    "**Note that this lab involves loading pre-trained models that may take long time depending upon your machine. So please start early and do not leave this lab for last minute.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use a subset of the good old [IMDB movie review data set](https://www.kaggle.com/utathya/imdb-review-dataset) for the first two exercises. Below I am providing starter code to create sub-corpus from this corpus. Replace the CSV path with your download path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "# Data loading and preprocessing\n",
    "imdb_df = pd.read_csv('data/imdb_master.csv', encoding = \"ISO-8859-1\")\n",
    "imdb_df.head()\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "imdb_df['label'].value_counts()\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "SUBSET_SIZE = 5000\n",
    "\n",
    "# A list of all reviews\n",
    "imdb_all_corpus = imdb_df['review'].tolist()\n",
    "\n",
    "# Shuffle reviews \n",
    "random.shuffle(imdb_all_corpus)\n",
    "\n",
    "# Create a small subset of the corpus\n",
    "imdb_subset = imdb_all_corpus[:SUBSET_SIZE]\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0: Warm up <a name=\"0\"></a>\n",
    "\n",
    "Typically, text data needs to be \"normalized\" before we do anything with it. I am providing you `MyPreprocessor` class in the file `preprossing.py` which carries out basic preprocessing. Throughout this lab, you will be using this  class for preprocessing and in this particular exercise, you'll use this class on a toy corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0(a) Preprocessing \n",
    "\n",
    "rubric={accuracy:2,reasoning:2}\n",
    "\n",
    "Your tasks: \n",
    "\n",
    "1. Preprocess the corpus below (`corpus`) using the `preprocess_corpus` method of the `MyPreprocessor` class. Print the preprocessed corpus. \n",
    "2. Write your observations about the preprocessed corpus. What do you think is the purpose of preprocessing text data? \n",
    "3. Now create a preprocessed corpus for the `imdb_subset` and store it into a variable called `pp_imdb_subset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE \n",
    "corpus = [\"\"\"The 21 Lessons for the 21st Century focuses on \n",
    "             current affairs and on the more immediate future \n",
    "             of humankind. In a world deluged by irrelevant \n",
    "             information clarity is power. Censorship works \n",
    "             not by blocking the flow of information, but rather \n",
    "             by flooding people with disinformation and distractions. \n",
    "             So what is really happening right now? \n",
    "             What are todayâ€™s greatest challenges and choices? \n",
    "             What should we pay attention to?\n",
    "         \"\"\",\n",
    "         \"\"\"\n",
    "         The Python Data Science Handbook provides a reference \n",
    "         to the breadth of computational and statistical \n",
    "         methods that are central to data-intensive science, \n",
    "         research, and discovery. \n",
    "         \"\"\"\n",
    "         ]\n",
    "### END STARTER CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:  Word meaning representation using co-occurrence matrix <a name=\"1\"></a>\n",
    "\n",
    "In this exercise you'll build sparse representation of words using term-term co-occurrence matrix and find cosine similarity scores between a set of word pairs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a) Build and visualize term-term co-occurrence matrix\n",
    "rubric={accuracy:2,viz:2}\n",
    "\n",
    "Below we are providing you some starter code for the class `CooccurrenceMatrix`. Read the docstrings of the class methods. \n",
    "\n",
    "Your tasks:\n",
    "1. Create a term-term co-occurrence matrix for the `pp_imdb_subset` with `window_size` 3. \n",
    "2. Show the first few rows of the co-occurrence matrix as a pandas DataFrame. Show the appropriate column and row labels (words associated with the indices) so that your co-occurrence matrix is interpretable. \n",
    "3. Get word vector for the word _cat_ using the `get_word_vector` method of the `CooccurrenceMatrix` class. What's the size of the vector? How many values are non-zero?      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "class CooccurrenceMatrix:\n",
    "    def __init__(self, corpus, \n",
    "                       tokenizer = word_tokenize, \n",
    "                       window_size = 3):\n",
    "        self.corpus = corpus\n",
    "        self.tokenizer = tokenizer\n",
    "        self.window_size = window_size\n",
    "        self.vocab = {}\n",
    "        self.cooccurrence_matrix = None    \n",
    "        \n",
    "    def fit_transform(self):\n",
    "        \"\"\"\n",
    "        Creates a co-occurrence matrix. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        dict, scipy.sparse.csr_matrix\n",
    "            Returns the vocabulary and a sparse cooccurrence matrix\n",
    "        \"\"\"\n",
    "        data=[]\n",
    "        row=[]\n",
    "        col=[]\n",
    "        for tokens in self.corpus:\n",
    "            for target_index, token in enumerate(tokens):\n",
    "                # Get the index of the word in the vocabulary. If the word is not in the vocabulary, \n",
    "                # set the index to the size of the vocabulary. \n",
    "                i = self.vocab.setdefault(token, len(self.vocab))\n",
    "                \n",
    "                # Consider the context words depending upon the context window \n",
    "                start = max(0, target_index - self.window_size)\n",
    "                end = min(len(tokens), target_index + self.window_size + 1)\n",
    "                \n",
    "                for context_index in range(start, end):\n",
    "                    # Do not consider the target word.  \n",
    "                    if target_index == context_index: \n",
    "                        continue                        \n",
    "                    j = self.vocab.setdefault(tokens[context_index], len(self.vocab))\n",
    "                    # Set diagonal to 0\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    data.append(1.0); row.append(i); col.append(j);\n",
    "        self.cooccurrence_matrix = csr_matrix((data,(row,col)))\n",
    "        return self.vocab, self.cooccurrence_matrix\n",
    "            \n",
    "    def get_word_vector(self, word):\n",
    "        \"\"\"\n",
    "        Given a word returns the word vector associated with it from the co-occurrence matrix. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        word : str \n",
    "            the word to look up in the vocab.\n",
    "        \"\"\"\n",
    "        if word in self.vocab: \n",
    "            return self.cooccurrence_matrix[self.vocab[word]]\n",
    "        else:\n",
    "            print('The word not present in the vocab')\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(b) Cosine similarity between sparse word vectors\n",
    "rubric={accuracy:2,reasoning:2}\n",
    "\n",
    "1. Now get word vectors for `word_pairs` shown below. It is not required but feel free to add more word pairs if you like.  \n",
    "2. Calculate cosine similarity between the word pairs using [`scikit-learn`'s cosine similarity function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html).\n",
    "3. Discuss your results. Do these similarity scores make sense to you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "word_pairs = [('coast','shore'), \n",
    "              ('clothes', 'closet'), \n",
    "              ('old', 'new'), \n",
    "              ('smart', 'intelligent'), \n",
    "              ('dog', 'cat'),\n",
    "              ('orange', 'lawyer')\n",
    "             ]\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Word embeddings (dense word representations) <a name=\"2\"></a>\n",
    "\n",
    "In Exercise 1, you created and worked with sparse word representations, where each word vector was of size $1 \\times V$ ($V$ = size of the vocabulary). In this exercise, you will create short and dense word representations using the Word2Vec algorithm. Before training word embedding models, we need to convert the data into a suitable format, which we have already done above. \n",
    "\n",
    "In this exercie, you will\n",
    " - Train Word2Vec and fastText algorithms on `pp_imdb_subset`\n",
    " - Calculate cosine similarity between word pairs with dense vectors\n",
    " - Use pre-trained word embeddings\n",
    " \n",
    "You will need to [install `gensim`](https://radimrehurek.com/gensim/index.html) for this exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2(a) Training `Word2Vec` and `fastText`\n",
    "rubric={accuracy:4,quality:2,reasoning:1}\n",
    "\n",
    "In this exercise, you will train two models on the preprocessed version of the subset imdb corpus `pp_imdb_subset` to get dense word representations: `Word2Vec` and `fastText`. \n",
    "\n",
    "Your tasks: \n",
    "\n",
    "1. Train [Word2Vec model](https://radimrehurek.com/gensim/models/word2vec.html) on `pp_imdb_subset` with following hyperparameters. (This might take some time so I recommend saving the model for later use.)\n",
    "    * size=100\n",
    "    * window=5\n",
    "2. Train [fastText model](https://radimrehurek.com/gensim/models/fasttext.html) on the tokenized corpus with the same set of hyperparameters. (This might take some time so I recommend saving the model for later use.)\n",
    "\n",
    "3. What is the vocabulary size in each model? \n",
    "\n",
    "Note that the word embeddings will be better quality if we use the full IMDB corpus instead of the subset. We are using a subset in this exercise to save some time. On my Macbook Air it took 204.8 s to train Word2Vec on the full IMDB corpus and 376.3 s to train fastText on the full IMDB corpus. If you are feeling adventurous, you are welcome to train it on the full corpus.  \n",
    "\n",
    "**Please do not submit your saved models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(b) Unknown words \n",
    "rubric={accuracy:2,reasoning:2}\n",
    "\n",
    "1. Is the word _appendicitis_ present in the vocabulary of the two models? You may try other words which are unlikely to occur in the IMDB dataset. \n",
    "2. Now look at the vectors for the word _appendicitis_ for both models. \n",
    "3. Note your observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2(c) Cosine similarity with dense vectors\n",
    "rubric={accuracy:2,reasoning:1}\n",
    "\n",
    "- Calculate cosine similarity between the word pairs (`word_pairs`) from Exercise 1(b) using the [model.similarity](https://radimrehurek.com/gensim/models/word2vec.html) method.\n",
    "- Comment on the quality of word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:  <a name=\"3\"></a>\n",
    "\n",
    "Training word embeddings on a large corpus is resource intensive and we might not be able train good quality embeddings on our laptops. (You might have noticed your laptop making noises if you tried the full IMDB data set in the previous exercise.)\n",
    "\n",
    "Using pre-trained word embeddings is very common in NLP. These embeddings are created by training a model like Word2Vec or fastText on a huge corpus of text such as a dump of Wikipedia or a dump of the web crawl. It has been shown pre-trained word embeddings [work well on a variety of text classification tasks](http://www.lrec-conf.org/proceedings/lrec2018/pdf/721.pdf). The rationale is that such corpora are representative of many different corpora you might be using in your specific domain (e.g., twitter domain, news domain).\n",
    "\n",
    "A number of pre-trained word embeddings are available. The most popular ones are:  \n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using the fastText algorithm\n",
    "    * published by Facebook\n",
    "    \n",
    "In this exercise, you will be downloading and using GloVe twitter pre-trained embeddings.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(a) Load GloVe twitter embeddings\n",
    "rubric={mechanics:3,accuracy:2,reasoning:1}\n",
    "\n",
    "In this exercise we will explore [GloVe](https://nlp.stanford.edu/projects/glove/) model trained on the twitter data.   \n",
    "\n",
    "Your tasks are:\n",
    "1. Download [GloVe embeddings for Twitter](http://nlp.stanford.edu/data/glove.twitter.27B.zip). This is a large file (the compressed file is ~1.42 GB ). **Please do not submit it.** \n",
    "2. Unzip the downloaded file. For this exercise we'll be using `glove.twitter.27B/glove.twitter.27B.100d.txt`. The file has words and their corresponding pre-trained embeddings.\n",
    "3. Convert the GloVe embeddings to the Word2Vec format using the following command. More details [here](https://www.pydoc.io/pypi/gensim-3.2.0/autoapi/scripts/glove2word2vec/index.html).\n",
    "\n",
    "> python -m gensim.scripts.glove2word2vec -i \"glove.twitter.27B.100d.txt\" -o \"glove.twitter.27B.100d.w2v.txt\"\n",
    "\n",
    "4. Load the `glove_twitter_model` using the following starter code.\n",
    "5. Compare the vocabulary size of `glove_twitter_model` with the two models in 2(b). \n",
    "6. Is the word *appendicitis* present in the vocabulary of the `glove_model`?\n",
    "\n",
    "\n",
    "**Note that the glove model is case sensitive and it only has representation of lower-case words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "from gensim.models import KeyedVectors\n",
    "glove_twitter_model = KeyedVectors.load_word2vec_format('<YOUR_PATH>/glove.twitter.27B.100d.w2v.txt', binary=False)  # C text format\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(b) Word similarity using pre-trained embeddings\n",
    "rubric={accuracy:1,reasoning:1}\n",
    "\n",
    "- Calculate cosine similarity between word pairs (`word_pairs`) above using the `wv.similarity` method of the pre-trained embeddings. Compare your results with similarity results in 2(c). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(c) Analogies \n",
    "rubric={accuracy:2,reasoning:2}\n",
    "\n",
    "- Try out four pairs of analogies (similar to how we did in class with English GoogleNews pre-trained word embeddings) with `glove_twitter_model`. \n",
    "- Recall that we noticed gender stereotypes when we used English GoogleNews pre-trained word embeddings. Do you see similar stereotypes with `glove_twitter_model`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "def analogy(word1, word2, word3, model=glove_twitter_model):\n",
    "    '''    \n",
    "    Returns analogy word using the given model. \n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str) \n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation    \n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation         \n",
    "    model : \n",
    "        word embedding model\n",
    "    \n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    '''\n",
    "    print('%s : %s :: %s : ?' %(word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=['Analogy word', 'Score'])\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(d) Building your own embeddings vs. using pre-trained embeddings\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Give example scenarios when you would train your own embeddings and when you would use pre-trained embeddings.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 3(e) Find the odd one out \n",
    "rubric={reasoning:1}\n",
    "\n",
    "Other than finding word similarity and analogies, we can also use word embeddings for finding an odd word out using the [`doesnt_match` method](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.doesnt_match.html?highlight=doesnt_match) of the model. \n",
    "\n",
    "- Try an example to find an odd word out using the pre-trained embeddings and examine whether the odd one out given by the algorithm makes sense or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 3(f) Distance between sentences\n",
    "rubric={reasoning:1}\n",
    "\n",
    "In addition, you can also use word embeddings to find distance between sentences using the [`wmdistance` method](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.wmdistance.html) of the medel. Find distance between two similar sentences (with non-overlapping words) and two completely unrelated sentences. Do the distances make sense?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Product recommendation using Word2Vec <a name=\"4\"></a>\n",
    "\n",
    "The Word2Vec algorithm can also be used in tasks beyond text and word similarity. In this exercise we will explore using it for product recommendations. We will build a Word2Vec model so that similar products (products occurring in similar contexts) occur close together in the vector space. The context of products can be determined by the purchase histories of customers. Once we have reasonable representation of products in the vector space, we can recommend products to customers that are \"similar\" (as depicted by the algorithm) to their previously purchased items or items in their cart. \n",
    "\n",
    "For this exercise, we will be using the [Online Retail Data Set from UCI ML repo](https://www.kaggle.com/jihyeseo/online-retail-data-set-from-uci-ml-repo#__sid=js0). The starter code below reads the data as a pandas dataframe `df`. \n",
    "\n",
    "Download the data and save it under data folder in your lab's directory. **Please do not push the data to your repository.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "### Read the data. Takes a while to read the data.\n",
    "### Change the path below to your download path\n",
    "df = pd.read_excel('data/Online_Retail.xlsx')\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "print(\"Data frame shape: \", df.shape)\n",
    "df.head(20)\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(a): Preprocessing data\n",
    "rubric={accuracy:2,reasoning:2}\n",
    "\n",
    "1. Carry out necessary preprocessing (e.g., getting rid of NaN, datatype conversions), if necessary. \n",
    "2. How many unique customers and unique products are there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(b): Prepare data for Word2Vec\n",
    "rubric={accuracy:8,quality:4}\n",
    "\n",
    "1. Split the customers into train (90%) and validation (10%) sets.\n",
    "2. For the train and validation customers, create purchasing history for the customers in the following format, where each inner list corresponds to the purchase history of a unique customer. Each item in the list is a `StockCode` in the purchase history of that customer ordered on the time of purchase. \n",
    "\n",
    "```\n",
    "[[CustomerID1_StockCode1, CustomerID1_StockCode2, ....], \n",
    " [CustomerID2_StockCode10, CustomerID2_StockCode1, ....], \n",
    " ...\n",
    " [CustomerID1000_StockCode99, CustomerID1000_StockCode10, ....],\n",
    " ...\n",
    " ]\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4(c): Train `Word2Vec` model \n",
    "rubric={accuracy:3}\n",
    "\n",
    "1. Now that your data is in the format suitable for training Word2Vec model, train `Word2Vec` model on the train split. Time your model and report how long it took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4(d): Examine product similarity \n",
    "rubric={accuracy:2,reasoning:4}\n",
    "\n",
    "Read the starter code below for the `get_most_similar` function. \n",
    "\n",
    "1. Get similar products for the following products. \n",
    "    - 'SAVE THE PLANET MUG'\n",
    "    - 'POLKADOT RAIN HAT'    \n",
    "2. Now pick 4 product descriptions from the validation set. Call `get_most_similar` for these product descriptions and examine similar products returned by the function.\n",
    "3. Discuss your observations. \n",
    "4. If a product does not appear in the train set but appears in the validation set, would the model return a list of similar products for this product? Does it make sense to use the `fastText` algorithm in this case instead of Word2Vec? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE \n",
    "# Create products id_name and name_id dictionaries\n",
    "products_id_name_dict = pd.Series(df.Description.str.strip().values,index=df.StockCode).to_dict()\n",
    "products_name_id_dict = pd.Series(df.StockCode.values,index=df.Description.str.strip()).to_dict()\n",
    "### END STARTER CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE \n",
    "def get_most_similar(prod_desc, n = 10, model = model):\n",
    "    \"\"\"   \n",
    "    Given product description, prod_desc, return the most similar \n",
    "    products  \n",
    "\n",
    "    Arguments\n",
    "    ---------     \n",
    "    prod_desc -- str\n",
    "        Product description     \n",
    "\n",
    "    Keyword arguments\n",
    "    ---------     \n",
    "    n -- integer\n",
    "        the number of similar items to return \n",
    "\n",
    "    model -- gensim Word2Vec model\n",
    "        trained gensim word2vec model on customer purchase histories\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A pandas dataframe containing n names of similar products \n",
    "        and their similarity scores with the input product \n",
    "        with desciption prod_desc.     \n",
    "    \n",
    "    \"\"\"\n",
    "    stock_id = products_name_id_dict[prod_desc]\n",
    "    try:\n",
    "        similar_stock_ids = model.wv.most_similar(stock_id, topn = n)\n",
    "    except: \n",
    "        print('The product %s is not in the vocabulary'%(prod_desc))    \n",
    "        return    \n",
    "\n",
    "    similar_prods = []\n",
    "        \n",
    "    for (sim_stock_id, score) in similar_stock_ids:\n",
    "        similar_prods.append((products_id_name_dict[sim_stock_id], score))\n",
    "    return pd.DataFrame(similar_prods, columns=['Product description', 'Similarity score'])\n",
    "### END STARTER CODE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
