

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>AppendixD: More RNNs, LSTMs &#8212; DSCI 575 Advanced Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/AppendixE-LSTMs';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Attributions" href="../attribution.html" />
    <link rel="prev" title="LDA details" href="AppendixD-LDA-details.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/mds-hex-sticker.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/mds-hex-sticker.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting ready</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="00_course-information.html">Course Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_Markov-models.html">Lecture 1: Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_LMs-text-preprocessing.html">Lecture 2: Applications of Markov Models  and Text Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_HMMs-intro.html">Lecture 3: Introduction to Hidden Markov Models (HMMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_Viterbi-Baum-Welch.html">Lecture 4: Decoding and Learning in HMMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_topic-modeling.html">Lecture 5: Topic Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_intro-to-RNNs.html">Lecture 6: Introduction to Recurrent Neural Networks (RNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_intro-to-transformers.html">Lecture 7: Introduction to self-attention and transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_more-transformers.html">Lecture 8: More transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Class demos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="class_demos/text-generation.html">Class Demo: Recipe generator</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="AppendixA-PageRank.html">PageRank as a Markov model</a></li>
<li class="toctree-l1"><a class="reference internal" href="AppendixB-HMM-POS.html">HMM supervised POS tagging</a></li>
<li class="toctree-l1"><a class="reference internal" href="AppendixC-BaumWelch.html">Baum-Welch (BW) algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="AppendixD-LDA-details.html">LDA details</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">AppendixD: More RNNs, LSTMs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Attribution</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../attribution.html">Attributions</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/UBC-MDS/DSCI_575_adv-mach-learn" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/AppendixE-LSTMs.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>AppendixD: More RNNs, LSTMs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-plan-imports-lo">Lecture plan, imports, LO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attributions">Attributions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-motivation">LSTM motivation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-rnns">Problems with RNNs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1">Problem 1</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2-vanishing-gradients">Problem 2: Vanishing gradients</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-for-image-captioning">LSTM for image captioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstms-for-video-captioning">LSTMs for video captioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstms-for-executing-python-programs">LSTMs for executing Python programs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#long-short-term-memory-networks-lstms">Long short-term memory networks (LSTMs)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Long Short Term Memory networks (LSTMs)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-vanilla-rnn">Simple (Vanilla) RNN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Long Short Term Memory networks (LSTMs)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstms-controlling-the-flow-of-information-using-gates">LSTMs: Controlling the flow of information using gates</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstms-the-core-idea">LSTMs: The core idea</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-lstm-work">How does LSTM work?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forget-operation">Forget operation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#store-operation">Store operation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-of-new-candidate-values">Vector of new candidate values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#update-operation-memory-cell-update">Update operation (memory cell update)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-operation">Output operation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output-update">Output update</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lstms-confusing-diagrams">LSTMs: Confusing diagrams!!!</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-build-lstms">Let’s build LSTMs!!!</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-some-data">Creating some data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-lstm-module">Defining the LSTM module</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-the-cell-state-and-hidden-state-at-time-step-0">Initializing the cell state and hidden state at time step 0</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation-of-lstm">Forward propagation of LSTM</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#this-was-not-that-that-complicated-but-when-we-want-to-use-it-on-text-data-it-s-a-quite-involved-process">This was not that that complicated but when we want to use it on text data it’s a quite involved process.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-classification-using-pytorch-and-torchtext">Text classification using <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> and <code class="docutils literal notranslate"><span class="pre">torchtext</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-preprocessing">Text preprocessing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-first-true"><code class="docutils literal notranslate"><span class="pre">batch_first=True</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-first-false-default"><code class="docutils literal notranslate"><span class="pre">batch_first=False</span></code> (default)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchtext"><code class="docutils literal notranslate"><span class="pre">torchtext</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-vs-lstms">RNNs vs. LSTMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#wiki-page-on-lstm-applications">Wiki page on LSTM applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">LSTM for image captioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#music2dance">music2dance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-select-all-of-the-following-statements-which-are-true-iclicker">Exercise: Select all of the following statements which are <strong>True</strong> (iClicker)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-questions-for-discussion">Exercise: Questions for discussion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-comments-and-summary">Final comments and summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-ideas-to-know">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-resources">RNN resources</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><img alt="" src="../_images/575_banner.png" /></p>
<section class="tex2jax_ignore mathjax_ignore" id="appendixd-more-rnns-lstms">
<h1>AppendixD: More RNNs, LSTMs<a class="headerlink" href="#appendixd-more-rnns-lstms" title="Permalink to this heading">#</a></h1>
<p>UBC Master of Data Science program, 2023-24</p>
<p>Instructor: Varada Kolhatkar</p>
<section id="lecture-plan-imports-lo">
<h2>Lecture plan, imports, LO<a class="headerlink" href="#lecture-plan-imports-lo" title="Permalink to this heading">#</a></h2>
<p><br><br></p>
</section>
<section id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">RNN</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.max_colwidth&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><br><br></p>
<section id="learning-outcomes">
<h3>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Permalink to this heading">#</a></h3>
<p>From this lecture you will be able to</p>
<ul class="simple">
<li><p>Broadly explain character-level text generation with RNNs;</p></li>
<li><p>Specify the shapes of weight matrices in RNNs;</p></li>
<li><p>Carry out forward pass with RNNs in <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code>;</p></li>
<li><p>Explain stacked RNNs and bidirectional RNNs and the difference between the two;</p></li>
<li><p>Broadly explain the problem of vanishing gradients;</p></li>
<li><p>Broadly explain the idea of LSTMs at a high level;</p></li>
<li><p>Carry out forward pass with LSTMs in <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code>;</p></li>
<li><p>Explain the shapes of input, output, hidden state and cell state in LSTMs;</p></li>
<li><p>Broadly explain numericalization, batching, and padding in text preprocessings;</p></li>
<li><p>Explain the motivation to use <code class="docutils literal notranslate"><span class="pre">torchtext</span></code>.</p></li>
</ul>
<p><br><br></p>
</section>
<section id="attributions">
<h3>Attributions<a class="headerlink" href="#attributions" title="Permalink to this heading">#</a></h3>
<p>This material is heavily based on <span class="xref myst">Jurafsky and Martin, Chapter 9</span>.</p>
<p><br><br><br><br></p>
</section>
</section>
<section id="lstm-motivation">
<h2>LSTM motivation<a class="headerlink" href="#lstm-motivation" title="Permalink to this heading">#</a></h2>
<section id="problems-with-rnns">
<h3>Problems with RNNs<a class="headerlink" href="#problems-with-rnns" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>In practice, you’ll hardly see people using vanilla RNNs because they are quite hard to train for tasks that require access to distant information.</p></li>
<li><p>Despite having access to the entire previous sequence, the information encoded in hidden states of RNNs is fairly local.</p></li>
</ul>
<section id="problem-1">
<h4>Problem 1<a class="headerlink" href="#problem-1" title="Permalink to this heading">#</a></h4>
<p>Consider the examples below in the context of language modeling.</p>
<blockquote>
<div><p>The students in the exam where the fire alarm is ringing <strong>are</strong> really stressed.</p>
</div></blockquote>
<blockquote>
<div><p>The flies munching on the banana that is lying under the tree which is in full bloom <strong>are</strong> really happy.</p>
</div></blockquote>
<ul class="simple">
<li><p>Assigning high probability to <strong><em>is</em></strong> following <em>alarm</em> is straightforward since it provides a local context for singular agreement.</p></li>
<li><p>However, assigning a high probability to <strong><em>are</em></strong> following <em>ringing</em> is quite difficult because not only the plural <em>students</em> is distant, but also the intervening context involves singular constituents.</p></li>
<li><p>Ideally, we want the network to retain the distant information about the plural <strong><em>students</em></strong> until it’s needed while still processing the intermediate parts of the sequence correctly.</p></li>
</ul>
<ul class="simple">
<li><p>The hidden layer and the weights that determine the values in the hidden layer are asked to perform two tasks simultaneously:</p>
<ul>
<li><p>Providing information useful for current decision</p></li>
<li><p>Updating and carrying forward information required for future decisions</p></li>
</ul>
</li>
</ul>
</section>
<section id="problem-2-vanishing-gradients">
<h4>Problem 2: Vanishing gradients<a class="headerlink" href="#problem-2-vanishing-gradients" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Another difficulty with training RNNs arises from the need to backpropagate the error signal back through time.</p></li>
<li><p>Recall that we learn RNNs with</p>
<ul>
<li><p>Forward pass</p></li>
<li><p>Backward pass (backprop through time)</p></li>
</ul>
</li>
<li><p>Computing new states and output in RNNs</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
s_t = g(Ws_{t-1} + Ux_t + b_1)\\
\hat{y}_t = \text{softmax}(Vs_t + b_2)
\end{split}\]</div>
<p><img alt="" src="../_images/RNN-dynamic-model.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_dynamic_model.png" height="800" width="800">  -->
<!-- </center>     --><p>Recall: Backpropagation through time</p>
<ul class="simple">
<li><p>When we do backprop with feedforward neural networks</p>
<ul>
<li><p>Take the gradient (derivative) of the loss with respect to the parameters.</p></li>
<li><p>Change parameters to minimize the loss.</p></li>
</ul>
</li>
<li><p>In RNNs we use a generalized version of backprop called Backpropogation Through Time (BPTT)</p>
<ul>
<li><p>Calculating gradient at each output depends upon the current time step as well as the previous time steps.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/RNN_loss.png" /></p>
<!-- <center>     -->
<!-- <img src="img/RNN_loss.png" height="800" width="800">  -->
<!-- </center> -->
<p><a class="reference external" href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf">Credit</a></p>
<ul class="simple">
<li><p>So in the backward pass of RNNs, we have to multiply many derivatives together, which very often results in</p>
<ul>
<li><p>vanishing gradients (gradients becoming very small and eventually driven to zero) in case of long sequences</p></li>
</ul>
</li>
<li><p>If we have a vanishing gradient, we might not be able to update our weights reliably.</p></li>
<li><p>Only able to capture short-term dependencies, which kind of defeats the whole purpose of using RNNs.</p></li>
</ul>
<ul class="simple">
<li><p>To address these issues more complex network architectures have been designed with the goal of maintaining relevant context over time by enabling the network to learn to forget the information that is no longer needed and to remember information required for decisions still to come.</p></li>
<li><p>One of the most commonly used such models are</p>
<ul>
<li><p>The Long short-term memory network (LSTM)</p></li>
<li><p>Gated Recurrent Units (GRU)</p></li>
</ul>
</li>
<li><p>Let’s look at some example applications of LSTMs.</p></li>
</ul>
</section>
</section>
<section id="lstm-for-image-captioning">
<h3>LSTM for image captioning<a class="headerlink" href="#lstm-for-image-captioning" title="Permalink to this heading">#</a></h3>
<p><img alt="" src="../_images/RNN_LSTM_image_captioning.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_LSTM_image_captioning.png" height="2000" width="2000">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="https://arxiv.org/pdf/1411.4555.pdf">LSTMs for image captioning</a>)</p>
<p><a class="reference external" href="https://github.com/ryankiros/neural-storyteller">Neural Storyteller</a></p>
<p><img alt="" src="../_images/RNN_example.jpg" /></p>
<!-- <img src="img/RNN_example.jpg" width="500" height="500"> -->
<blockquote>        
<p style="font-size:30px">We were barely able to catch the breeze at the beach , and it felt as if someone stepped out of my mind . She was in love with him for the first time in months , so she had no intention of escaping . The sun had risen from the ocean , making her feel more alive than normal . She 's beautiful , but the truth is that I do n't know what to do ...</p>
</blockquote>    </section>
<section id="lstms-for-video-captioning">
<h3>LSTMs for video captioning<a class="headerlink" href="#lstms-for-video-captioning" title="Permalink to this heading">#</a></h3>
<p><img alt="" src="../_images/RNN_video_captioning2.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_video_captioning2.png" height="1000" width="1000">  -->
<!-- </center> -->
<p>(Credit: <a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Venugopalan_Sequence_to_Sequence_ICCV_2015_paper.pdf">LSTMs for video captioning</a>)</p>
</section>
<section id="lstms-for-executing-python-programs">
<h3>LSTMs for executing Python programs<a class="headerlink" href="#lstms-for-executing-python-programs" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>In 2014, Google researchers built an LSTM that learns to execute simple
Python programs!</p></li>
</ul>
<p><img alt="" src="../_images/RNN_learning_to_execute.png" /></p>
<!-- <img src="img/RNN_learning_to_execute.png" width="1500" height="1500"> -->
<p>(Credit: <a class="reference external" href="https://arxiv.org/pdf/1410.4615.pdf">Learning to execute</a>)</p>
<p><br><br><br><br></p>
</section>
</section>
<section id="long-short-term-memory-networks-lstms">
<h2>Long short-term memory networks (LSTMs)<a class="headerlink" href="#long-short-term-memory-networks-lstms" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Recommendation: Go through these notes once to get a general idea of LSTMs. But try not to get bogged down in all the details.</p></li>
<li><p>I want you to get the general intuition of these models so that you have a general idea of their purpose and how they work.</p></li>
</ul>
<section id="id1">
<h3>Long Short Term Memory networks (LSTMs)<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://www.bioinf.jku.at/publications/older/2604.pdf">Invented in 1997</a> by Hochreiter and Schmidhuber.</p></li>
<li><p>Designed so that model can remember things for a long time (hundreds of time steps)!</p></li>
</ul>
</section>
<section id="simple-vanilla-rnn">
<h3>Simple (Vanilla) RNN<a class="headerlink" href="#simple-vanilla-rnn" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>In a simple RNN, we have a series of these repeating modules.</p></li>
<li><p>How does the information flow in one cell of RNN?</p></li>
</ul>
<p><img alt="" src="../_images/RNN_alternative_representation.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_alternative_representation.png" height="1000" width="1000">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a>)</p>
</section>
<section id="id2">
<h3>Long Short Term Memory networks (LSTMs)<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>In an LSTM, the repeating module is more complicated.</p></li>
<li><p>LSTMs add an explicit context layer to the architecture for context management.</p></li>
<li><p>It’s connected to the hiddent layer via specialized neural units which selectively control the flow of information using gates.</p></li>
</ul>
<p><img alt="" src="../_images/RNN_alternative_representation.png" />
<img alt="" src="../_images/LSTM0.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_alternative_representation.png" height="600" width="600">  -->
<!-- <img src="img/LSTM0.png" height="700" width="800">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a>)</p>
</section>
<section id="lstms-controlling-the-flow-of-information-using-gates">
<h3>LSTMs: Controlling the flow of information using gates<a class="headerlink" href="#lstms-controlling-the-flow-of-information-using-gates" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>LSTMs divide context management into two sub problems: removing information no longer needed from the context and adding information likely to be needed for later decision making.</p></li>
<li><p>The information is added and removed through a structure called gates.</p></li>
<li><p>The gates share a common design pattern: each consists of a feedforward layer followed by a sigmoid activation function, followed by a pointwise multiplication with the layer being gated.</p>
<ul>
<li><p>The sigmoid layer pushes its output to either 0 or 1, deciding how much of each component should be let through.</p></li>
<li><p>Combining this with pointwise multiplication has the effect of a binary mask; the values that align with 1 in the mask are passed through unchanged while the values corresponding to lower values are erased.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/RNN_LSTM2.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_LSTM2.png" height="800" width="800">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a>)</p>
</section>
<section id="lstms-the-core-idea">
<h3>LSTMs: The core idea<a class="headerlink" href="#lstms-the-core-idea" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The core idea in LSTMs is using a context or cell state (memory cell)</p></li>
<li><p>Information can flow along the memory unchanged.</p></li>
<li><p>Information can be removed or written to the cells regulated by gates.</p></li>
</ul>
<p><img alt="" src="../_images/RNN_LSTM0.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_LSTM0.png" height="800" width="800">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a>)</p>
</section>
<section id="how-does-lstm-work">
<h3>How does LSTM work?<a class="headerlink" href="#how-does-lstm-work" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Four operations: forget, store (input), update, output</p></li>
</ul>
<p><img alt="" src="../_images/RNN_LSTM0.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_LSTM0.png" height="800" width="800">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a>)</p>
</section>
<section id="forget-operation">
<h3>Forget operation<a class="headerlink" href="#forget-operation" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The purpose of this gate is to delete information from the context that is no longer needed.</p></li>
<li><p>A sigmoid layer, <strong>forget gate</strong>, decides which values of the memory cell to reset.</p></li>
<li><p>Decides what part of the history is worth forgetting.</p></li>
<li><p><span class="math notranslate nohighlight">\(f_t = \sigma(W_fh_{t-1} + U_fx_t + b_f)\)</span></p></li>
</ul>
<p><img alt="" src="../_images/RNN_LSTM3.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_LSTM3.png" height="800" width="800">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a>)</p>
</section>
<section id="store-operation">
<h3>Store operation<a class="headerlink" href="#store-operation" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Decides what part of the new information is worth storing.</p></li>
<li><p>Two parts:</p>
<ul>
<li><p>A sigmoid layer, <strong>input gate.</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(i_t = \sigma(W_ih_{t-1} + U_ix_t + b_i)\)</span></p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/RNN_LSTM4.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_LSTM4.png" height="800" width="800">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a>)</p>
</section>
<section id="vector-of-new-candidate-values">
<h3>Vector of new candidate values<a class="headerlink" href="#vector-of-new-candidate-values" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>A <span class="math notranslate nohighlight">\(tanh\)</span> layer creates a vector of new candidate values <span class="math notranslate nohighlight">\(\tilde{c}_t\)</span> to write to the memory cell.</p></li>
<li><p><span class="math notranslate nohighlight">\(\tilde{c}_t = tanh(W_{c}h_{t-1} + U_{c}x_t + b_c)\)</span></p></li>
</ul>
<p><img alt="" src="../_images/RNN_LSTM5.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_LSTM5.png" height="800" width="800">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a>)</p>
</section>
<section id="update-operation-memory-cell-update">
<h3>Update operation (memory cell update)<a class="headerlink" href="#update-operation-memory-cell-update" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The previous steps decided which values of the memory cell to reset and overwrite.</p></li>
<li><p>Now the LSTM applies the decisions to the memory cells.</p></li>
<li><p><span class="math notranslate nohighlight">\(c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t\)</span></p></li>
</ul>
<p><img alt="" src="../_images/RNN_LSTM6.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_LSTM6.png" height="800" width="800">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a>)</p>
</section>
<section id="output-operation">
<h3>Output operation<a class="headerlink" href="#output-operation" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The sigmoid layer, <strong>output gate</strong>, decides which values should be sent to the network in the next time step.</p></li>
<li><p><span class="math notranslate nohighlight">\(o_t = \sigma(W_{o}h_{t-1} + U_ox_t + b_o)\)</span></p></li>
</ul>
<p><img alt="" src="../_images/RNN_LSTM7.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_LSTM7.png" height="800" width="800">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a>)</p>
<section id="output-update">
<h4>Output update<a class="headerlink" href="#output-update" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>The memory cell goes through <span class="math notranslate nohighlight">\(tanh\)</span> and is multiplied by the output gate</p></li>
<li><p><span class="math notranslate nohighlight">\(h_t = o_t \odot \tanh(c_t)\)</span></p></li>
</ul>
<p><img alt="" src="../_images/RNN_LSTM8.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_LSTM8.png" height="800" width="800">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a>)</p>
<ul class="simple">
<li><p>So an LSTM unit at one time step takes as input</p>
<ul>
<li><p>the context layer</p></li>
<li><p>the hidden layer from previous time step</p></li>
<li><p>the current input vector</p></li>
</ul>
</li>
<li><p>As output it generates</p>
<ul>
<li><p>Updated context vector</p></li>
<li><p>Updated hidden vector</p></li>
</ul>
</li>
<li><p>The nice thing is that the complexity in LSTMs is encapsulated within a the basic processing unit allowing us to maintain modularity and experiment with different architectures quite easily.</p></li>
<li><p>This modularity is the key to the power and widespread applicability of LSTMs!</p></li>
<li><p>Similar to RNNs it’s possible to stack LSTM layers or use it in a bidirectional setting.</p></li>
<li><p>There are other variants such as Gated Recurrent Units (GRU) which follow similar idea but are less complicated.</p></li>
</ul>
</section>
</section>
</section>
<section id="lstms-confusing-diagrams">
<h2>LSTMs: Confusing diagrams!!!<a class="headerlink" href="#lstms-confusing-diagrams" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>LSTMs are not very intuitive.</p></li>
<li><p>Complicated combination of state in the past, observation at the moment and different ways to either forget the observation or keep it around.</p></li>
<li><p>Famous for confusing illustrative diagrams.</p></li>
</ul>
<p><img alt="" src="../_images/RNN_confusing_LSTMs.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_confusing_LSTMs.png" height="1000" width="1000">  -->
<!-- </center>     -->
<p><br><br><br><br></p>
</section>
<section id="let-s-build-lstms">
<h2>Let’s build LSTMs!!!<a class="headerlink" href="#let-s-build-lstms" title="Permalink to this heading">#</a></h2>
<section id="creating-some-data">
<h3>Creating some data<a class="headerlink" href="#creating-some-data" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># sequence_len, batch_size, input_size</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-2.0929e+00, -8.1991e-01, -4.2104e-01, -9.6200e-01,  1.2825e+00,
           8.7684e-01,  1.6221e+00, -1.4779e+00, -1.7018e+00, -7.4980e-01],
         [-1.1285e+00,  4.1351e-01,  2.8917e-01,  2.2473e+00, -8.0364e-01,
          -2.8084e-01, -4.2036e-01,  1.3111e+00, -2.1993e-01,  2.1895e-01],
         [ 2.0451e-01,  5.1463e-01,  9.9376e-01, -2.5873e-01,  1.5118e-01,
           1.0364e-01, -2.1996e+00, -8.8490e-02, -5.6120e-01,  6.7155e-01],
         [ 6.9330e-01, -9.4872e-01, -1.1440e+00,  2.4362e-01, -5.6734e-02,
           3.7841e-01,  1.6863e+00,  2.5529e-01, -5.4963e-01,  1.0042e+00]],

        [[ 3.5068e-01,  1.5434e+00,  1.4058e-01,  1.0617e+00, -9.9292e-01,
          -1.6025e+00, -1.0764e+00,  9.0315e-01, -1.6461e+00,  1.0720e+00],
         [ 1.5026e+00, -8.1899e-01,  2.6860e-01, -2.2150e+00, -1.3193e+00,
          -2.0915e+00,  1.2767e+00, -9.9480e-01,  1.2176e+00, -2.2817e-01],
         [ 1.3382e+00,  1.9929e+00,  1.3708e+00, -5.0087e-01, -2.3244e+00,
           1.2311e+00, -1.0973e+00, -9.6690e-01,  6.7125e-01, -9.4053e-01],
         [-4.6806e-01,  1.0322e+00, -8.9568e-01,  1.1124e+00, -4.1684e-01,
          -1.7106e+00, -3.2902e-01,  1.3966e+00, -9.9491e-01, -1.5822e-03]],

        [[-5.5601e-01, -2.7202e+00,  5.4215e-01, -1.1541e+00,  7.7631e-01,
          -2.5822e-01, -2.0407e+00, -8.0156e-01, -6.0270e-01, -4.7965e-02],
         [ 5.3490e-01,  1.1031e+00,  1.3334e+00, -1.4053e+00, -5.9217e-01,
          -2.5479e-01, -8.9886e-01,  8.1377e-01,  6.5323e-01,  6.5572e-01],
         [-1.4056e+00, -1.2743e+00,  4.5128e-01, -2.2801e-01,  1.6014e+00,
          -2.2577e+00, -1.8009e+00,  7.0147e-01,  5.7028e-01,  1.8790e+00],
         [-9.1925e-01,  1.0318e+00,  1.4353e+00,  8.8307e-02, -1.2037e+00,
           1.0964e+00,  2.4210e+00,  2.4489e-01,  1.8118e+00, -4.4241e-01]]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="defining-the-lstm-module">
<h3>Defining the LSTM module<a class="headerlink" href="#defining-the-lstm-module" title="Permalink to this heading">#</a></h3>
<p>An LSTM is defined by <code class="docutils literal notranslate"><span class="pre">torch.nn.LSTM</span></code> module with the following parameters.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_size</span></code>: the number of features in x (e.g., size of the word embedding)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_size</span></code>: the number of nodes in the hidden layer</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_layers</span></code>: number of recurrent layers</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lstm_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="initializing-the-cell-state-and-hidden-state-at-time-step-0">
<h3>Initializing the cell state and hidden state at time step 0<a class="headerlink" href="#initializing-the-cell-state-and-hidden-state-at-time-step-0" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>first dimension: number of LSTM layers</p></li>
<li><p>second dimension: batch_size</p></li>
<li><p>third dimension: hidden_size / number of nodes in a hidden layer</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">c0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="forward-propagation-of-lstm">
<h3>Forward propagation of LSTM<a class="headerlink" href="#forward-propagation-of-lstm" title="Permalink to this heading">#</a></h3>
<p>Unlike RNN, LSTM module takes three inputs</p>
<ul class="simple">
<li><p>the initial hidden state for each element in the batch (t=0)</p></li>
<li><p>initial cell state for each element in the batch</p></li>
<li><p>the input features (e.g., embeddings)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># h0 and c0 is optional input, defaults to tensor of 0&#39;s when not provided</span>
<span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">hn</span><span class="p">,</span> <span class="n">cn</span><span class="p">)</span> <span class="o">=</span> <span class="n">lstm_rnn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># output = seq_len, batch_size, hidden_size (output features from last layer of LSTM)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output size: &quot;</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>output size:  torch.Size([3, 4, 5])
</pre></div>
</div>
</div>
</div>
</section>
<section id="this-was-not-that-that-complicated-but-when-we-want-to-use-it-on-text-data-it-s-a-quite-involved-process">
<h3>This was not that that complicated but when we want to use it on text data it’s a quite involved process.<a class="headerlink" href="#this-was-not-that-that-complicated-but-when-we-want-to-use-it-on-text-data-it-s-a-quite-involved-process" title="Permalink to this heading">#</a></h3>
</section>
<section id="text-classification-using-pytorch-and-torchtext">
<h3>Text classification using <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> and <code class="docutils literal notranslate"><span class="pre">torchtext</span></code><a class="headerlink" href="#text-classification-using-pytorch-and-torchtext" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>A lot of work in building LSTMs for text data goes into data preprocessing and getting the text into the suitable format.</p></li>
<li><p>There are a number of steps involved in data preprocessing.</p>
<ul>
<li><p>[ ] Data splitting (train, valid, test)</p></li>
<li><p>[ ] Loading the data files</p></li>
<li><p>[ ] Tokenization</p></li>
<li><p>[ ] Creating a vocabulary: Creating a list of unique words</p></li>
<li><p>[ ] Numericalization: Converting text to a set of integers.</p></li>
<li><p>[ ] Word vectors</p></li>
<li><p>[ ] Embedding lookup</p></li>
<li><p>[ ] Batching</p></li>
</ul>
</li>
</ul>
</section>
<section id="text-preprocessing">
<h3>Text preprocessing<a class="headerlink" href="#text-preprocessing" title="Permalink to this heading">#</a></h3>
<p><img alt="" src="../_images/lstm-preprocess.png" /></p>
<!-- <img src="img/lstm-preprocess.png" height="800" width="800">  --></section>
<section id="batch-first-true">
<h3><code class="docutils literal notranslate"><span class="pre">batch_first=True</span></code><a class="headerlink" href="#batch-first-true" title="Permalink to this heading">#</a></h3>
<p><img alt="" src="../_images/batch_first_true.png" /></p>
<!-- <img src="img/batch_first_true.png" height="800" width="800">  --></section>
<section id="batch-first-false-default">
<h3><code class="docutils literal notranslate"><span class="pre">batch_first=False</span></code> (default)<a class="headerlink" href="#batch-first-false-default" title="Permalink to this heading">#</a></h3>
<p><img alt="" src="../_images/batch_first_false.png" /></p>
<!-- <img src="img/batch_first_false.png" height="800" width="800">  --></section>
<section id="torchtext">
<h3><a class="reference external" href="https://pytorch.org/text/stable/index.html"><code class="docutils literal notranslate"><span class="pre">torchtext</span></code></a><a class="headerlink" href="#torchtext" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>There is a library called <code class="docutils literal notranslate"><span class="pre">torchtext</span></code> to help us with these tasks.</p></li>
<li><p>To use it You will first need to <a class="reference external" href="https://pypi.org/project/torchtext/">install <code class="docutils literal notranslate"><span class="pre">torchtext</span></code></a> in the environment.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">torchtext</span>
</pre></div>
</div>
<ul class="simple">
<li><p>I’ve included an adapted version of <span class="xref myst"><code class="docutils literal notranslate"><span class="pre">torchtext</span></code> tutorial</span> from MDS-CL as an appendix.</p></li>
<li><p>You can also included a demo of <span class="xref myst">text classification using LSTMs</span> as an appendix.</p></li>
</ul>
</section>
<section id="rnns-vs-lstms">
<h3>RNNs vs. LSTMs<a class="headerlink" href="#rnns-vs-lstms" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>RNNs suffer from the problem of vanishing gradients.</p>
<ul>
<li><p>Vanishing gradients lead to difficulty in training.</p></li>
</ul>
</li>
<li><p>We saw that LSTMs mitigate this problem by introducing a cell state and managing the context better.</p></li>
<li><p>They are better suited for sequence modeling and capturing long-distance dependencies.</p></li>
</ul>
<p><img alt="" src="../_images/RNN_alternative_representation.png" />
<img alt="" src="../_images/LSTM0.png" /></p>
<!-- <img src="img/RNN_alternative_representation.png" height="400" width="400">  -->
<!-- <img src="img/LSTM0.png" height="400" width="400">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a>)</p>
<p>There are tons of applications of LSTMs.</p>
</section>
<section id="wiki-page-on-lstm-applications">
<h3>Wiki page on LSTM applications<a class="headerlink" href="#wiki-page-on-lstm-applications" title="Permalink to this heading">#</a></h3>
<p><img alt="" src="../_images/lstm_applications_wiki.png" /></p>
<!-- <img src="img/lstm_applications_wiki.png" width="1500" height="1500"> -->
<p>(Credit: <a class="reference external" href="https://arxiv.org/pdf/1410.4615.pdf">Learning to execute</a>)</p>
</section>
<section id="id3">
<h3>LSTM for image captioning<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p><img alt="" src="../_images/RNN_LSTM_image_captioning.png" /></p>
<!-- <center> -->
<!-- <img src="img/RNN_LSTM_image_captioning.png" height="2000" width="2000">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="https://arxiv.org/pdf/1411.4555.pdf">LSTMs for image captioning</a>)</p>
<p><br><br></p>
</section>
<section id="music2dance">
<h3>music2dance<a class="headerlink" href="#music2dance" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://hcsi.cs.tsinghua.edu.cn/Paper/Paper18/MM18-TANGTAORAN.pdf">Dance with Melody: An LSTM-autoencoder Approach to Music-oriented Dance Synthesis</a></p></li>
<li><p><a class="reference external" href="https://www.cs.ubc.ca/~rozentil/data/music2dance.pdf">Follow the Music: Dance Motion Synthesis Corresponding to Arbitrary Music</a> (UBC researchers)</p></li>
</ul>
<p><img alt="" src="../_images/music2dance.png" /></p>
<!-- <center> -->
<!-- <img src="img/music2dance.png" height="1500" width="1500">  -->
<!-- </center>     -->
<p><br><br><br><br></p>
</section>
</section>
<section id="questions-for-you">
<h2>❓❓ Questions for you<a class="headerlink" href="#questions-for-you" title="Permalink to this heading">#</a></h2>
<p>iClicker cloud join link: https://join.iclicker.com/4QVT4</p>
<section id="exercise-select-all-of-the-following-statements-which-are-true-iclicker">
<h3>Exercise: Select all of the following statements which are <strong>True</strong> (iClicker)<a class="headerlink" href="#exercise-select-all-of-the-following-statements-which-are-true-iclicker" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>(A) Input to an RNN or LSTM is a 3-D tensor with three components: sequence length, batch size, and input size.</p></li>
<li><p>(B) In the forward pass of LSTMs, in addition to the previous hidden state we also pass the previous cell state.</p></li>
<li><p>(C) In LSTMs, creating batches with sequences of similar lengths might minimize padding.</p></li>
<li><p>(D) HMMs can handle variable length sequences but LSTMs cannot.</p></li>
<li><p>(E) It’s almost always a good idea to use LSTMs to model text data over bag-of-words models, as they encode sequential information and are likely to beat bag-of-words models.</p></li>
</ul>
<p><br><br><br><br></p>
<div class="tip dropdown admonition">
<p class="admonition-title">Exercise: V’s Solutions!</p>
<ul class="simple">
<li><p>(A) True</p></li>
<li><p>(B) True</p></li>
<li><p>(C) True</p></li>
<li><p>(D) False</p></li>
<li><p>(E) False</p></li>
</ul>
</div>
</section>
<section id="exercise-questions-for-discussion">
<h3>Exercise: Questions for discussion<a class="headerlink" href="#exercise-questions-for-discussion" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Suppose you are using pre-trained embeddings to train a sentiment analysis model with LSTMs on a small corpus shown below. Now given a test sentence “amazing”, would you expect the model to classify the sentence correctly? Why or why not? State your assumptions.</p></li>
</ol>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>text</p></th>
<th class="head text-center"><p>sentiment</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>what a wonderful movie !</p></td>
<td class="text-center"><p>positive</p></td>
</tr>
<tr class="row-odd"><td><p>boring … fell asleep a couple of times</p></td>
<td class="text-center"><p>negative</p></td>
</tr>
<tr class="row-even"><td><p>astonishing ! !</p></td>
<td class="text-center"><p>positive</p></td>
</tr>
</tbody>
</table>
<ol class="arabic simple" start="2">
<li><p>In text generation using LSTMs or RNNs, why it might be a good idea to sample from the distribution rather than picking the next word with highest probability?</p></li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Exercise 8.2: V’s Solutions!</p>
<ol class="arabic simple">
<li><p>Most likely yes. Although the word <strong>amazing</strong> does not occur in the training corpus, we would be incorporating the information in the model, via pre-trained word embeddings, that the word amazing is similar to wonderful and astonishing which are associated with positive sentiment.</p></li>
<li><p>To add novelty.</p></li>
</ol>
</div>
<p><br><br><br><br></p>
</section>
</section>
<section id="final-comments-and-summary">
<h2>Final comments and summary<a class="headerlink" href="#final-comments-and-summary" title="Permalink to this heading">#</a></h2>
<section id="important-ideas-to-know">
<h3>Important ideas to know<a class="headerlink" href="#important-ideas-to-know" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>RNNS and LSTMs are well suited for sequence modeling tasks.</p></li>
<li><p>They are widely used models to process sequential data in deep learning community and have a wide range of applications.</p></li>
<li><p>Training RNNs is hard because of vanishing and exploding gradients.</p></li>
<li><p>LSTMs mitigate the problem by introducing a mechanism to selectively control the flow of information in the network.</p></li>
</ul>
</section>
<section id="id4">
<h3>Important ideas to know<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>LSTMs use <strong>gates to control the flow of information</strong>.</p></li>
<li><p>They maintain <strong>a separate cell or context state</strong> in addition to the hidden state.</p></li>
<li><p>Maintaining cell or context state allows for efficient training with back-propagation through time.</p></li>
</ul>
</section>
<section id="id5">
<h3>Important ideas to know<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The shapes of weight matrices (<span class="math notranslate nohighlight">\(U, V, W\)</span>) in vanilla RNNs.</p></li>
<li><p>Forward pass of RNNs and LSTMs.</p></li>
<li><p>The shapes of input, output, and hidden and cell states of LSTM.</p></li>
</ul>
</section>
<section id="id6">
<h3>Important ideas to know<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>If you want to use LSTMs for text data a number of steps are involved before passing text data to LSTMs including</p>
<ul>
<li><p>tokenization</p></li>
<li><p>embedding lookup</p></li>
<li><p>numericalization</p></li>
<li><p>batching</p></li>
<li><p>padding</p></li>
</ul>
</li>
<li><p>We’ll use a library called <a class="reference external" href="https://pytorch.org/text/stable/index.html"><code class="docutils literal notranslate"><span class="pre">torchtext</span></code></a> to help us with some of the preprocessing tasks.</p></li>
</ul>
<p><br><br><br><br></p>
</section>
<section id="rnn-resources">
<h3>RNN resources<a class="headerlink" href="#rnn-resources" title="Permalink to this heading">#</a></h3>
<p>A lot of material is available on the web. Here are some resources that were useful for me.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">An illustration of LSTMs with nice visualizations</a></p></li>
<li><p>Geoff Hinton’s <a class="reference external" href="https://www.youtube.com/watch?v=93rzMHtYT_0">short talk</a> and <a class="reference external" href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec7.pdf">lecture</a> on LSTMs</p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=AYku9C9XoB8&amp;t=884s">Yoshua Bengio’s lecture</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/results?search_query=ali+ghodsi+RNNs">Ali Ghodsi’s lecture on RNNs</a></p></li>
<li><p>Richard Socher’s <a class="reference external" href="https://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf">slides</a> and <a class="reference external" href="https://www.youtube.com/watch?v=Keqep_PKrY8">lecture</a> on RNNs</p></li>
<li><p><a class="reference external" href="https://github.com/ajhalthor/awesome-rnn">A list of RNN resources</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=SEnXr6v2ifU">MIT 6.S191 (2020): Recurrent Neural Networks</a></p></li>
</ul>
<p><br><br><br><br></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-env-563-py"
        },
        kernelOptions: {
            name: "conda-env-563-py",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-env-563-py'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="AppendixD-LDA-details.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">LDA details</p>
      </div>
    </a>
    <a class="right-next"
       href="../attribution.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Attributions</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-plan-imports-lo">Lecture plan, imports, LO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attributions">Attributions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-motivation">LSTM motivation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-rnns">Problems with RNNs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1">Problem 1</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2-vanishing-gradients">Problem 2: Vanishing gradients</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-for-image-captioning">LSTM for image captioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstms-for-video-captioning">LSTMs for video captioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstms-for-executing-python-programs">LSTMs for executing Python programs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#long-short-term-memory-networks-lstms">Long short-term memory networks (LSTMs)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Long Short Term Memory networks (LSTMs)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-vanilla-rnn">Simple (Vanilla) RNN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Long Short Term Memory networks (LSTMs)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstms-controlling-the-flow-of-information-using-gates">LSTMs: Controlling the flow of information using gates</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstms-the-core-idea">LSTMs: The core idea</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-lstm-work">How does LSTM work?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forget-operation">Forget operation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#store-operation">Store operation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-of-new-candidate-values">Vector of new candidate values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#update-operation-memory-cell-update">Update operation (memory cell update)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-operation">Output operation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output-update">Output update</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lstms-confusing-diagrams">LSTMs: Confusing diagrams!!!</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-build-lstms">Let’s build LSTMs!!!</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-some-data">Creating some data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-lstm-module">Defining the LSTM module</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-the-cell-state-and-hidden-state-at-time-step-0">Initializing the cell state and hidden state at time step 0</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation-of-lstm">Forward propagation of LSTM</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#this-was-not-that-that-complicated-but-when-we-want-to-use-it-on-text-data-it-s-a-quite-involved-process">This was not that that complicated but when we want to use it on text data it’s a quite involved process.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-classification-using-pytorch-and-torchtext">Text classification using <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> and <code class="docutils literal notranslate"><span class="pre">torchtext</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-preprocessing">Text preprocessing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-first-true"><code class="docutils literal notranslate"><span class="pre">batch_first=True</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-first-false-default"><code class="docutils literal notranslate"><span class="pre">batch_first=False</span></code> (default)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchtext"><code class="docutils literal notranslate"><span class="pre">torchtext</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-vs-lstms">RNNs vs. LSTMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#wiki-page-on-lstm-applications">Wiki page on LSTM applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">LSTM for image captioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#music2dance">music2dance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-select-all-of-the-following-statements-which-are-true-iclicker">Exercise: Select all of the following statements which are <strong>True</strong> (iClicker)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-questions-for-discussion">Exercise: Questions for discussion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-comments-and-summary">Final comments and summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-ideas-to-know">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-resources">RNN resources</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Varada Kolhatkar
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>