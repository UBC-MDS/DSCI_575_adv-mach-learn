{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spam classification using LSTMs: Demo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.legacy.data import (\n",
    "    BucketIterator,\n",
    "    Field,\n",
    "    Iterator,\n",
    "    LabelField,\n",
    "    TabularDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN architectures\n",
    "\n",
    "- We have seen before that a number of RNN architectures are possible. \n",
    "\n",
    "<img src=\"img/RNN_architectures.png\" height=\"1500\" width=\"1500\">     \n",
    "\n",
    "[source](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM text classification \n",
    "\n",
    "- An example of many-to-one architecture is **text classification**. \n",
    "- In this notebook we are going to build an LSTM for spam classification. \n",
    "\n",
    "<center>\n",
    "<img src=\"img/lstm-text-classification.png\" height=\"800\" width=\"800\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "We'll use [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                sms\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df = pd.read_csv(\"data/spam.csv\", encoding=\"latin-1\")\n",
    "sms_df = sms_df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "sms_df = sms_df.rename(columns={\"v1\": \"label\", \"v2\": \"sms\"})\n",
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's create train/test/validation CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_test_df = train_test_split(sms_df, test_size=0.2, random_state=123)\n",
    "valid_df, test_df = train_test_split(valid_test_df, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = \"./data/sms_split/\"\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cols = [\"sms\", \"label\"]\n",
    "data_dir = \"data/sms/\"\n",
    "train_df.to_csv(data_path + \"train.csv\", columns=cols, index=False)\n",
    "valid_df.to_csv(data_path + \"valid.csv\", columns=cols, index=False)\n",
    "test_df.to_csv(data_path + \"test.csv\", columns=cols, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have `train.csv`, `valid.csv`, and `test.csv` files written under the `data_path` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Steps involved\n",
    "\n",
    "- Text preprocessing \n",
    "- Defining the network architecture and the forward method for the network   \n",
    "- Training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preprocessing with `torchtext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We'll be using `torchtext` to carry out preprocessing. \n",
    "\n",
    "<center>\n",
    "<img src=\"img/lstm-preprocess.png\" height=\"600\" width=\"600\"> \n",
    "</center>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_nltk(text):\n",
    "    \"\"\"\n",
    "    Simple tokenization on white spaces.\n",
    "    \"\"\"\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_nltk(\"This is a test! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining `TEXT` and `LABEL` fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# treat it as sequential data\n",
    "TEXT = Field(sequential=True, tokenize=tokenize_nltk, lower=True)\n",
    "\n",
    "# Don not treat labels as sequential data\n",
    "LABEL = Field(sequential=False, unk_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fields = [(\"sms\", TEXT), (\"label\", LABEL)]\n",
    "train, valid, test = TabularDataset.splits(\n",
    "    path=data_path,  # the root directory where the data lies\n",
    "    train=\"train.csv\",\n",
    "    validation=\"valid.csv\",\n",
    "    test=\"test.csv\",\n",
    "    format=\"csv\",\n",
    "    skip_header=True,\n",
    "    fields=fields,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Build vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab:  2557\n",
      "Number of classes:  2\n"
     ]
    }
   ],
   "source": [
    "# It would be better to use glove.twitter.27B.100d here. I'm using the following to save time.\n",
    "TEXT.build_vocab(train, min_freq=3, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "print(\"Size of vocab: \", len(TEXT.vocab))\n",
    "print(\"Number of classes: \", len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create data iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(32, 32, 32),\n",
    "    sort_key=lambda x: len(x.sms),\n",
    "    sort=True,\n",
    "    sort_within_batch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examining batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for batch in train_iter:\n",
    "    if count == 4:\n",
    "        messages_b4 = batch.sms\n",
    "        labels_b4 = batch.label\n",
    "\n",
    "    if count == 10:\n",
    "        messages_b10 = batch.sms\n",
    "        labels_b10 = batch.label\n",
    "        break\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_b4.shape  # sequence_len, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_b10.shape  # sequence_len, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def print_preprocessed_examples(messages, labels, n=4):\n",
    "    print(\"preprocessed corpus:\")\n",
    "    df_data = defaultdict(list)\n",
    "    for j in range(n):  # sample loop\n",
    "        df_data[\"tokens\"].append(\n",
    "            [TEXT.vocab.itos[messages[i, j]] for i in range(messages.shape[0])]\n",
    "        )\n",
    "        df_data[\"example\"].append(j)\n",
    "        df_data[\"label\"] = labels[j].item()\n",
    "    return pd.DataFrame(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed corpus:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>example</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[what, happen, dear, tell, me]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[&lt;unk&gt;, between, 10am-7pm, cost, 10p]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[he, is, a, &lt;unk&gt;, &lt;unk&gt;]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[wat, u, doing, there, ?]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tokens  example  label\n",
       "0         [what, happen, dear, tell, me]        0      0\n",
       "1  [<unk>, between, 10am-7pm, cost, 10p]        1      0\n",
       "2              [he, is, a, <unk>, <unk>]        2      0\n",
       "3              [wat, u, doing, there, ?]        3      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_preprocessed_examples(messages_b4, labels_b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed corpus:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>example</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[what, about, this, one, then, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[prepare, to, be, &lt;unk&gt;, :, )]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[sorry, ,, i, 'll, call, later]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[its, a, part, of, checking, iq]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              tokens  example  label\n",
       "0  [what, about, this, one, then, .]        0      0\n",
       "1     [prepare, to, be, <unk>, :, )]        1      0\n",
       "2    [sorry, ,, i, 'll, call, later]        2      0\n",
       "3   [its, a, part, of, checking, iq]        3      0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_preprocessed_examples(messages_b10, labels_b10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Embedding layer\n",
    "\n",
    "- The embedding layer in `Pytorch` is where we pass our vocabulary to get back a word vector for each word in the vocabulary. \n",
    "- This is our embedding lookup table.\n",
    "\n",
    "<center>\n",
    "<img src=\"img/embedding_layer.png\" height=\"700\" width=\"700\"> \n",
    "</center>    \n",
    "\n",
    "[Source](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating an embedding lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2557"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD_VEC_SIZE = 100\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding lookup table shape =  torch.Size([2557, 100])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(VOCAB_SIZE, WORD_VEC_SIZE)\n",
    "print(\"Embedding lookup table shape = \", embedding.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 9.8933e-01,  1.0475e+00, -4.8583e-01,  ..., -1.4443e+00,\n",
       "         -1.5130e+00,  1.0711e+00],\n",
       "        [ 5.6870e-01, -8.6652e-01, -1.4143e+00,  ..., -1.2739e+00,\n",
       "         -1.3763e+00, -9.3116e-01],\n",
       "        [ 1.0930e+00,  6.1339e-01,  5.4294e-01,  ...,  1.0198e-01,\n",
       "         -4.8129e-01, -1.5260e-04],\n",
       "        ...,\n",
       "        [-1.3053e+00,  1.4037e-01, -4.6418e-01,  ...,  2.7488e-01,\n",
       "         -3.0211e-02,  1.3917e-01],\n",
       "        [-1.6779e-01,  1.5651e+00,  3.7909e-01,  ..., -8.1619e-02,\n",
       "         -1.1620e+00,  3.1804e-01],\n",
       "        [ 5.9278e-02,  2.3898e-03, -6.1134e-01,  ...,  1.2081e+00,\n",
       "         -8.1040e-01,  4.1732e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `PyTorch` initializes word vectors with a normal distribution. \n",
    "- The word embedding weights are by default learnable parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Initializing word vectors\n",
    "\n",
    "- But they could be initialized with external pre-trained word embeddings such as `GloVe` or `fasttext`. \n",
    "- The weights could be frozen (`freeze=True`) or we could choose to keep learning them with the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we define our model, we will initialize embedding weights with pre-trained embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Input to LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32, 100])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_b4_embeddings = embedding(messages_b4)\n",
    "messages_b4_embeddings.shape  # sequence_length, batch_size, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7803, -0.7371, -0.4452, -0.1246, -0.6809, -0.4757,  1.0876,  1.1386,\n",
       "        -1.7562, -0.8781, -0.2473, -0.8505,  1.5113,  0.6343, -0.9098, -1.1002,\n",
       "         0.3118,  1.8158,  0.2263, -0.7970, -0.5171, -0.4518, -0.2540,  0.8129,\n",
       "        -1.7383, -1.5922,  0.0507, -0.7480, -0.0856, -1.8469,  0.0423, -0.1457,\n",
       "         0.2489,  0.0788,  0.8069,  1.4356, -0.3759, -1.3385, -0.7030, -0.2304,\n",
       "         0.2911, -0.0935, -0.6701, -1.4933, -0.3423,  0.2331,  2.9337, -1.5512,\n",
       "         0.4270,  0.0139,  0.6805,  1.1590, -0.6142,  1.7147, -1.0021, -0.7155,\n",
       "         0.2496, -0.4726, -0.4040, -1.7886, -0.4634,  0.7034, -0.4215, -0.6943,\n",
       "         2.1463,  1.3065, -0.2575, -1.5041,  0.8911,  0.4946, -0.3457, -0.3844,\n",
       "         0.4186, -1.4241,  0.9184,  0.4148,  2.2295, -1.3660, -0.9637, -0.0531,\n",
       "        -2.0992, -0.5671, -1.0233,  0.6194,  0.0986, -0.5512, -0.2059,  1.7479,\n",
       "         0.9942,  0.9426, -0.2746,  0.5260,  0.5577, -0.0053,  0.2977,  0.3086,\n",
       "         0.1106, -1.2561,  1.0544, -0.7437], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_b10_embeddings = embedding(messages_b10)\n",
    "messages_b10_embeddings.shape  # sequence_length, batch_size, embedding_size\n",
    "messages_b10_embeddings[0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining LSTM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our network will have the following layers. \n",
    "- Embedding layer ([`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html))\n",
    "- One or more unidirectional LSTM layers ([`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html))\n",
    "- An activation function layer for non linearity ([`nn.Tanh`](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html) or [`nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html))\n",
    "- A linear layer ([`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html))\n",
    "- A `LogSoftmax` layer on top of the output of linear layer ([`nn.LogSoftmax`](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html)).\n",
    "\n",
    "Return the output of `LogSoftmax` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_size, vocab_size, output_size, hidden_size, num_layers, dropout\n",
    "    ):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embedding_size\n",
    "        )\n",
    "        \n",
    "        self.lstm_rnn = nn.LSTM(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.activation_fn = nn.ReLU()\n",
    "        self.linear_layer = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax_layer = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out, (h_state, c_state) = self.lstm_rnn(\n",
    "            out\n",
    "        )  # c_0 and h_0 initialized to zeros by default\n",
    "\n",
    "        # classify based on the hidden representation at the last token\n",
    "        out = out[-1]\n",
    "        out = self.activation_fn(out)\n",
    "        out = self.linear_layer(out)\n",
    "        out = self.softmax_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An embedding layer (`nn.Embedding`)\n",
    "\n",
    "- Creates a lookup table \n",
    "- input dimension equal to the size of your `TEXT` vocabulary \n",
    "- the output as a vector of size `embedding_size` \n",
    "- We could initialize embeddings with random weights and learn as part of the training process. This way we get task-specific embeddings. \n",
    "- For example, the parameters of this embedding layer could be randomly initialized with numbers sampled from a normal distribution. \n",
    "- We could also initialize these weights with pre-trained embeddings (e.g., `Glove` and `fasttext`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM layers (`nn.LSTM`)\n",
    "\n",
    "- We pass input size, hidden size, number of layers, dropout etc. \n",
    "- We can have one or more `LSTM` layers. \n",
    "- In `PyTorch` we'll define more than one layers by setting `num_layers` parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activation functions (`nn.Tanh` or `nn.ReLU`)\n",
    "\n",
    "- We pass the last hidden layer into an activation function before feeding it into a linear layer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear layer (`nn.Linear`)\n",
    "\n",
    "- The dimensionality of this layer is equal to the number of classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Softmax layer (`nn.LogSoftmax`)\n",
    "\n",
    "- The `LogSoftmax` layer gives the log probabilities of each class. \n",
    "- We can pick the class with maximum log probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest of the pipeline looks similar to feedforward neural networks code (except that we are using `torchtext` instead of `DataLoader`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_seed = 123\n",
    "torch.manual_seed(manual_seed)  # set the seed (for reproducibility)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given a data iterator, the `train` and `evaluate` functions below train and evaluate the model for all batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_samples = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.sms\n",
    "        batch_output = batch.label\n",
    "\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        model_outputs = model(batch_input)\n",
    "\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_samples += batch_output.shape[0]\n",
    "\n",
    "    return total_loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():  # for efficiency\n",
    "        for batch in dataloader:\n",
    "            # load the current batch\n",
    "            try:\n",
    "                batch_input = batch.sms\n",
    "                batch_output = batch.label\n",
    "\n",
    "                batch_input = batch_input.to(device)\n",
    "                batch_output = batch_output.to(device)\n",
    "\n",
    "                # forward propagation\n",
    "                model_outputs = model(batch_input)\n",
    "\n",
    "                # identify the predicted class for each example in the batch\n",
    "                probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "\n",
    "                preds.extend(predicted)\n",
    "                labels.extend(batch_output)\n",
    "            except:\n",
    "                print(\"Error calculating predictions\")\n",
    "                print(batch)\n",
    "\n",
    "    accuracy = accuracy_score(preds, labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128  # number of units in the hidden layer\n",
    "NUM_LAYERS = 2  # number of hidden layers\n",
    "MAX_EPOCHS = 4  # number of passes over the training data\n",
    "LEARNING_RATE = 0.3  # learning rate for the weight update rule\n",
    "NUM_CLASSES = 2  # number of classes for the problem\n",
    "EMBEDDING_SIZE = 100  # size of the word embedding\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (embedding): Embedding(2557, 100)\n",
      "  (lstm_rnn): LSTM(100, 128, num_layers=2, dropout=0.2)\n",
      "  (activation_fn): ReLU()\n",
      "  (linear_layer): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (softmax_layer): LogSoftmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "\n",
    "model = LSTMModel(\n",
    "    EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS, DROPOUT\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);  # ship the  to the right device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a directory for writing models.\n",
    "import os\n",
    "\n",
    "CHECKPOINT_PATH = \"./checkpoint\"\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    os.mkdir(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()  # define the loss function (last node of the network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Initializing embedding weights with pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2557, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Carrying out one forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 32])\n"
     ]
    }
   ],
   "source": [
    "print(messages_b4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  0.7440012693405151\n"
     ]
    }
   ],
   "source": [
    "preds = model(messages_b4)\n",
    "loss = criterion(preds, labels_b4)\n",
    "print(\"loss = \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 0.0093, Train accuracy 0.8622; Validation accuracy 0.8869\n",
      "Epoch 2, Loss 0.0067, Train accuracy 0.8618; Validation accuracy 0.8851\n",
      "Epoch 3, Loss 0.0049, Train accuracy 0.9349; Validation accuracy 0.9461\n",
      "Epoch 4, Loss 0.0041, Train accuracy 0.9435; Validation accuracy 0.9551\n"
     ]
    }
   ],
   "source": [
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "epochs, losses, train_accs, valid_accs = [], [], [], []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # train the model for one pass over the data\n",
    "    train_loss = train(train_iter)\n",
    "    losses.append(train_loss)\n",
    "\n",
    "    # compute the training accuracy\n",
    "    train_acc = evaluate(train_iter)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # compute the validation accuracy\n",
    "    valid_acc = evaluate(valid_iter)\n",
    "    valid_accs.append(valid_acc)\n",
    "\n",
    "    epochs.append(epoch + 1)\n",
    "\n",
    "    # print the loss for every epoch\n",
    "    print(\n",
    "        \"Epoch %d, Loss %0.4f, Train accuracy %0.4f; Validation accuracy %0.4f\"\n",
    "        % (epoch + 1, train_loss, train_acc, valid_acc)\n",
    "    )\n",
    "\n",
    "    # save model, optimizer, and number of epoch to a dictionary\n",
    "    model_save = {\n",
    "        \"epoch\": epoch,  # number of epoch\n",
    "        \"model_state_dict\": model.state_dict(),  # model parameters\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),  # save optimizer\n",
    "        \"loss\": train_loss,  # training loss\n",
    "    }\n",
    "    torch.save(model_save, CHECKPOINT_PATH + \"/model_{}.pt\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
