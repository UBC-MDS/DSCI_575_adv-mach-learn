{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DSCI 575: Advanced Machine Learning \n",
    "## (with an undercurrent of Natural Language Processing (NLP) applications)\n",
    "\n",
    "\n",
    "## Lecture 5: The Viterbi algorithm and topic modeling \n",
    "\n",
    "UBC Master of Data Science program, 2018-19\n",
    "\n",
    "Instructor: Varada Kolhatkar [ʋəɾəda kɔːlɦəʈkər]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# And import the libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Thanks to Firas for the following code to make jupyter RISE slides pretty! \n",
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "from pathlib import Path\n",
    "path = Path.home() / \".jupyter\" / \"nbconfig\"\n",
    "cm = BaseJSONConfigManager(config_dir=str(path))\n",
    "tmp = cm.update(\n",
    "        \"rise\",\n",
    "        {\n",
    "            \"theme\": \"serif\",\n",
    "            \"transition\": \"fade\",\n",
    "            \"start_slideshow_at\": \"selected\",            \n",
    "            \"width\": \"100%\",\n",
    "            \"height\": \"100%\",\n",
    "            \"header\": \"\",\n",
    "            \"footer\":\"\",\n",
    "            \"scroll\": True,\n",
    "            \"enable_chalkboard\": True,\n",
    "            \"slideNumber\": True,\n",
    "            \"center\": False,\n",
    "            \"controlsLayout\": \"edges\",\n",
    "            \"slideNumber\": True,\n",
    "            \"hash\": True,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "     font-size: 130%;\n",
       "}\n",
       "\n",
       "body.rise-enabled div.inner_cell>div.input_area {\n",
       "    font-size: 100%;\n",
       "}\n",
       "\n",
       "body.rise-enabled div.output_subarea.output_text.output_result {\n",
       "    font-size: 100%;\n",
       "}\n",
       "body.rise-enabled div.output_subarea.output_text.output_stream.output_stdout {\n",
       "  font-size: 150%;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
    "     font-size: 130%;\n",
    "}\n",
    "\n",
    "body.rise-enabled div.inner_cell>div.input_area {\n",
    "    font-size: 100%;\n",
    "}\n",
    "\n",
    "body.rise-enabled div.output_subarea.output_text.output_result {\n",
    "    font-size: 100%;\n",
    "}\n",
    "body.rise-enabled div.output_subarea.output_text.output_stream.output_stdout {\n",
    "  font-size: 150%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim \n",
    "\n",
    "from gensim import matutils, models\n",
    "import string\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning outcomes\n",
    "\n",
    "From this lesson you will be able to\n",
    "\n",
    "- explain the general idea and purpose of the Viterbi algorithm\n",
    "- apply the Viterbi algorithm given an HMM\n",
    "- explain the general idea of topic modeling\n",
    "- explain the data generation process given a topic model \n",
    "- explain at a high-level how to go from raw data to topics \n",
    "- carry out topic modeling by training an [LDA model with gensim](https://radimrehurek.com/gensim/models/ldamodel.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Three fundamental questions for an HMM\n",
    "\n",
    "#### Likelihood\n",
    "Given a model with parameters $\\theta = <\\pi, T, B>$, how do we efficiently compute the likelihood of a particular observation sequence $O$?\n",
    "#### Decoding\n",
    "Given an observation sequence $O$ and a model $\\theta$ how do we choose a state sequence $Q={q_0, q_1, \\dots q_T}$ that best explains the observation sequence?\n",
    "#### Learning\n",
    "Training: Given a large observation sequence $O$ how do we choose the best parameters $\\theta$ that explain the data $O$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decoding\n",
    "\n",
    "- Given an observation sequence $O$ and a model $\\theta$ how do we choose a state sequence $Q={q_0, q_1, \\dots q_T}$ that best explains the observation sequence?\n",
    "- Purpose: finding what's most likely going on under the hood. \n",
    "- For example: It tells us the most likely part-of-speech tags given an English sentence.\n",
    "\n",
    "<blockquote>\n",
    "Will/MD tshe/DT chair/NN chair/VB the/DT meeting/NN from/IN that/DT chair/NN?\n",
    "</blockquote>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### The Viterbi algorithm: Choosing $Q={q_{0:T}}$\n",
    "\n",
    "- Given an HMM, choose the state sequence that maximizes the probability of the output sequence.  \n",
    " * $Q^* = \\arg \\max\\limits_Q P(O,Q;\\theta)$, \n",
    " * $P(O,Q;\\theta) = \\pi_{q_0}b_{q_0}(o_0) \\prod\\limits_{t=1}^{T}a_{q_{t-1}}a_{q_t}b_{q_t}(o_t)$\n",
    "\n",
    "<img src=\"images/HMM_example_trellis.png\" height=\"700\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Viterbi algorithm: Choosing $Q={q_{0:T}}$\n",
    "\n",
    "- Dynamic programming algorithm.\n",
    "- We use a different kind of trellis.\n",
    "- Want: Given an HMM, choose the state sequence that maximizes the probability of the output sequence.  \n",
    " * $Q^* = \\arg \\max\\limits_Q P(O,Q;\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Viterbi algorithm: Choosing $Q={q_{0:T}}$\n",
    "\n",
    "- We store $\\delta$ and $\\psi$ values at each node in the trellis\n",
    "\n",
    "- $\\delta_i(t)$ = the probability of the most probable path leading to the trellis node at state $i$ and time $t$\n",
    "- $\\psi_i(t) =$ The best possible previous state if I am in state $i$ at time $t$. \n",
    "\n",
    "<img src=\"images/HMM_example_trellis.png\" height=\"700\" width=\"700\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Viterbi: Initialization\n",
    "- Initialize with $\\delta_i(0) = \\pi_i b_i(o_0)$ for all states\n",
    "    - $\\delta_🙂(0) = \\pi_🙂 b_🙂(E) = 0.8 \\times 0.2 = 0.16$\n",
    "    - $\\delta_😔(0) = \\pi_😔 b_😔(E) = 0.2 \\times 0.1 = 0.02$\n",
    "    \n",
    "- Initialize with $\\psi_i(0) = 0 $, for all states   \n",
    "    - $\\psi_🙂(0) = 0, \\psi_😔(0) = 0$\n",
    "    \n",
    "<img src=\"images/HMM_example_trellis.png\" height=\"700\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Viterbi: Induction\n",
    "\n",
    "The best path $\\delta_t$ to state $j$ at time $t$ depends on the best path to each\n",
    "possible previous state $\\delta_i(t-1)$ and their transitions to $j$ ($a_{ij}$). \n",
    "\n",
    "- $\\delta_j(t) = \\max\\limits_i [\\delta_i(t-1)a_{ij}] b_j(o_t)$\n",
    "- $\\psi_j(t) = \\arg \\max\\limits_i [\\delta_i(t-1)a_{ij}] $\n",
    "\n",
    "<img src=\"images/HMM_example_trellis.png\" height=\"800\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Viterbi induction: $\\delta$ and $\\psi$ intuition\n",
    "\n",
    "<img src=\"images/viterbi_explanation.png\" height=\"150\" width=\"150\"> \n",
    "\n",
    "\n",
    "- There are two possible paths to state 🙂 at $T = 1$. Which is the best one? \n",
    "- $\\delta_🙂(1) = \\max \\begin{bmatrix} \\delta_🙂(0) \\times a_{🙂🙂} \\\\ \\delta_😔(0) \\times a_{😔🙂}\\end{bmatrix}  \\times b_🙂(L)$\n",
    "- First take the max between $\\delta_🙂(0) \\times a_{🙂🙂}$ and $\\delta_😔(0) \\times a_{😔🙂}$ and then multiply the max by $b_🙂(L)$.   \n",
    "    \n",
    "- $\\psi_🙂(1)$ = the state at $T=0$ from where the path to 🙂 at $T=1$ was the best one.     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Viterbi:  notation\n",
    "\n",
    "- **Note that we use square parentheses to show two quantities for taking the max. (Not the best notation but I have seen it being used in this context.)**  \n",
    "- $\\delta_🙂(1) = \\max \\begin{bmatrix} \\delta_🙂(0) \\times a_{🙂🙂} \\\\ \\delta_😔(0) \\times a_{😔🙂}\\end{bmatrix}  \\times b_🙂(L)$\n",
    "- First take the max between $\\delta_🙂(0) \\times a_{🙂🙂}$ and $\\delta_😔(0) \\times a_{😔🙂}$ and then multiply the max by $b_🙂(L)$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Viterbi: Induction (T = 1)\n",
    "\n",
    "$\\delta$ and $\\psi$ at state 🙂 and T = 1\n",
    "- $\\delta_{🙂}(1) = \\max\\limits_i [\\delta_i(0)a_{ij}] b_j(o_t) = \n",
    "\\max \\begin{bmatrix} 0.16 \\times 0.7 \\\\ 0.02 \\times 0.4\\end{bmatrix} \\times 0.7 = 0.0784$\n",
    "- $\\psi_{🙂}(1) = \\arg \\max\\limits_i [\\delta_i(0)a_{ij}] = 🙂$\n",
    "\n",
    "$\\delta$ and $\\psi$ at state 😔 and T = 1\n",
    "- $\\delta_{😔}(1) = \\max\\limits_i [\\delta_i(0)a_{ij}] b_j(o_t) =  \\max \\begin{bmatrix} 0.16 \\times 0.3 \\\\ 0.02 \\times 0.6\\end{bmatrix} \\times 0.1 = 0.0048$\n",
    "- $\\psi_{😔}(1) = \\arg \\max\\limits_i [\\delta_i(0)a_{ij}] = 🙂$\n",
    "\n",
    "<img src=\"images/HMM_example_trellis.png\" height=\"800\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Viterbi: Induction (T = 1)\n",
    "\n",
    "$\\delta$ and $\\psi$ at state 🙂 and T = 1\n",
    "- $\\delta_{🙂}(1) = \\max \\begin{bmatrix} \\delta_🙂(0) \\times a_{🙂🙂} \\\\ \\delta_😔(0) \\times a_{😔🙂}\\end{bmatrix}  \\times b_🙂(L) = \n",
    "\\max \\begin{bmatrix} 0.16 \\times 0.7 \\\\ 0.02 \\times 0.4\\end{bmatrix} \\times 0.7 = 0.0784$\n",
    "- $\\psi_{🙂}(1)  = 🙂$\n",
    "\n",
    "$\\delta$ and $\\psi$ at state 😔 and T = 1\n",
    "- $\\delta_{😔}(1) = \\max\\limits_i [\\delta_i(0)a_{ij}] b_j(o_t) =  \\max \\begin{bmatrix} 0.16 \\times 0.3 \\\\ 0.02 \\times 0.6\\end{bmatrix} \\times 0.1 = 0.0048$\n",
    "- $\\psi_{😔}(1) = \\arg \\max\\limits_i [\\delta_i(0)a_{ij}] = 🙂$\n",
    "\n",
    "<img src=\"images/HMM_example_trellis.png\" height=\"800\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Viterbi: Induction (T = 2)\n",
    "\n",
    "- $\\delta$ and $\\psi$ at state 🙂 and T = 2\n",
    "    - $\\delta_{🙂}(2) = \\max\\limits_i [\\delta_i(1)a_{ij}] b_j(o_t) =  \\max \\begin{bmatrix} 0.0.784 \\times 0.7 \\\\ 0.0048 \\times 0.4 \\end{bmatrix}\\times 0 = 0\n",
    "$\n",
    "    - $\\psi_{🙂}(2) = \\arg \\max\\limits_i [\\delta_i(1)a_{ij}] = 🙂$\n",
    "\n",
    "- $\\delta$ and $\\psi$ at state 😔 and T = 2\n",
    "    - $\\delta_{😔}(2) = \\max\\limits_i [\\delta_i(1)a_{ij}] b_j(o_t) =  \\max \\begin{bmatrix} 0.0.784 \\times 0.3 \\\\ 0.0048 \\times 0.6 \\end{bmatrix}\\times 0.2 = 4.704 \\times 10^{-3}$\n",
    "    - $\\psi_{😔}(2) = \\arg \\max\\limits_i [\\delta_i(1)a_{ij}] = 🙂$\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/HMM_example_trellis.png\" height=\"400\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Viterbi: Induction (T = 3)\n",
    "\n",
    "- $\\delta$ and $\\psi$ at state 🙂 and T = 3\n",
    "    - $\\delta_{🙂}(3) = \\max\\limits_i [\\delta_i(2)a_{ij}] b_j(o_t) = \\max \\begin{bmatrix} 0 \\times 0.7 \\\\ 4.704 \\times 10^{-3} \\times 0.4 \\end{bmatrix} \\times 0.1 = 1.88\\times10^{-4}\n",
    "$\n",
    "    - $\\psi_{🙂}(3) = \\arg \\max\\limits_i [\\delta_i(2)a_{ij}] = 😔$\n",
    "\n",
    "- $\\delta$ and $\\psi$ at state 😔 and T = 3\n",
    "    - $\\delta_{😔}(3) = \\max\\limits_i [\\delta_i(2)a_{ij}] b_j(o_t) = \\max \\begin{bmatrix} 0 \\times 0.3 \\\\ 4.704 \\times 10^{-3} \\times 0.6 \\end{bmatrix} \\times 0.6 = 1.69 \\times 10^{-3}$\n",
    "    - $\\psi_{😔}(3) = \\arg \\max\\limits_i [\\delta_i(2)a_{ij}] = 😔$\n",
    "\n",
    "<br>\n",
    "<img src=\"images/HMM_example_trellis.png\" height=\"400\" width=\"400\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Viterbi conclusion\n",
    "\n",
    "- Choose the best final state: $q_t^* = \\arg \\max\\limits_i \\delta_i(t)$\n",
    "- Recursively choose the best previous state: $q_{t-1}^* = \\psi_{q_t^*}(t)$\n",
    "    - The most likely state sequence for the observation sequence ELFC is 🙂🙂😔😔.\n",
    "- The probability of the state sequence is the probability of $q_t^*$\n",
    "    - $P(🙂🙂😔😔) = 1.69 \\times 10^{-3}$    \n",
    "    \n",
    "<br>\n",
    "<img src=\"images/HMM_example_trellis.png\" height=\"800\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Viterbi final comments\n",
    "\n",
    "- This is how you find the best state sequence that explains the observation sequence using the Viterbi algorithm!   \n",
    "- Much faster than the brute force approach of considering all possible state combinations, calculating probabilities for each of them and taking the one resulting in maximum probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### HMM for ASR\n",
    "\n",
    "\n",
    "<img src=\"images/HMM_ASR.png\" height=\"1000\" width=\"1000\"> \n",
    "\n",
    "(Credit: [Stanford cs224s lectures](https://web.stanford.edu/class/cs224s/lectures/224s.17.lec3.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### HMM for Gene prediction\n",
    "\n",
    "- Annotated VSG genes and their predictions by an HMM with 3 states for the sequence 9.\n",
    "\n",
    "<center>\n",
    "<img src=\"files/images/HMM_gene_prediction.png\" height=\"600\" width=\"600\"> \n",
    "</center>\n",
    "\n",
    "(Credit: [Mesa et al. 2015](https://arxiv.org/pdf/1508.05367.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### (Optional) HMMs with [ `hmmlearn`](https://hmmlearn.readthedocs.io) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Initializing an HMM \n",
    "states = ['Happy', 'Sad']\n",
    "n_states = len(states)\n",
    "\n",
    "observations = ['Learn', 'Eat', 'Cry', 'Facebook']\n",
    "n_observations = len(observations)\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=n_states, init_params=\"\")\n",
    "model.startprob_ = np.array([0.8,0.2])\n",
    "model.transprob_ = np.array([\n",
    " [0.7, 0.3],\n",
    " [0.4, 0.6]\n",
    "])\n",
    "model.emissionprob_ = np.array([\n",
    "    [0.6, 0.3, 0.1, 0.0],\n",
    "    [0.1, 0.1, 0.6, 0.2]\n",
    "])\n",
    "\n",
    "observation_sequence = np.array([[0, 0, 2, 3, 2, 0, 1, 0, 1, 2, 2]]).T\n",
    "print(observation_sequence)\n",
    "\n",
    "# Fit the model\n",
    "model = model.fit(observation_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [2]]\n",
      "[0 0 0 1 1]\n",
      "loglikelihood of X:  -3.864361568851093\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [3]]\n",
      "[0 0 0 0 1 1 1 1 1]\n",
      "loglikelihood of X:  -9.733978147247116\n"
     ]
    }
   ],
   "source": [
    "# Likelihood computation\n",
    "X, Z = model.sample(5)\n",
    "print(X)\n",
    "print(Z)\n",
    "print('loglikelihood of X: ', model.score(X))\n",
    "X, Z = model.sample(9)\n",
    "print(X)\n",
    "print(Z)\n",
    "print('loglikelihood of X: ', model.score(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State sequence:  [0 0 1 1 1 0 0 0 0 1 1]\n",
      "Observations:  Learn, Learn, Cry, Facebook, Cry, Learn, Eat, Learn, Eat, Cry, Cry\n",
      "State sequence:  Happy, Happy, Sad, Sad, Sad, Happy, Happy, Happy, Happy, Sad, Sad\n",
      "Log probability of the state sequence:  -12.541693722559724\n"
     ]
    }
   ],
   "source": [
    "# Decoding\n",
    "logprob, state_seq = model.decode(observation_sequence, algorithm=\"viterbi\")\n",
    "print('State sequence: ', state_seq)\n",
    "print('Observations: ', \", \".join(map(lambda x: observations[x], observation_sequence.T[0])))\n",
    "print('State sequence: ', \", \".join(map(lambda x: states[x], state_seq)))\n",
    "print('Log probability of the state sequence: ', logprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic modeling\n",
    "\n",
    "Attribution: Material and presentation in the next slides is adapted from [Jordan Boyd-Graber's excellent material on LDA](http://users.umiacs.umd.edu/~jbg/teaching/CMSC_726/16a.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling motivation\n",
    "\n",
    "- Suppose you have a large collection of documents on a variety of topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: A corpus of news articles \n",
    "\n",
    "<img src=\"images/TM_NYT_articles.png\" height=\"2000\" width=\"2000\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: A corpus of food magazines \n",
    "\n",
    "<img src=\"images/TM_food_magazines.png\" height=\"2000\" width=\"2000\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A corpus of scientific articles\n",
    "\n",
    "<img src=\"images/TM_science_articles.png\" height=\"2000\" width=\"2000\"> \n",
    "\n",
    "(Credit: [Dave Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling motivation\n",
    "\n",
    "- Humans are pretty good at reading and understanding a document and answering questions such as \n",
    "    - What is it about?  \n",
    "    - Which documents is it related to?     \n",
    "- But for a large collection of documents it would take years to read all documents and organize and categorize them so that they are easy to search.\n",
    "- You need an automated way\n",
    "    - to get an idea of what's going on in the data or \n",
    "    - to pull documents related to a certain topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling\n",
    "\n",
    "- Topic modeling gives you an ability to summarize the major themes in a large collection of documents (corpus). \n",
    "    - Example: The major themes in a collection of news articles could be \n",
    "        - **politics**\n",
    "        - **entertainment**\n",
    "        - **sports**\n",
    "        - **technology**\n",
    "        - ...\n",
    "- A common tool to solve such problems is unsupervised ML methods.\n",
    "- Given the hyperparameter $K$, the idea of topic modeling is to describe the data using $K$ \"topics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: Input and output\n",
    "\n",
    "- Input\n",
    "    - A large collection of documents\n",
    "    - A value for the hyperparameter $K$ (e.g., $K = 3$)\n",
    "- Output\n",
    "    1. Topic-words association \n",
    "        - For each topic, what words describe that topic? \n",
    "    2. Document-topics association\n",
    "        - For each document, what topics are expressed by the document? \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: Example\n",
    "\n",
    "- Topic-words association \n",
    "    - For each topic, what words describe that topic?  \n",
    "    - A topic is a mixture of words. \n",
    "\n",
    "<img src=\"images/topic_modeling_word_topics.png\" height=\"1000\" width=\"1000\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: Example\n",
    "\n",
    "- Document-topics association \n",
    "    - For each document, what topics are expressed by the document?\n",
    "    - A document is a mixture of topics. \n",
    "    \n",
    "<img src=\"images/topic_modeling_doc_topics.png\" height=\"800\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: Input and output\n",
    "\n",
    "- Input\n",
    "    - A large collection of documents\n",
    "    - A value for the hyperparameter $K$ (e.g., $K = 3$)\n",
    "- Output\n",
    "    - For each topic, what words describe that topic?  \n",
    "    - For each document, what topics are expressed by the document?\n",
    "\n",
    "<img src=\"images/topic_modeling_output.png\" height=\"1000\" width=\"1000\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: Some applications\n",
    "\n",
    "- Topic modeling is a great EDA tool to get a sense of what's going on in a large corpus. \n",
    "- Some examples\n",
    "    - If you want to pull documents related to a particular lawsuit. \n",
    "    - You want to examine people's sentiment towards a particular candidate and/or political party and so you want to pull tweets or Facebook posts related to election.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we do topic modeling? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling as matrix factorization\n",
    "\n",
    "- You can think of topic modeling as a matrix factorization problem. \n",
    "- Given\n",
    "    - $K \\rightarrow $ Number of topics\n",
    "    - $M \\rightarrow $ Number of documents\n",
    "    - $V \\rightarrow $ Size of vocabulary\n",
    "\n",
    "<img src=\"images/topic_modeling_matrix_factorization.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "- Use SVD for factorization and it's referred to as Latent Semantic Indexing (LSA) in information retrieval. \n",
    "- Perfectly valid approach! \n",
    "\n",
    "[Source](http://users.umiacs.umd.edu/~jbg/teaching/CMSC_726/16b.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternative: Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "- A Bayesian, probabilistic, and generative approach  \n",
    "- Developed by [David Blei](http://www.cs.columbia.edu/~blei/) and colleagues in 2003. \n",
    "    * One of the most cited papers in the last 15 years.\n",
    "- DISCLAIMER    \n",
    "    - We won't go into the math because we do not have time to go in details. \n",
    "    - My goal is to give you an intuition of the model and show you how to use it to solve your problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA high-level idea\n",
    "\n",
    "- Dirichlet distribution is a distribution of distributions. \n",
    "- In our case,\n",
    "    - Every document is a discrete probability distribution of topics. \n",
    "    - Every topic is a discrete probability distribution of words.\n",
    "    - So we are have distributions of distributions.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA: insight\n",
    "- Each document is a mixture of corpus-wide topics\n",
    "- Every topic is a mixture words\n",
    "\n",
    "<img src=\"images/TM_dist_topics_words_blei.png\" height=\"1000\" width=\"1000\"> \n",
    "\n",
    "(Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generative story of LDA\n",
    "- The story that tells us how our data was generated. \n",
    "- The generative story of LDA to create Document 1 below:     \n",
    "    1. Pick a topic from the topic distribution for Document 1. \n",
    "    2. Pick a word from the selected topic's word distribution. \n",
    "\n",
    "<img src=\"images/topic_modeling_generative_story.png\" height=\"700\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical presentation of the generative story (plate diagram)\n",
    "\n",
    "- We are not going into the details but I would like you to be familiar with this picture at a high-level because it's likely that you might see it in the context of topic modeling. \n",
    "\n",
    "<img src=\"images/topic_modeling_plate_diagram.png\" height=\"500\" width=\"500\"> \n",
    "\n",
    "- For each topic $k \\in \\{1, \\dots, K\\}$ draw a multinomial distribution $\\beta_k$ from a Dirichlet distribution with parameter $\\lambda$. \n",
    "- For each document $d \\in \\{1, \\dots, M\\}$, draw a multinomial distribution $\\theta_d$\n",
    "from a Dirichlet distribution with parameter $\\alpha$\n",
    "- For each word position $n \\in \\{1, \\dots, N\\}$, select a hidden topic $Z_n$ from the multinomial distribution parameterized by $\\theta$.\n",
    "- Choose the observed word $w_n$ from the distribution $\\beta_{Z_n}$. \n",
    "\n",
    "[Source](http://users.umiacs.umd.edu/~jbg/teaching/CMSC_726/16a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA Inference\n",
    "\n",
    "- Infer the underlying topic structure in the documents. In particular, \n",
    "    - Learn the discrete probability distributions of topics in each document\n",
    "    - Learn the discrete probability distributions of words in each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA Inference\n",
    "\n",
    "- We are interested in the posterior distribution: $P(z, \\beta, \\theta| w_n, \\alpha, \\lambda)$\n",
    "- Observations: words. Everything else is hidden (latent). \n",
    "\n",
    "<img src=\"images/topic_modeling_plate_diagram.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "\n",
    "- $\\lambda$: Hyperparameter for word proportion\n",
    "    - High $\\lambda$ &rarr; every topic contains a mixture of most of the words\n",
    "    - Low $\\lambda$ &rarr; every topic contains a mixture of only few words\n",
    "    \n",
    "- $\\alpha$: Hyperparameter for topic proportion  \n",
    "   - High $\\alpha$ &rarr; every document contains a mixture of most of the topics\n",
    "   - Low $\\alpha$ &rarr; every document is representative of only a few topics    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we find the posterior distribution? \n",
    "\n",
    "- We are interested in the posterior distribution: $P(z, \\beta, \\theta| w_n, \\alpha, \\lambda)$\n",
    "- How do we find it? \n",
    "    - Variational inference\n",
    "    - **Gibbs sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm: Gibbs sampling \n",
    "\n",
    "- Sample topic assignments\n",
    "- Calculate conditional probability of single word topic assignment conditioned on the rest of the parameters. \n",
    "\n",
    "<img src=\"images/topic_modeling_topic_word_assignment.png\" height=\"700\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gibbs sampling equation: Calculating the conditional probability\n",
    "\n",
    "- Two components\n",
    "    - How much this document likes topic $k$: \n",
    "    $$\\frac{n_{d,k} + \\alpha_k}{\\sum^K_i n_{d,i} + \\alpha_i}$$\n",
    "    - How much this topic likes word $w_{d,n}$: $$\\frac{V_{k, w_{d,n}} + \\lambda_{w_{d,n}}}{\\sum_i V_{k,i} + \\lambda_i}$$ \n",
    "- The conditional probability of word topic assignment given everything else in the model: \n",
    "\n",
    "$$\\frac{n_{d,k} + \\alpha_k}{\\sum^K_i n_{d,i} + \\alpha_i} \\frac{V_{k, w_{d,n}} + \\lambda_{w_{d,n}}}{\\sum_i V_{k,i} + \\lambda_i}$$\n",
    "\n",
    "- $n_{d,k} \\rightarrow$ number of times document $d$ uses topic $k$ \n",
    "- $V_{k, w_{d,n}} \\rightarrow$ number of times topic $k$ uses word type $w_{d,n}$\n",
    "- $\\alpha_k \\rightarrow$ Dirichlet parameter for document to topic distribution\n",
    "- $\\lambda_{w_{d,n}} \\rightarrow$ Dirichlet parameter for topic to word distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### LDA algorithm \n",
    "\n",
    "- Suppose $K$ is number of topics\n",
    "- For each iteration $i$\n",
    "    - For each document $d$ and word $n$ currently assigned to topic $Z_{old}$\n",
    "        - Decrement $n_{d,Z_{old}}$ and $V_{Z_{old}, w_{d,n}}$\n",
    "        - Sample $Z_{new} = k$ with probability proportional to $\\frac{n_{d,k} + \\alpha_k}{\\sum^K_i n_{d,i} + \\alpha_i} \\frac{V_{k, w_{d,n}} + \\lambda_{w_{d,n}}}{\\sum_i V_{k,i} + \\lambda_i}$\n",
    "        - Increment $n_{d, Z_{new}} and V_{Z_{new}, w_{d,n}}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm example: Random topic assignment\n",
    "\n",
    "- Randomly assign each word in each document to one of the topics. \n",
    "    - The same word in the vocabulary may have different topic assignments in different instances.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm example: Sample document and random topic assignment\n",
    "- Consider this sample document (Document 10) with random topic assignment\n",
    "<img src=\"images/topic_modeling_word_topic_assignment.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "\n",
    "- With the current topic assignment, here are the topic counts in our document \n",
    "<img src=\"images/topic_modeling_doc_topic_counts.png\" height=\"700\" width=\"700\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm example: Total topic counts\n",
    "\n",
    "- For each word in our current document (Document 10), calculate how often that word occurs with each topic in all documents\n",
    "\n",
    "<img src=\"images/topic_modeling_word_topic_counts.png\" height=\"700\" width=\"700\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm example: Sample a word-topic assignment\n",
    "\n",
    "- Suppose our sampled word-topic assignment is the word _probabilistic_ in Document 10 with assigned topic 3. \n",
    "- How often does the Topic 3 occur in Document 10? Once. \n",
    "- How often does the word _probabilistic_ occur with Topic 3 in the corpus? Twice.  \n",
    "\n",
    "\n",
    "<img src=\"images/topic_modeling_word_topic_assignment.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "<img src=\"images/topic_modeling_doc_topic_counts.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "<img src=\"images/topic_modeling_word_topic_counts.png\" height=\"600\" width=\"600\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm example: Decrement counts\n",
    "\n",
    "- We want to update the word topic assignment of _probabilistic_ and Topic 3. \n",
    "- Decrement the count of the word from the word-topic counts.\n",
    "    \n",
    "<img src=\"images/topic_modeling_count_decrement.png\" height=\"1200\" width=\"1200\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm example: Calculating conditional probability distribution\n",
    "- How much does this document like each topic?\n",
    "    - The document likes Topics 1 and 2 equally (2 occurrences each)\n",
    "    - $\\frac{n_{d,k} + \\alpha_k}{\\sum^K_i n_{d,i} + \\alpha_i}$\n",
    "\n",
    "- How much does each topic like the word? \n",
    "    - Topic 1 likes the word _probabilistic_ compared to other topics (15 occurrences in topic 1 vs. 1 occurrence in topic 2 and 1 occurrence in topic 3)\n",
    "    - $\\frac{V_{k, w_{d,n}} + \\lambda_{w_{d,n}}}{\\sum_i V_{k,i} + \\lambda_i}$\n",
    "<img src=\"images/topic_modeling_decremented_counts.png\" height=\"400\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm example: Calculating conditional probability distribution \n",
    "\n",
    "- How much does Document 10 like each topic?\n",
    "- How much does each topic like word _probabilistic_ ? \n",
    "\n",
    "<img src=\"images/topic_modeling_decremented_counts.png\" height=\"500\" width=\"500\"> \n",
    "\n",
    "<img src=\"images/topic_modeling_conditional_proba.png\" height=\"800\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Updating topic assignment\n",
    "\n",
    "- So update the topic of the current word _probabilistic_ in document 10 to **topic 1**\n",
    "- Update the document-topic and word-topic counts accordingly. \n",
    "\n",
    "<img src=\"images/topic_modeling_update_count.png\" height=\"1200\" width=\"1200\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm: conclusion\n",
    "\n",
    "- In one pass, the algorithm repeats the above steps for each word in the corpus\n",
    "- If you do this for several passes, meaningful topics emerge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic modeling examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: Input \n",
    "\n",
    "<br><br>\n",
    "<img src=\"images/TM_science_articles.png\" height=\"2000\" width=\"2000\"> \n",
    "    \n",
    "Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: output\n",
    "\n",
    "<img src=\"images/TM_topics.png\" height=\"900\" width=\"900\"> \n",
    "\n",
    "\n",
    "(Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: output with interpretation\n",
    "- Assigning labels is a human thing. \n",
    "\n",
    "<img src=\"images/TM_topics_with_labels.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "(Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA topics in Yale Law Journal\n",
    "<img src=\"images/TM_yale_law_journal.png\" height=\"1500\" width=\"1500\"> \n",
    "\n",
    "(Credit: [David Blei's paper](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA topics in social media\n",
    "\n",
    "<img src=\"images/TM_health_topics_social_media.png\" height=\"1300\" width=\"1300\"> \n",
    "\n",
    "\n",
    "(Credit: [Health topics in social media](https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0103408.g002))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling with Python \n",
    "\n",
    "- Training LDA with [gensim](https://radimrehurek.com/gensim/models/ldamodel.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Topic modeling pipeline \n",
    "\n",
    "- Preprocess your corpus (CRUCIAL!!)\n",
    "- Train LDA using Gensim\n",
    "- Interpret your topics     \n",
    "- Evaluate\n",
    "    - How well your model does on unseen documents? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training LDA with [gensim](https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "\n",
    "To train an LDA model with [gensim](https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    ", you need\n",
    "\n",
    "- Document-term matrix \n",
    "- Dictionary (vocabulary)\n",
    "- The number of topics ($K$): `num_topics`\n",
    "- The number of passes: `passes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>fashion model pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>fashion model probabilistic topic model confer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>fresh fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>creative fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>fashion model probabilistic topic model confer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>probabilistic model pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>fashion model probabilistic topic model confer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>kiwi health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>fresh apple health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>creative health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>hidden markov model probabilistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>apple kiwi health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>fresh kiwi health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                               text\n",
       "0        1                               famous fashion model\n",
       "1        2                             fashion model pattern \n",
       "2        3  fashion model probabilistic topic model confer...\n",
       "3        4                               famous fashion model\n",
       "4        5                                fresh fashion model\n",
       "5        6                               famous fashion model\n",
       "6        7                               famous fashion model\n",
       "7        8                               famous fashion model\n",
       "8        9                               famous fashion model\n",
       "9       10                             creative fashion model\n",
       "10      11                               famous fashion model\n",
       "11      12                               famous fashion model\n",
       "12      13  fashion model probabilistic topic model confer...\n",
       "13      14                          probabilistic topic model\n",
       "14      15                        probabilistic model pattern\n",
       "15      16                          probabilistic topic model\n",
       "16      17                          probabilistic topic model\n",
       "17      18                          probabilistic topic model\n",
       "18      19                          probabilistic topic model\n",
       "19      20                          probabilistic topic model\n",
       "20      21                          probabilistic topic model\n",
       "21      22  fashion model probabilistic topic model confer...\n",
       "22      23                               apple kiwi nutrition\n",
       "23      24                              kiwi health nutrition\n",
       "24      25                                 fresh apple health\n",
       "25      26                          probabilistic topic model\n",
       "26      27                          creative health nutrition\n",
       "27      28                          probabilistic topic model\n",
       "28      29                          probabilistic topic model\n",
       "29      30                  hidden markov model probabilistic\n",
       "30      31                          probabilistic topic model\n",
       "31      32                          probabilistic topic model\n",
       "32      33                               apple kiwi nutrition\n",
       "33      34                                  apple kiwi health\n",
       "34      35                               apple kiwi nutrition\n",
       "35      36                                  fresh kiwi health\n",
       "36      37                               apple kiwi nutrition\n",
       "37      38                               apple kiwi nutrition\n",
       "38      39                               apple kiwi nutrition"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df = pd.read_csv('data/toy_lda_data.csv')\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['famous', 'fashion', 'model'],\n",
       " ['fashion', 'model', 'pattern'],\n",
       " ['fashion', 'model', 'probabilistic', 'topic', 'model', 'conference'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['fresh', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['creative', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['fashion', 'model', 'probabilistic', 'topic', 'model', 'conference'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'model', 'pattern'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['fashion', 'model', 'probabilistic', 'topic', 'model', 'conference'],\n",
       " ['apple', 'kiwi', 'nutrition'],\n",
       " ['kiwi', 'health', 'nutrition'],\n",
       " ['fresh', 'apple', 'health'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['creative', 'health', 'nutrition'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['hidden', 'markov', 'model', 'probabilistic'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['apple', 'kiwi', 'nutrition'],\n",
       " ['apple', 'kiwi', 'health'],\n",
       " ['apple', 'kiwi', 'nutrition'],\n",
       " ['fresh', 'kiwi', 'health'],\n",
       " ['apple', 'kiwi', 'nutrition'],\n",
       " ['apple', 'kiwi', 'nutrition'],\n",
       " ['apple', 'kiwi', 'nutrition']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [doc.split() for doc in toy_df['text'].tolist()]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'famous': 0, 'fashion': 1, 'model': 2, 'pattern': 3, 'conference': 4, 'probabilistic': 5, 'topic': 6, 'fresh': 7, 'creative': 8, 'apple': 9, 'kiwi': 10, 'nutrition': 11, 'health': 12, 'hidden': 13, 'markov': 14}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "# Create a vocabulary for the lda model \n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(1, 1), (2, 1), (3, 1)],\n",
       " [(1, 1), (2, 2), (4, 1), (5, 1), (6, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(1, 1), (2, 1), (7, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(1, 1), (2, 1), (8, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(1, 1), (2, 2), (4, 1), (5, 1), (6, 1)],\n",
       " [(2, 1), (5, 1), (6, 1)],\n",
       " [(2, 1), (3, 1), (5, 1)],\n",
       " [(2, 1), (5, 1), (6, 1)],\n",
       " [(2, 1), (5, 1), (6, 1)],\n",
       " [(2, 1), (5, 1), (6, 1)],\n",
       " [(2, 1), (5, 1), (6, 1)],\n",
       " [(2, 1), (5, 1), (6, 1)],\n",
       " [(2, 1), (5, 1), (6, 1)],\n",
       " [(1, 1), (2, 2), (4, 1), (5, 1), (6, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(10, 1), (11, 1), (12, 1)],\n",
       " [(7, 1), (9, 1), (12, 1)],\n",
       " [(2, 1), (5, 1), (6, 1)],\n",
       " [(8, 1), (11, 1), (12, 1)],\n",
       " [(2, 1), (5, 1), (6, 1)],\n",
       " [(2, 1), (5, 1), (6, 1)],\n",
       " [(2, 1), (5, 1), (13, 1), (14, 1)],\n",
       " [(2, 1), (5, 1), (6, 1)],\n",
       " [(2, 1), (5, 1), (6, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(9, 1), (10, 1), (12, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(7, 1), (10, 1), (12, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert our corpus into document-term matrix for Lda\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Train an lda model\n",
    "lda = models.LdaModel(corpus=doc_term_matrix, \n",
    "                      id2word=dictionary, \n",
    "                      num_topics=3, \n",
    "                      passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.228*\"kiwi\" + 0.203*\"nutrition\" + 0.203*\"apple\" + 0.130*\"health\"'),\n",
       " (1, '0.326*\"fashion\" + 0.306*\"model\" + 0.192*\"famous\" + 0.053*\"pattern\"'),\n",
       " (2,\n",
       "  '0.316*\"model\" + 0.296*\"probabilistic\" + 0.263*\"topic\" + 0.054*\"conference\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Examine the topics in our LDA model\n",
    "lda.print_topics(num_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document:  ['famous', 'fashion', 'model']\n",
      "Topic assignment for document:  [(0, 0.08340233), (1, 0.8284444), (2, 0.08815324)]\n"
     ]
    }
   ],
   "source": [
    "### Examine the topic distribution for a document\n",
    "print('Document: ', corpus[0])\n",
    "print('Topic assignment for document: ', lda[doc_term_matrix[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda, doc_term_matrix, dictionary, sort_topics=False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tips when you build an LDA model on a large corpus \n",
    "\n",
    "- **Preprocessing is crucial!**\n",
    "    - Tokenize, remove punctuation, convert text to lower case\n",
    "    - Discard words with length < threshold or word frequency < threshold        \n",
    "    - Stoplist: Remove most commonly used words in English \n",
    "    - Possibly lemmatization: Consider the lemmas instead of inflected forms. \n",
    "    - Depending upon your application, restrict to specific part of speech;\n",
    "        * For example, only consider nouns, verbs, and adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN STARTER CODE\n",
    "import spacy\n",
    "# Load English model for SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from urllib.request import urlopen\n",
    "alice_url = \"http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt\"\n",
    "alice_text = urlopen(alice_url).read().decode(\"utf-8\")\n",
    "alice_text = re.sub(r'\\s+',' ', alice_text)\n",
    "alice_text = alice_text[2000:2100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:  [ , but, it, was, too, dark, to, see, anything, ;, then, she, looked, at, the, sides, of, the, well, ,, and, noticed, that, the]\n",
      "\n",
      "Lemmas:  [' ', 'but', '-PRON-', 'be', 'too', 'dark', 'to', 'see', 'anything', ';', 'then', '-PRON-', 'look', 'at', 'the', 'side', 'of', 'the', 'well', ',', 'and', 'notice', 'that', 'the']\n",
      "\n",
      "POS:  ['SPACE', 'CCONJ', 'PRON', 'VERB', 'ADV', 'ADJ', 'PART', 'VERB', 'NOUN', 'PUNCT', 'ADV', 'PRON', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT', 'CCONJ', 'VERB', 'ADP', 'DET']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(alice_text)\n",
    "tokens = [token for token in doc]\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "pos = [token.pos_ for token in doc]\n",
    "print('\\nTokens: ', tokens)\n",
    "print('\\nLemmas: ', lemmas)\n",
    "print('\\nPOS: ', pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- Viterbi algorithm (HMM wrap-up)\n",
    "    - We saw the Viterbi algorithm which is a decoding algorithm for an HMM    \n",
    "- Topic modeling\n",
    "    - A tool to uncover themes in a large collection of documents\n",
    "    - We used LDA model for topic modeling, which is a Bayesian, probabilistic, and generative model. \n",
    "    - The primary idea of the model is \n",
    "        - A document is a mixture of topics \n",
    "        - A topic is a mixture of words in the vocabulary \n",
    "    - In the lab you will be training and interpreting your own LDA model using `gensim`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some useful resources and links \n",
    "- [Frank Rudzicz's slides on HMM](http://www.cs.toronto.edu/~frank/csc401/lectures2018/5-HMMs.pdf) \n",
    "- [Andrew McCallum's slides on HMM](https://people.cs.umass.edu/~mccallum/courses/inlp2004a/lect10-hmm2.pdf)\n",
    "- [Jordan Boyd-Graber's very approachable explanation of LDA](https://www.youtube.com/watch?v=fCmIceNqVog)\n",
    "- [lda2vec](https://github.com/cemoody/lda2vec)\n",
    "- [Original topic modeling paper: David Blei et al. 2003](http://www.cs.columbia.edu/~blei/papers/BleiNgJordan2003.pdf)\n",
    "- [Topic modeling for computational social scientists ](http://topicmodels.info/)\n",
    "- [spaCy's Python for data science cheat sheet](http://datacamp-community-prod.s3.amazonaws.com/29aa28bf-570a-4965-8f54-d6a541ae4e06)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
