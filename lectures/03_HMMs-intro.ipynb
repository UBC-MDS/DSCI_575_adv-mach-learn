{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/575_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 3: Introduction to Hidden Markov Models (HMMs)\n",
    "\n",
    "UBC Master of Data Science program, 2022-23\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture plan, imports, LO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Lecture plan \n",
    "\n",
    "- Motivation (~5 mins)\n",
    "- Definition and terminology of HMMs (~15 mins)\n",
    "- Q&A and activities (~5 mins) \n",
    "- Break (~5 mins)\n",
    "- The forward algorithm (~25 mins)\n",
    "- Supervised training of HMMs (~10 mins)\n",
    "- Q&A and activities (~5 mins) \n",
    "- Final comments and summary (~2 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import IPython\n",
    "from IPython.display import HTML, display\n",
    "from nltk.tag.hmm import HiddenMarkovModelTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learning outcomes\n",
    "\n",
    "From this lesson you will be able to\n",
    "\n",
    "- explain the motivation for using HMMs\n",
    "- define an HMM\n",
    "- state the Markov assumption in HMMs\n",
    "- explain three fundamental questions for an HMM\n",
    "- apply the forward algorithm given an HMM\n",
    "- explain supervised training in HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Personal virtual assistants\n",
    "\n",
    "- An important component of virtual assistants (e.g., Siri, Cortana, Google Home, Alexa) is speech recognition. \n",
    "- We ask such assistants questions. They convert the question into text, make sense of the question, and return the appropriate answer most of the times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"900\"\n",
       "            src=\"https://www.ibm.com/demos/live/speech-to-text/self-service/home\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1058d8f70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.ibm.com/demos/live/speech-to-text/self-service/home\"\n",
    "\n",
    "IPython.display.IFrame(url, width=800, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A number of speech recognition API's are available out there.\n",
    "- You can access them with Python. \n",
    "- A Python module called [`SpeechRecognition`](https://pypi.org/project/SpeechRecognition/) can let you access some of these APIs. \n",
    "    - CMU Sphinx (works offline)\n",
    "    - Google Speech Recognition\n",
    "    - Google Cloud Speech API\n",
    "    - Wit.ai\n",
    "    - Microsoft Bing Voice Recognition\n",
    "    - Houndify API\n",
    "    - IBM Speech to Text\n",
    "    - Snowboy Hotword Detection (works offline)\n",
    "- Usually, you have to pay some money if you want to use these APIs.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Speech recognition \n",
    "\n",
    "- You are given a sequence of sound waves and your job is to recognize the corresponding sequence of phonemes or words. \n",
    "- Phonemes: distinct units of sound. For example: \n",
    "    - tree $\\rightarrow$ T R IY\n",
    "    - cat $\\rightarrow$ K AE T\n",
    "    - stats $\\rightarrow$ S T AE T S\n",
    "    - eks $\\rightarrow$ E K S     \n",
    "- There are ~44 phonemes in North American English. \n",
    "- Is it possible to use the ML models we learned in 571, 573, 563 for this problem?\n",
    "- In written text, we know that certain transitions are more likely than others\n",
    "    - \"th\" as in \"this\"\n",
    "    - \"sh\" as in \"shoe\"\n",
    "    - \"ch\" as in \"chair\"\n",
    "    - \"ck\" as in \"back\"\n",
    "- Which transition do you think is easier and more natural/efficient/common for phonemes? \n",
    "    - /s/ to /t/: \"stop\", \"best\", \"fast\"\n",
    "    - /t/ and /r/: \"try\", \"tree\", \"train\"\n",
    "    - /f/ to /v/: \"of value\"\n",
    "    - /s/ to /b/\n",
    "    - In other words is it easier to say \"stop\" or \"of value\"?\n",
    "    \n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Speech recognition is a sequence modeling problem. \n",
    "    - It's a good idea to incorporate sequential information in the model for speech recognition. \n",
    "- Many modern statistical speech recognition systems are based on hidden Markov models. \n",
    "\n",
    "> Note that the most recent speech recognition models use deep learning but HMMs are still popular. They are particular useful when the training data is small and interpretation is important. Also, it's useful to understand HMMs before moving on to deep learning models for sequence processing (e.g., RNNs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are HMMs? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Observable Markov models \n",
    "\n",
    "- Example\n",
    "    - States: {uniformly, are, charming}   \n",
    "\n",
    "![](img/observable_Markov.png)\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/observable_Markov.png\" height=\"600\" width=\"600\"> -->\n",
    "<!-- </center> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/A.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden phenomenon \n",
    "\n",
    "Very often the things you observe in the real world can be thought of as a function of some other **hidden** variables.\n",
    "\n",
    "Example 1: \n",
    "- Observations: Acoustic features of the speech signal, hidden states: phonemes that are spoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example 2: \n",
    "- Observations: Words, hidden states: parts-of-speech\n",
    "\n",
    "![](img/hmm_pos_tagging.png)\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/hmm_pos_tagging.png\" height=\"1000\" width=\"1000\"> -->\n",
    "<!-- </center> -->\n",
    "\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/8.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More examples\n",
    "\n",
    "- Observations: Encrypted symbols, hidden states: messages\n",
    "- Observations: Exchange rates, hidden states: volatility of the market\n",
    "\n",
    "<!-- ![](img/stock_market_hmm.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HMM definition and example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Last week we used the following toy example to demonstrate how do we learn initial state probabilities and transition probabilities in Markov models. \n",
    "- Companies such Facebook or Google can track many of our activities. Suppose they want to predict our mood so that they can sent you certain ads to us. They cannot directly observe our mood but they can predict our mood depending upon our activities. So the mood is hidden here and activities are obervable.  \n",
    "\n",
    "![](img/activity-seqs.png)\n",
    "<!-- <img src=\"img/activity-seqs.png\" height=\"800\" width=\"800\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Markov process with hidden variables: Example\n",
    "\n",
    "- Let's simplify above example. \n",
    "- Suppose you have a little robot that is trying to estimate the posterior probability that you are **Happy (H or 🙂)** or **Sad (S or 😔)**, given that the robot has observed whether you are doing one of the following activities: \n",
    "    - **Learning data science (L or 📚)**\n",
    "    - **Eat (E or 🍎)** \n",
    "    - **Cry (C or 😿)** \n",
    "    - **Social media (F)**\n",
    "\n",
    "- The robot is trying to estimate the unknown (hidden) state $Q$, where $Q =H$ when you are happy (🙂) and $Q = S$ when you are sad (😔). \n",
    "- The robot is able to observe the activity you are doing: $O = {L, E, C, F}$ \n",
    "\n",
    "(Attribution: Example adapted from [here](https://www.cs.ubc.ca/~nando/340-2012/lectures/l6.pdf).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Example questions we are interested in answering are:\n",
    "    - What is the probability of observation sequence 📚📚😿📚📚?\n",
    "    - What is the best possible sequence of state of mind (e.g.,🙂,🙂,😔,🙂,🙂 ) given an observation sequence (e.g., L,L,C,L,L or 📚📚😿📚📚). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### HMM ingredients\n",
    "\n",
    "- State space (e.g., 🙂 (H), 😔 (S))\n",
    "- An initial probability distribution over the states\n",
    "- Transition probabilities\n",
    "- **Emission probabilities** \n",
    "    - Conditional probabilities for all observations given a hidden state\n",
    "    - Example: Below $P(L|🙂) = 0.7$ and $P(L|😔) = 0.1$\n",
    "    \n",
    "![](img/HMM_example.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example.png\" height=\"600\" width=\"600\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of an HMM\n",
    "\n",
    "- A hidden Markov model (HMM) is specified by the 5-tuple:  $\\{S, Y, \\pi, T, B\\}$ \n",
    "    - $S = \\{s_1, s_2, \\dots, s_n\\}$ is a set of states (e.g., moods)\n",
    "    - **$Y = \\{y_1, y_2, \\dots, y_k\\}$ is output alphabet (e.g., set of activities)**\n",
    "    - $\\pi = {\\pi_1, \\pi_2, \\dots, \\pi_n}$ is discrete initial state probability distribution \n",
    "    - Transition probability matrix $T$, where each $a_{ij}$ represents the probability of moving from state $s_i$ to state $s_j$\n",
    "    - **Emission probabilities B = $b_i(o), i \\in S, o \\in Y\\$**\n",
    "    \n",
    "![](img/HMM_example.png)    \n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example.png\" height=\"600\" width=\"600\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Yielding the state sequence and the observation sequences in an unrolled HMM \n",
    "    - State sequence: $Q = {q_0,q_1, q_2, \\dots q_T}, q_i \\in S$ \n",
    "    - Observation sequence: $O = {o_0,o_1, o_2, \\dots o_T}, o_i \\in Y$\n",
    "<!-- ![](img/HMM_unrolling_timesteps.png) -->\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example.png\" height=\"600\" width=\"600\"> -->\n",
    "<!-- </center> -->\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_unrolling_timesteps.png\" height=\"700\" width=\"700\"> -->\n",
    "\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is an example of an unrolled HMM for six time steps, a possible realization of a sequence of states and a sequence of observations. \n",
    "\n",
    "![](img/HMM_unrolling_timesteps.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_unrolling_timesteps.png\" height=\"800\" width=\"800\"> -->\n",
    "<!-- </center> -->\n",
    "\n",
    "- Each state produces only a single observation and the sequence of hidden states and the sequence of observations have the same length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HMM assumptions\n",
    "\n",
    "- **The probability of a particular state only depends on the previous state.**\n",
    "    * $P(q_i|q_0,q_1,\\dots,q_{i-1})$ = $P(q_i|q_{i-1})$\n",
    "    \n",
    "- **The probability of an output observation $o_i$ depends only on the state that produces the observation and not on any other state or any other observation.** \n",
    "    * $P(o_i|q_0,q_1,\\dots,q_{i-1}, o_0,o_1,\\dots,o_{i-1})$ = $P(o_i|q_i)$\n",
    "\n",
    "<!-- ![](img/HMM_unrolling_timesteps.png) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) Emission probabilities in our toy example give us the probabilities of being happy or sad given that you are performing one of the four activities: Learn, Eat, Cry, Facebook.  \n",
    "- (B) In hidden Markov models, the observation at time step $t$ is conditionally independent of previous observations and previous hidden states given the hidden state at time $t$. \n",
    "- (C) In hidden Markov models, given the hidden state at time $t-1$, the hidden state at time step $t$ is conditionally independent of the previous hidden states and observations. \n",
    "- (D) In hidden Markov models, each hidden state has a probability distribution over all observations. \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 3.1: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "- (A) False\n",
    "- (B) True\n",
    "- (C) True\n",
    "- (D) True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Discuss the following questions with your neighbour. \n",
    "1. What are the parameters $\\theta$ of a hidden Markov model?\n",
    "2. Below is a hidden Markov model that relates numbers of ice creams eaten by Jason to the weather. Identify observations, hidden states, transition probabilities, and emission probabilities in the model.\n",
    "\n",
    "![](img/ice-cream-hmm.png)\n",
    "\n",
    "<!-- <img src=\"img/ice-cream-hmm.png\" height=\"600\" width=\"600\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/A.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 3.2: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "1. initial state probabilities $\\pi_0$, transition probabilities $T$, emission probabilities $B$\n",
    "2. \n",
    "- Observations: 1, 2, 3\n",
    "- Hidden states: HOT, COLD \n",
    "- transition probabilities: \n",
    "\n",
    "|               | HOT  | COLD |\n",
    "| ------------- |:---------:| -----:|\n",
    "| HOT         | 0.6       | 0.4   |\n",
    "| COLD        | 0.5       | 0.5   |\n",
    "\n",
    "Emission probabilities: \n",
    "|               | 1  | 2 | 3 | \n",
    "| ------------- |:---------:| -----:| -----:|\n",
    "| HOT         | 0.2       | 0.4   | 0.4 |\n",
    "| COLD        | 0.5       | 0.4   | 0.1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Three fundamental questions for an HMM\n",
    "\n",
    "#### Likelihood\n",
    "Given a model with parameters $\\theta = <\\pi, T, B>$, how do we efficiently compute the likelihood of a particular observation sequence $O$?\n",
    "#### Decoding\n",
    "Given an observation sequence $O$ and a model $\\theta$ how do we choose a state sequence $Q={q_0, q_1, \\dots q_T}$ that best explains the observation sequence?\n",
    "#### Learning\n",
    "Training: Given a large observation sequence $O$ how do we choose the best parameters $\\theta$ that explain the data $O$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break (~5 mins)\n",
    "\n",
    "![](img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the context of HMMs, the likelihood of an observation sequence is the probability of observing that sequence given a particular set of model parameters $\\theta$. \n",
    "\n",
    "Given a model with parameters $\\theta = <\\pi, T, B>$, how do we efficiently compute the likelihood of a particular observation sequence $O$?\n",
    "\n",
    "- Example: What's the probability of the sequence below? \n",
    "\n",
    "![](img/HMM_example_activity_seq.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_activity_seq.png\" height=\"400\" width=\"400\"> -->\n",
    "<!-- </center> -->\n",
    "\n",
    "- Recall that in HMMs, the observations are dependent upon the hidden states in the same time step. \n",
    "<br><br>\n",
    "\n",
    "![](img/HMM_likelihood_known_hidden.png)\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_likelihood_known_hidden.png\" height=\"500\" width=\"500\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Probability of an observation sequence given the state sequence \n",
    "\n",
    "- Suppose we know both the sequence of hidden states (moods) and the sequence of activities emitted by them. \n",
    "- $P(O|Q) = \\prod\\limits_{i=1}^{T} P(o_i|q_i)$\n",
    "- $P(E L F C|🙂 🙂 😔 😔) = P(E|🙂) \\times P(L|🙂) \\times P(F|😔) \\times P(C|😔)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Joint probability of observations and a possible hidden sequence \n",
    "\n",
    "- Let's consider the joint probability of being in a particular state sequence $Q$ and generating a particular sequence $O$ of activities. \n",
    "\n",
    "<br>\n",
    "\n",
    "![](img/HMM_likelihood_unknown_hidden.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_likelihood_unknown_hidden.png\" height=\"600\" width=\"500\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- $P(O,Q) = P(O|Q)\\times P(Q) = \\prod\\limits_{i=1}^T P(o_i|q_i) \\times \\prod\\limits_{i=1}^T P(q_i|q_{i-1})$ \n",
    "\n",
    "For example, for our toy sequence: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(E L F C, 🙂 🙂 😔 😔) = & P(🙂|start)\\\\ \n",
    "                          & \\times P(🙂|🙂) \\times P(😔|🙂) \\times P(😔|😔)\\\\\n",
    "                          & \\times P(E|🙂) \\times P(L|🙂) \\times P(F|😔) \\times P(C|😔)\\\\\n",
    "                      = & 0.8 \\times 0.7 \\times 0.3 \\times 0.6 \\times 0.2 \\times 0.7 \\times 0.2 \\times 0.6 \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "<br>\n",
    "![](img/HMM_likelihood_unknown_hidden.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_likelihood_unknown_hidden.png\" height=\"500\" width=\"500\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Total probability of an observation sequence \n",
    "\n",
    "- But we do not know the hidden state sequence $Q$.\n",
    "- We need to look at all combinations of hidden states. \n",
    "- We need to compute the probability of activity sequence (ELFC) by summing over all possible state (mood) sequences.  \n",
    "- $P(O) = \\sum\\limits_Q P(O,Q) = \\sum\\limits_QP(O|Q)P(Q)$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(E L F C) = & P(E L F C,🙂🙂🙂🙂)\\\\ \n",
    "             & + P(E L F C,🙂🙂🙂😔)\\\\\n",
    "             & + P(E L F C,🙂🙂😔😔) + \\dots\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "- Computationally inefficient \n",
    "    - For HMMs with $n$ hidden states and an observation sequence of $T$ observations, there are $n^T$ possible hidden sequences!!\n",
    "    - In real-world problems both $n$ and $T$ are large numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to compute $P(O)$ cleverly? \n",
    "\n",
    "- To avoid this complexity we use **dynamic programming**; we remember the results rather than recomputing them. \n",
    "- We make a **trellis** which is an array of states vs. time.\n",
    "- Note the alternative paths in the trellis. We are covering all the 16 combinations of states. \n",
    "- We compute $\\alpha_i(t)$ at each $(i,t)$, which represents the probability of being in state $i$ at time $t$ after seeing all previous observations and emitting the current observation at time step $t$. \n",
    "\n",
    "![](img/HMM_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_trellis.png\" height=\"400\" width=\"400\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: intuition \n",
    "\n",
    "- To compute $\\alpha_j(t)$, we can compute $\\alpha_{i}(t-1)$ for all possible states $i$ and then use our knowledge of $a_{ij}$ and $b_j(o_t)$.\n",
    "- We compute the trellis left-to-right because of the convention of time.\n",
    "- Remember that $o_t$ is fixed and known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure\n",
    "\n",
    "Three steps of the forward procedure. \n",
    "\n",
    "- Initialization: Compute the $\\alpha$ values for nodes in the first column of the trellis $(t = 0)$.\n",
    "- Induction: Iteratively compute the $\\alpha$ values for nodes in the rest of the trellis $(1 \\leq t < T)$.\n",
    "- Conclusion: Sum over the $\\alpha$ values for nodes in the last column of the trellis $(t = T)$.\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\"> -->\n",
    "<!-- </center> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Initialization $\\alpha_🙂(0)$ and $\\alpha_😔(0)$\n",
    "\n",
    "- Compute the nodes in the first column of the trellis $(T = 0)$.\n",
    "    * Probability of starting at state 🙂 and observing the activity E: $\\alpha_🙂(0) = \\pi_🙂 \\times b_🙂(E) = 0.8 \\times 0.2 = 0.16$ \n",
    "    * Probability of starting at state 😔 and observing the activity E: $\\alpha_😔(0) = \\pi_😔 \\times b_😔(E) = 0.2 \\times 0.1 = 0.02$  \n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\"> -->\n",
    "<!-- </center> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction\n",
    "\n",
    "- Iteratively compute the nodes in the rest of the trellis $(1 \\leq t < T)$.\n",
    "-  To compute $\\alpha_j(t+1)$ we can compute $\\alpha_{i}(t)$ for all possible states $i$ and then use our knowledge of $a_{ij}$ and $b_j(o_{t+1})$ \n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\"> -->\n",
    "<!-- </center> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction $\\alpha_🙂(1)$\n",
    "\n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "\n",
    "- Probability of being at state 🙂 at $t=1$ and observing the activity L\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\alpha_🙂(1) = & \\alpha_🙂(0)a_{🙂🙂}b_🙂(L) + \\alpha_😔(0)a_{😔🙂}b_🙂(L)\\\\\n",
    "             = & 0.16 \\times 0.7 \\times 0.7 + 0.02 \\times 0.4 \\times 0.7\\\\ \n",
    "             = & 0.084\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\"> -->\n",
    "<!-- </center> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction $\\alpha_😔(1)$\n",
    "\n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "- Probability of being at state 😔 at $t=1$ and observing the activity L:\n",
    "\\begin{equation}\n",
    "\\begin{split}             \n",
    "\\alpha_😔(1) = & \\alpha_🙂(0)a_{🙂😔}b_😔(L) + \\alpha_😔(0)a_{😔😔}b_😔(L)\\\\\n",
    "             = & 0.16 \\times 0.3 \\times 0.1 + 0.02 \\times 0.6 \\times 0.1\\\\\n",
    "             = & 0.006\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction $\\alpha_🙂(2)$\n",
    "\n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "\n",
    "- Probability of being at state 🙂 at $t=2$ and observing the activity F\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\alpha_🙂(2) = & \\alpha_🙂(1)a_{🙂🙂}b_🙂(F) + \\alpha_😔(1)a_{😔🙂}b_🙂(F)\\\\\n",
    "             = & 0.084 \\times 0.7 \\times 0.0 + 0.006 \\times 0.4 \\times 0.0\\\\ \n",
    "             = & 0.0\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction $\\alpha_😔(2)$\n",
    "\n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "- Probability of being at state 😔 at $t=2$ and observing the activity F:\n",
    "\\begin{equation}\n",
    "\\begin{split}             \n",
    "\\alpha_😔(2) = & \\alpha_🙂(1)a_{🙂😔}b_😔(F) + \\alpha_😔(1)a_{😔😔}b_😔(F)\\\\\n",
    "             = & 0.084 \\times 0.3 \\times 0.2 + 0.006 \\times 0.6 \\times 0.2\\\\\n",
    "             = & 0.00576\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "<!-- ![](img/HMM_example_trellis.png) -->\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction $\\alpha_🙂(3)$ (Activity)\n",
    "\n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "\n",
    "- Probability of being at state 🙂 at $t=3$ and observing the activity C:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\alpha_🙂(3) = & \\alpha_🙂(2)a_{🙂🙂}b_🙂(C) + \\alpha_😔(2)a_{😔🙂}b_🙂(C)\\\\\n",
    "             = & 0 \\times 0.7 \\times 0.1 + 0.00576 \\times 0.4 \\times 0.1\\\\ \n",
    "             = & 2.3 \\times 10^{-4}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction $\\alpha_😔(3)$ (Activity)\n",
    "\n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "- Probability of being at state 😔 at $t=3$ and observing the activity C:\n",
    "\\begin{equation}\n",
    "\\begin{split}             \n",
    "\\alpha_😔(3) = & \\alpha_🙂(2)a_{🙂😔}b_😔(C) + \\alpha_😔(2)a_{😔😔}b_😔(C)\\\\\n",
    "             = & 0.0 \\times 0.3 \\times 0.6 + 0.00576 \\times 0.6 \\times 0.6\\\\\n",
    "             = & 2.07 \\times 10^{-3}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Conclusion\n",
    "\n",
    "- Sum over all possible final states:\n",
    "  * $P(O;\\theta) = \\sum\\limits_{i=1}^{n}\\alpha_i(T-1)$\n",
    "  * $P(E,L,F,C) = \\alpha_🙂(3) + \\alpha_😔(3) = 2.3 \\times 10^{-4} + 2.07 \\times 10^{-3}$ \n",
    "\n",
    "![](img/HMM_example_trellis.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example_trellis.png\" height=\"700\" width=\"700\"> -->\n",
    "<!-- </center> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The forward procedure using dynamic programming needs only $\\approx 2n^2T$ multiplications compared to the $\\approx(2T)n^T$ multiplications with the naive approach!! \n",
    "- Why? Discuss with your neighbour.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised training of HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### (Optional) Generation with an HMM\n",
    "\n",
    "- An HMM is a generative model and we can generate new sequences using an HMM\n",
    "- $t = 0$\n",
    "- Start in state $q_0$ = $s_i$ with probability $\\pi_i$\n",
    "- Emit observation symbol $o_0 = y_k$ with probability $b_i(o_0)$\n",
    "- While (not forever): \n",
    "    * Go from state $q_t = s_i$ to state $q_{t+1} = s_j$ with probability $a_{ij}$\n",
    "    * Emit observation symbol $o_{t+1} = y_k$ with probability $b_j(o_{t+1})$\n",
    "    * $t = t + 1$  \n",
    "    \n",
    "![](img/HMM_example.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example.png\" height=\"500\" width=\"500\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised training of HMMs\n",
    "\n",
    "- Suppose we have training data where we have $O$ and corresponding $Q$, then we can use MLE to learn parameters $\\theta = <\\pi, T, B>$\n",
    "- Get transition matrix and the emission probabilities. \n",
    "    - Suppose $i$, $j$ are unique states from the state space and $k$ is a unique observation.    \n",
    "    - $\\pi_0(i) = P(q_0 = i) = \\frac{Count(q_0 = i)}{\\#sequences}$\n",
    "    - $a_{ij} = P(q_{t+1} = j|q_t = i) = \\frac{Count(i,j)}{Count(i, anything)}$\n",
    "    - $b_i(k) = P(o_{t} = k|q_t = i) = \\frac{Count(i,k)}{Count(i, anything)}$\n",
    "\n",
    "![](img/HMM_unrolling_timesteps.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_unrolling_timesteps.png\" height=\"700\" width=\"700\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "- Suppose we have training data where we have $O$ and corresponding $Q$, then we can use MLE to learn parameters $\\theta = <\\pi, T, B>$\n",
    "    - Count how often $q_{i-1}$ and $q_i$ occur together normalized by how often $q_{i-1}$ occurs with anything: \n",
    "      $p(q_i|q_{i-1}) = \\frac{Count(q_{i-1} q_i)}{Count(q_{i-1} \\text{anything})}$\n",
    "    - Count how often $q_i$ is associated with the observation $o_i$.   \n",
    "      $p(o_i|q_{i}) = \\frac{Count(o_i \\wedge q_i)}{Count(q_{i} \\text{anything})}$    \n",
    "\n",
    "<!-- ![](img/HMM_unrolling_timesteps.png) -->\n",
    "\n",
    "<center>\n",
    "<img src=\"img/HMM_unrolling_timesteps.png\" height=\"700\" width=\"700\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**In real life, all the calculations above are done with log probabilities for numerical stability.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HMM supervised training demo\n",
    "\n",
    "Part-of-speech tagging task\n",
    "\n",
    "- Given a text assign part-of-speech tags to the words in the text.\n",
    "\n",
    "- Input sentence: \n",
    "<blockquote>\n",
    "    MDS students are hard-working .\n",
    "</blockquote>    \n",
    "\n",
    "- POS-tagged sentence: \n",
    "<blockquote>\n",
    "    MDS/<span style=\"color:green\">PROPER_NOUN</span> students/<span style=\"color:green\">NOUN</span> are/<span style=\"color:green\">VERB</span> hard-working/<span style=\"color:green\">ADJECTIVE</span> ./<span style=\"color:green\">PUNCTUATION</span>\n",
    "</blockquote>    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "words = [\"book\", \"that\", \"flight\", \"like\", \"I\", \".\"]\n",
    "POS = [\"Noun\", \"Verb\", \"Punct\", \"Pron\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    [(\"book\", \"Verb\"), (\"that\", \"Pron\"), (\"flight\", \"Noun\"), (\".\", \"Punct\")],\n",
    "    [\n",
    "        (\"I\", \"Pron\"),\n",
    "        (\"like\", \"Verb\"),\n",
    "        (\"that\", \"Pron\"),\n",
    "        (\"book\", \"Noun\"),\n",
    "        (\".\", \"Punct\"),\n",
    "    ],\n",
    "    [(\"book\", \"Verb\"), (\"flight\", \"Noun\"), (\".\", \"Punct\")],\n",
    "    [(\"book\", \"Verb\"), (\"like\", \"Noun\"), (\"flight\", \"Noun\")],\n",
    "    [(\"I\", \"Pron\"), (\"book\", \"Verb\"), (\"flight\", \"Noun\"), (\".\", \"Punct\")],\n",
    "    [\n",
    "        (\"I\", \"Pron\"),\n",
    "        (\"like\", \"Verb\"),\n",
    "        (\"that\", \"Pron\"),\n",
    "        (\"book\", \"Noun\"),\n",
    "        (\".\", \"Punct\"),\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax is a bit weird. This is just for demonstration purpose. You're unlikely to use this when you carry out POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "trainer = HiddenMarkovModelTrainer(POS, words)\n",
    "hmm = trainer.train_supervised(\n",
    "    corpus,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm._create_cache()\n",
    "P, O, X, S = hmm._cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### From the documentation: \n",
    "\n",
    "The cache is a tuple (P, O, X, S) where:\n",
    "\n",
    "- S maps symbols to integers.  I.e., it is the inverse\n",
    "mapping from self._symbols; for each symbol s in\n",
    "self._symbols, the following is true::\n",
    "\n",
    "  ```self._symbols[S[s]] == s```\n",
    "\n",
    "- O is the log output probabilities::\n",
    "\n",
    "  ```O[i,k] = log( P(token[t]=sym[k]|tag[t]=state[i]) )```\n",
    "\n",
    "- X is the log transition probabilities::\n",
    "\n",
    "  ```X[i,j] = log( P(tag[t]=state[j]|tag[t-1]=state[i]) )```\n",
    "\n",
    "- P is the log prior probabilities::\n",
    "\n",
    "  ```P[i] = log( P(tag[0]=state[i]) )```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Mapping between the observations (symbols) to integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book': 0, 'that': 1, 'flight': 2, 'like': 3, 'I': 4, '.': 5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### HMM states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Noun', 'Verb', 'Punct', 'Pron']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm._states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Log prior probabilities \n",
    "- $\\pi_0$ for all states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pi_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Noun</th>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Verb</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Punct</th>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pron</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pi_0\n",
       "Noun   -inf\n",
       "Verb   -1.0\n",
       "Punct  -inf\n",
       "Pron   -1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(P, index=hmm._states, columns=[\"pi_0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Log output probabilities\n",
    "\n",
    "- log(P(observation | tag)) for all observations and tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>that</th>\n",
       "      <th>flight</th>\n",
       "      <th>like</th>\n",
       "      <th>I</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Noun</th>\n",
       "      <td>-1.807355</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.807355</td>\n",
       "      <td>-2.807355</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Verb</th>\n",
       "      <td>-0.584962</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-1.584962</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Punct</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pron</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           book  that    flight      like    I    .\n",
       "Noun  -1.807355  -inf -0.807355 -2.807355 -inf -inf\n",
       "Verb  -0.584962  -inf      -inf -1.584962 -inf -inf\n",
       "Punct      -inf  -inf      -inf      -inf -inf  0.0\n",
       "Pron       -inf  -1.0      -inf      -inf -1.0 -inf"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(O, index=hmm._states, columns=S.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Log transition probabilities \n",
    "\n",
    "- Transition matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Noun</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Punct</th>\n",
       "      <th>Pron</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Noun</th>\n",
       "      <td>-2.584963</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.263034</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Verb</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Punct</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pron</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Noun  Verb     Punct  Pron\n",
       "Noun  -2.584963  -inf -0.263034  -inf\n",
       "Verb  -1.000000  -inf      -inf  -1.0\n",
       "Punct      -inf  -inf      -inf  -inf\n",
       "Pron  -1.000000  -1.0      -inf  -inf"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X, index=hmm._states, columns=hmm._states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tagging a sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('book', 'Verb'), ('flight', 'Noun'), ('.', 'Punct')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.tag([\"book\", \"flight\", \".\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see in the next lecture the algorithm used for such tagging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's try it out on a bigger dataset\n",
    "\n",
    "- You don't have to understand the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"code/.\")\n",
    "from hmm_pos_demo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HMM POS tagging demo\n",
      "\n",
      "Training HMM...\n",
      "Testing...\n",
      "Test: the/AT fulton/NP county/NN grand/JJ jury/NN said/VBD friday/NR an/AT investigation/NN of/IN atlanta's/NP$ recent/JJ primary/NN election/NN produced/VBD ``/`` no/AT evidence/NN ''/'' that/CS any/DTI irregularities/NNS took/VBD place/NN ./.\n",
      "\n",
      "Untagged: the fulton county grand jury said friday an investigation of atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "\n",
      "HMM-tagged: the/AT fulton/NP county/NN grand/JJ jury/NN said/VBD friday/NR an/AT investigation/NN of/IN atlanta's/NP$ recent/JJ primary/NN election/NN produced/VBD ``/`` no/AT evidence/NN ''/'' that/CS any/DTI irregularities/NNS took/VBD place/NN ./.\n",
      "\n",
      "Entropy: 18.733173970451787\n",
      "\n",
      "------------------------------------------------------------\n",
      "Test: the/AT jury/NN further/RBR said/VBD in/IN term-end/NN presentments/NNS that/CS the/AT city/NN executive/JJ committee/NN ,/, which/WDT had/HVD over-all/JJ charge/NN of/IN the/AT election/NN ,/, ``/`` deserves/VBZ the/AT praise/NN and/CC thanks/NNS of/IN the/AT city/NN of/IN atlanta/NP ''/'' for/IN the/AT manner/NN in/IN which/WDT the/AT election/NN was/BEDZ conducted/VBN ./.\n",
      "\n",
      "Untagged: the jury further said in term-end presentments that the city executive committee , which had over-all charge of the election , `` deserves the praise and thanks of the city of atlanta '' for the manner in which the election was conducted .\n",
      "\n",
      "HMM-tagged: the/AT jury/NN further/RBR said/VBD in/IN term-end/AT presentments/NN that/CS the/AT city/NN executive/NN committee/NN ,/, which/WDT had/HVD over-all/VBN charge/NN of/IN the/AT election/NN ,/, ``/`` deserves/VBZ the/AT praise/NN and/CC thanks/NNS of/IN the/AT city/NN of/IN atlanta/NP ''/'' for/IN the/AT manner/NN in/IN which/WDT the/AT election/NN was/BEDZ conducted/VBN ./.\n",
      "\n",
      "Entropy: 27.07087255187996\n",
      "\n",
      "------------------------------------------------------------\n",
      "Test: the/AT september-october/NP term/NN jury/NN had/HVD been/BEN charged/VBN by/IN fulton/NP superior/JJ court/NN judge/NN durwood/NP pye/NP to/TO investigate/VB reports/NNS of/IN possible/JJ ``/`` irregularities/NNS ''/'' in/IN the/AT hard-fought/JJ primary/NN which/WDT was/BEDZ won/VBN by/IN mayor-nominate/NN ivan/NP allen/NP jr./NP ./.\n",
      "\n",
      "Untagged: the september-october term jury had been charged by fulton superior court judge durwood pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by mayor-nominate ivan allen jr. .\n",
      "\n",
      "HMM-tagged: the/AT september-october/JJ term/NN jury/NN had/HVD been/BEN charged/VBN by/IN fulton/NP superior/JJ court/NN judge/NN durwood/TO pye/VB to/TO investigate/VB reports/NNS of/IN possible/JJ ``/`` irregularities/NNS ''/'' in/IN the/AT hard-fought/JJ primary/NN which/WDT was/BEDZ won/VBN by/IN mayor-nominate/NP ivan/NP allen/NP jr./NP ./.\n",
      "\n",
      "Entropy: 33.82818742372735\n",
      "\n",
      "------------------------------------------------------------\n",
      "Test: ``/`` only/RB a/AT relative/JJ handful/NN of/IN such/JJ reports/NNS was/BEDZ received/VBN ''/'' ,/, the/AT jury/NN said/VBD ,/, ``/`` considering/IN the/AT widespread/JJ interest/NN in/IN the/AT election/NN ,/, the/AT number/NN of/IN voters/NNS and/CC the/AT size/NN of/IN this/DT city/NN ''/'' ./.\n",
      "\n",
      "Untagged: `` only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
      "\n",
      "HMM-tagged: ``/`` only/RB a/AT relative/JJ handful/NN of/IN such/JJ reports/NNS was/BEDZ received/VBN ''/'' ,/, the/AT jury/NN said/VBD ,/, ``/`` considering/IN the/AT widespread/JJ interest/NN in/IN the/AT election/NN ,/, the/AT number/NN of/IN voters/NNS and/CC the/AT size/NN of/IN this/DT city/NN ''/'' ./.\n",
      "\n",
      "Entropy: 11.437819859630912\n",
      "\n",
      "------------------------------------------------------------\n",
      "Test: the/AT jury/NN said/VBD it/PPS did/DOD find/VB that/CS many/AP of/IN georgia's/NP$ registration/NN and/CC election/NN laws/NNS ``/`` are/BER outmoded/JJ or/CC inadequate/JJ and/CC often/RB ambiguous/JJ ''/'' ./.\n",
      "\n",
      "Untagged: the jury said it did find that many of georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n",
      "\n",
      "HMM-tagged: the/AT jury/NN said/VBD it/PPS did/DOD find/VB that/CS many/AP of/IN georgia's/NP$ registration/NN and/CC election/NN laws/NNS ``/`` are/BER outmoded/VBG or/CC inadequate/JJ and/CC often/RB ambiguous/VB ''/'' ./.\n",
      "\n",
      "Entropy: 20.816362319185743\n",
      "\n",
      "------------------------------------------------------------\n",
      "Test: it/PPS recommended/VBD that/CS fulton/NP legislators/NNS act/VB ``/`` to/TO have/HV these/DTS laws/NNS studied/VBN and/CC revised/VBN to/IN the/AT end/NN of/IN modernizing/VBG and/CC improving/VBG them/PPO ''/'' ./.\n",
      "\n",
      "Untagged: it recommended that fulton legislators act `` to have these laws studied and revised to the end of modernizing and improving them '' .\n",
      "\n",
      "HMM-tagged: it/PPS recommended/VBD that/CS fulton/NP legislators/NNS act/VB ``/`` to/TO have/HV these/DTS laws/NNS studied/VBD and/CC revised/VBD to/IN the/AT end/NN of/IN modernizing/NP and/CC improving/VBG them/PPO ''/'' ./.\n",
      "\n",
      "Entropy: 20.324492120324763\n",
      "\n",
      "------------------------------------------------------------\n",
      "Test: the/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN other/AP topics/NNS ,/, among/IN them/PPO the/AT atlanta/NP and/CC fulton/NP county/NN purchasing/VBG departments/NNS which/WDT it/PPS said/VBD ``/`` are/BER well/QL operated/VBN and/CC follow/VB generally/RB accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT interest/NN of/IN both/ABX governments/NNS ''/'' ./.\n",
      "\n",
      "Untagged: the grand jury commented on a number of other topics , among them the atlanta and fulton county purchasing departments which it said `` are well operated and follow generally accepted practices which inure to the best interest of both governments '' .\n",
      "\n",
      "HMM-tagged: the/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN other/AP topics/NNS ,/, among/IN them/PPO the/AT atlanta/NP and/CC fulton/NP county/NN purchasing/NN departments/NNS which/WDT it/PPS said/VBD ``/`` are/BER well/RB operated/VBN and/CC follow/VB generally/RB accepted/VBN practices/NNS which/WDT inure/VBZ to/IN the/AT best/JJT interest/NN of/IN both/ABX governments/NNS ''/'' ./.\n",
      "\n",
      "Entropy: 31.3834231468673\n",
      "\n",
      "------------------------------------------------------------\n",
      "Test: merger/NN proposed/VBN\n",
      "\n",
      "Untagged: merger proposed\n",
      "\n",
      "HMM-tagged: merger/PPS proposed/VBD\n",
      "\n",
      "Entropy: 5.671820394600895\n",
      "\n",
      "------------------------------------------------------------\n",
      "Test: however/WRB ,/, the/AT jury/NN said/VBD it/PPS believes/VBZ ``/`` these/DTS two/CD offices/NNS should/MD be/BE combined/VBN to/TO achieve/VB greater/JJR efficiency/NN and/CC reduce/VB the/AT cost/NN of/IN administration/NN ''/'' ./.\n",
      "\n",
      "Untagged: however , the jury said it believes `` these two offices should be combined to achieve greater efficiency and reduce the cost of administration '' .\n",
      "\n",
      "HMM-tagged: however/WRB ,/, the/AT jury/NN said/VBD it/PPS believes/VBZ ``/`` these/DTS two/CD offices/NNS should/MD be/BE combined/VBN to/TO achieve/VB greater/JJR efficiency/NN and/CC reduce/VB the/AT cost/NN of/IN administration/NN ''/'' ./.\n",
      "\n",
      "Entropy: 8.275459439082953\n",
      "\n",
      "------------------------------------------------------------\n",
      "Test: the/AT city/NN purchasing/VBG department/NN ,/, the/AT jury/NN said/VBD ,/, ``/`` is/BEZ lacking/VBG in/IN experienced/VBN clerical/JJ personnel/NNS as/CS a/AT result/NN of/IN city/NN personnel/NNS policies/NNS ''/'' ./.\n",
      "\n",
      "Untagged: the city purchasing department , the jury said , `` is lacking in experienced clerical personnel as a result of city personnel policies '' .\n",
      "\n",
      "HMM-tagged: the/AT city/NN purchasing/NN department/NN ,/, the/AT jury/NN said/VBD ,/, ``/`` is/BEZ lacking/VBG in/IN experienced/AT clerical/JJ personnel/NNS as/CS a/AT result/NN of/IN city/NN personnel/NNS policies/NNS ''/'' ./.\n",
      "\n",
      "Entropy: 16.762253727845437\n",
      "\n",
      "------------------------------------------------------------\n",
      "accuracy over 284 tokens: 92.96\n"
     ]
    }
   ],
   "source": [
    "hmm = demo_pos_supervised()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the output\n",
    "\n",
    "- What do these tags (e.g., NN, AT, IN, NNS etc) mean? Where do they come from?\n",
    "    - These tags come from [the Penn Treebank Project](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "    - The Penn Treebank tagset consists of 36 POS tags to label different parts of speech of words in English. \n",
    "- Entropy is a common metric used to measure the degree of uncertainty or ambiguity in the tagging process. \n",
    "    - Lower entropy $\\rightarrow$ the tagger is relatively certain about the tags\n",
    "    - High entropy $\\rightarrow$ the tagger is less certain about the tags    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's try it out on a new unseen sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('keep', 'VB'),\n",
       " ('the', 'AT'),\n",
       " ('book', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('table', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.tag([\"keep\", \"the\", \"book\", \"on\", \"the\", \"table\", \".\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other libraries \n",
    "\n",
    "Some other libraries \n",
    "- [hmmlearn](https://hmmlearn.readthedocs.io/en/latest/)\n",
    "- [pomegranate](https://github.com/jmschrei/pomegranate)\n",
    "> Note that there are not many actively maintained off-the-shelf libraries available for supervised training of HMMs. [seqlearn](https://pypi.org/project/seqlearn/) used to be part of `sklearn`. But it's separated now and is not being maintained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why not use traditional ML models? \n",
    "\n",
    "- We could extract features and treat it as a multi-class classification problem of predicting POS for each word. Some example features could be: \n",
    "    - Whether the word ends with an \"ing\" (for verbs)\n",
    "    - What's the previous word?         \n",
    "    - Or whether the word occurs at the beginning or end of a sentence  \n",
    "- But coming up with such features is time consuming and limited. It can get unwieldy quite quickly and it leads to fragile and overfit models.     \n",
    "- HMM provide a much more elegant way to model sequences and usually they are a preferred way to model sequences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓❓ Questions for you\n",
    "\n",
    "### Exercise 3.3: Discuss the following question with your neighbour. \n",
    "\n",
    "- Give an advantage of using the forward procedure compared to summing over all possible state combinations of length T. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 3.3: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "The forward procedure is a computationally efficient procedure compared to the method of summing over all possible state combinations of length $T$. The former requires $2Tn^T$ multiplications compared to $2n^2T$ multiplications in the latter, where $N$ is the number of states and $T$ is the number of time steps.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- Hidden Markov models (HMMs) model time-series with latent factors.\n",
    "- There are tons of applications associated with them and they are more realistic than Markov models. \n",
    "- The most successful application of HMMs is speech recognition. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Important ideas we learned \n",
    "\n",
    "- HMM ingredients\n",
    "    - Hidden states (e.g., Happy, Sad)\n",
    "    - Output alphabet or output symbols (e.g., learn, study, cry, facebook)\n",
    "    - Discrete initial state probability distribution\n",
    "    - Transition probabilities\n",
    "    - Emission probabilities    \n",
    "\n",
    "![](img/HMM_example.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/HMM_example.png\" height=\"600\" width=\"600\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fundamental questions for HMMs \n",
    "\n",
    "- Three fundamental questions for HMMs: \n",
    "    - likelihood\n",
    "    - decoding\n",
    "    - parameter learning \n",
    "- The forward algorithm is a dynamic programming algorithm to efficiently calculate the probability of an observation sequence given an HMM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised training of HMMs \n",
    "- HMMs for POS tagging.\n",
    "- Not many tools out there for supervised training of HMMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coming up\n",
    "\n",
    "- Decoding: Viterbi algorithm\n",
    "    - Given an HMM model and an observation sequence, how do we efficiently compute the corresponding hidden state sequence. \n",
    "- Unsupervised training of HMMs (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resources\n",
    "\n",
    "- [Hidden Markov Models chapter from Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/A.pdf)\n",
    "- Attribution: Many presentation ideas in this notebook are taken from [Frank Rudzicz's slides](http://www.cs.toronto.edu/~frank/csc401/lectures2018/5-HMMs.pdf).\n",
    "- [Jason Eisner's lecture on hidden Markov Models](https://vimeo.com/31374528)\n",
    "- [Jason Eisner's interactive spreadsheet for HMMs](https://cs.jhu.edu/~jason/papers/eisner.hmm.xls)\n",
    "- [Who each player is guarding?](https://www.youtube.com/watch?v=JvNkZdZJBt4)\n",
    "- [The Viterbi Algorithm: A Personal History](https://arxiv.org/pdf/cs/0504020v2.pdf)\n",
    "- [A nice demo of independent vs. Markov vs. HMMs for DNA](https://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/src/chapter10.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
