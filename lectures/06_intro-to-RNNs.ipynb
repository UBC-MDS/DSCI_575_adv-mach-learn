{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/575_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 6: Introduction to Recurrent Neural Networks (RNNs)\n",
    "\n",
    "UBC Master of Data Science program, 2021-22\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture plan, imports, LO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Lecture plan \n",
    "\n",
    "- Left-over iClicker questions from lecture 5\n",
    "- RNNs motivation\n",
    "- RNN inference\n",
    "- Break\n",
    "- iClicker questions\n",
    "- RNN training \n",
    "- RNN architectures\n",
    "- Final comments and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learning outcomes\n",
    "\n",
    "From this lecture you will be able to \n",
    "\n",
    "- Explain the motivation to use RNNs. \n",
    "- Explain how an RNN differs from a feed-forward neural network. \n",
    "- Define vanilla or simple RNNs. \n",
    "- Explain three weight matrices in RNNs. \n",
    "- Explain parameter sharing in RNNs. \n",
    "- Explain how states and outputs are calculated in the forward pass of an RNN. \n",
    "- Explain the backward pass in RNNs at a high level. \n",
    "- Specify different architectures of RNNs and explain how these architectures are used in the context of NLP applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN-generated music! \n",
    "\n",
    "- [Magenta PerformanceRNN](https://www.youtube.com/watch?v=dMhQalLBXIU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/dMhQalLBXIU\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x110606b00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### An example of a state-of-the-art language model\n",
    "url = \"https://www.youtube.com/embed/dMhQalLBXIU\"\n",
    "IPython.display.IFrame(url, width=500, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Language is an inherently sequential phenomenon.\n",
    "- This temporal nature of language is reflected in the metaphors used to describe language \n",
    "    - *flow of conversation*, *news feeds*, or *twitter streams*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fixed-length input\n",
    "\n",
    "- ML algorithms we have seen in 571, 572, and 573 work with fixed length input.  \n",
    "    - SVM\n",
    "    - Logistic Regression\n",
    "    - Multi-layer Perceptron\n",
    "\n",
    "- Example of fixed length input\n",
    "$$X = \\begin{bmatrix}1 & 0.8 & \\ldots & 0.3\\\\ 0 & 0 &  \\ldots & 0.4\\\\ 1 & 0.2 &  \\ldots & 0.8 \\end{bmatrix}$$ \n",
    "\n",
    "$$y = \\begin{bmatrix}1 \\\\ 0 \\\\ 1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fixed-length input\n",
    "\n",
    "- When we used these models for sentiment analysis we created a **fixed size** input representation using `CountVectorizer`, where we had simultaneous access to all aspects of the input. \n",
    "\n",
    "$$X = \\begin{bmatrix}\\text{\"@united you're terrible. You don't understand safety\"}\\\\ \\text{\"@JetBlue safety first !! #lovejetblue\"}\\\\ \\text{\"@SouthwestAir truly the best in #customerservice!\"}\\\\ \\end{bmatrix} \\text{ and } y = \\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\end{bmatrix} $$ \n",
    "<br><br>\n",
    "$$X_{counts} = \\begin{bmatrix}1 & 3 & \\ldots & 2\\\\ 1 & 0 & \\ldots & 0\\\\ 0 & 2 & \\ldots & 1\\end{bmatrix} \\text{ and } y = \\begin{bmatrix}1 \\\\ 0 \\\\ 1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentiment analysis using feed-forward neural networks \n",
    "\n",
    "- Reminder: In feed-forward neural networks, \n",
    "    - all connections flow forward (no loops)\n",
    "    - each layer of hidden units is fully connected to the next\n",
    "- We pass fixed sized vector representation of text (e.g., representation created with `CountVectorizer`) as input. \n",
    "- We lose the temporal aspect of text in this representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mglearn\n",
    "\n",
    "# display(mglearn.plots.plot_single_hidden_layer_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How about using Markov models? \n",
    "\n",
    "- They have some temporal aspect. \n",
    "\n",
    "![](img/Markov_assumption.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/Markov_assumption.png\" height=\"550\" width=\"550\">  -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall language modeling task \n",
    "\n",
    "- Recall the task of predicting the next word given a sequence. \n",
    "- What's the probability of an upcoming word?  \n",
    "    - $P(w_t|w_1,w_2,\\dots,w_{t-1})$\n",
    "    \n",
    "<blockquote>\n",
    "    I am studying medicine at UBC because I want to work as a ___.\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Language modeling: Why should we care?\n",
    "\n",
    "Powerful idea in NLP and helps in many tasks.\n",
    "- Machine translation \n",
    "    * P(In the age of data algorithms have the answer) > P(the age data of in algorithms answer the have)\n",
    "- Spelling correction\n",
    "    * My office is a 20  <span style=\"color:red\">minuet</span> bike ride from my home.  \n",
    "        * P(20 <span style=\"color:blue\">minute</span> bike ride from my home) > P(20 <span style=\"color:red\">minuet</span> bike ride from my home)\n",
    "- Speech recognition \n",
    "    * P(<span style=\"color:blue\">I read</span> a book) > P(<span style=\"color:red\">Eye red</span> a book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation: Language modeling task \n",
    "\n",
    "- Recall that when we used Markov models for this task, we made Markov assumption. \n",
    "    - Markov model: $P(w_t|w_1,w_2,\\dots,w_{t-1}) \\approx P(w_t|w_{t-1})$\n",
    "    - Markov model with more context: $P(w_t|w_1,w_2,\\dots,w_{t-1}) \\approx P(w_t|w_{t-2}, w_{t-1})$ \n",
    "- These models are 'memoryless' in the sense that they do not have memory beyond the previous 2, 3 or maximum $n$ steps and when $n$ becomes larger, there is sparsity problem.  \n",
    "- Also, they have huge RAM requirements because you have to store all ngrams. \n",
    "- Would a Markov model with $n=5$ predict the correct words in the following cases? \n",
    "<blockquote>\n",
    "    I am studying medicine at UBC because I want to work as a <b>__</b>.<br>\n",
    "    I am studying law at UBC because I want to work as a <b>__</b>.<br>\n",
    "    I am studying history at UBC because I want to work as a <b>__</b>.     \n",
    "</blockquote>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNNs motivation \n",
    "\n",
    "- RNNs can help us with this limited memory problem!\n",
    "- **RNNs are a kind of neural network model which use hidden units to remember things over time.**   \n",
    "- Critically, unlike Markov models, this approach does not impose a fixed-length limit on this prior context; the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence.\n",
    "- Condition the neural network on all previous time steps. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN intuition: Example\n",
    "\n",
    "- Put a number of feedforward networks together.\n",
    "- Suppose I have 1 word represented by a vector of size 4 and I want to predict something about that word, I use one feedforward neural network. \n",
    "- Suppose I have 2 words, I use 2 of these networks and put them together. \n",
    "\n",
    "![](img/RNN_two_feedforward.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_two_feedforward.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center> -->\n",
    "\n",
    "(Image credit: [learnopencv](https://www.learnopencv.com/understanding-feedforward-neural-networks/))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we put multiple feedforward networks together? \n",
    "\n",
    "- Put a number of feedforward networks together by making connections between the hidden layers.\n",
    "- Process sequences by presenting one element at a time to the network.\n",
    "- So we have an input, hidden layer, and an output and the hidden layer is connected to itself. \n",
    "\n",
    "![](img/RNN_introduction.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_introduction.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [Stanford CS224d slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN presentations\n",
    "\n",
    "- Unrolled presentation \n",
    "\n",
    "![](img/RNN_introduction.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_introduction.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- </center>  -->\n",
    "\n",
    "- Recursive presentation\n",
    "\n",
    "![](img/RNN_recursive_2.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_recursive_2.png\" height=\"300\" width=\"300\">  -->\n",
    "<!-- </center>      -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are adding a temporal dimension to the feed-forward neural network. \n",
    "- The hidden layer from the previous time step provides a form of **memory** which informs the decisions to be made at later points in time. \n",
    "- The main difference between non-recurrent architectures and recurrent architectures is the new set of weights that connect the hidden layer from previous time step to the current hidden layer. \n",
    "- These weights determine how the network will make use of the previous context when calculating output for the current input. \n",
    "- Similar to other weights in neural network models, these weights will be trained with backpropagation. \n",
    "\n",
    "![](img/RNN_introduction.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN_introduction.png\" height=\"500\" width=\"500\">  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN as a graphical model\n",
    "\n",
    "- RNNs can be visualized as a graphical model. The states below are the hidden layers in each time step.  \n",
    "    - Somewhat similar to hidden Markov models (HMMs) \n",
    "    - But a hidden state in an RNN is continuous valued, high dimensional, and much richer. \n",
    "- Each state is a function of the previous state and the input.\n",
    "- A state contains information about the whole past sequence. \n",
    "    - $s_t = g(x_t, x_{t-1}, \\dots, x_2, x_1)$ \n",
    "\n",
    "![](img/RNN_dynamic_model.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_dynamic_model.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameter sharing\n",
    "\n",
    "- What are the parameters of this model? We have three weight matrices. \n",
    "    - Input to hidden weight matrix: $U$\n",
    "    - Hidden to output weight matrix: $V$    \n",
    "    - Hidden to hidden weight matrix: $W$\n",
    "    \n",
    "- The key point in RNNs: **We share all weights between time steps.**    \n",
    "\n",
    "![](img/RNN_dynamic_model.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_dynamic_model.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### RNN parameters\n",
    "\n",
    "- Input size: Suppose $x \\in \\mathbb{R}^d$\n",
    "- Output size: Suppose $y \\in \\mathbb{R}^q$\n",
    "- Hidden size: Suppose $s \\in \\mathbb{R}^p$\n",
    "- Three kinds of weights: $U_{d\\times p}$, $V_{p\\times q}$, $W_{p\\times p}$    \n",
    "\n",
    "![](img/RNN_dynamic_model.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN_dynamic_model.png\" height=\"800\" width=\"800\">  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN parameters: Language modeling example\n",
    "\n",
    "- Embedding size: 300, vocabulary size: 10,000\n",
    "- Hidden layer size: 100 (memory of the network)\n",
    "- $x_t \\in \\mathbb{R}^{300}$\n",
    "- $y_t \\in \\mathbb{R}^{10000}$\n",
    "- $s_t \\in \\mathbb{R}^{100}$\n",
    "- $U_{300\\times 100}$, $V_{100\\times 10000}$, $W_{100\\times 100}$\n",
    "\n",
    "![](img/RNN_dynamic_model.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_dynamic_model.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass in RNNs\n",
    "\n",
    "- Given an input $x_t$ at timestep $t$, how do we compute the new state $s_{t}$ and output $\\hat{y_t}$? \n",
    "\n",
    "![](img/RNN_dynamic_model.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN_dynamic_model.png\" height=\"800\" width=\"800\">  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the new state $s_t$\n",
    "\n",
    "- Multiply the input $x_t$ with the weight matrix between input and hidden layer ($U$) and the state or the hidden layer from the previous time step $s_{t-1}$ with the weight matrix between hidden layers ($W$). \n",
    "- Add these values together. \n",
    "- Add the bias vector and pass the result through a suitable activation function $g$. \n",
    "\n",
    "$$\n",
    "s_t = g(Ws_{t-1} + Ux_t + b_1)\\\\\n",
    "$$ \n",
    "\n",
    "![](img/RNN_dynamic_model.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_dynamic_model.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the output $\\hat{y_t}$\n",
    "\n",
    "- Once we have the value for the new state $s_t$, we can calculate the output vector $\\hat{y_t}$ by multiplying $s_t$ with the weight matrix $V$ between the hidden layer and the output layer, adding the bias vector, and applying an appropriate activation function $f$ the multiplication.  \n",
    "\n",
    "$$\n",
    "\\hat{y}_t = f(Vs_t + b_2)\n",
    "$$ \n",
    "\n",
    "![](img/RNN_dynamic_model.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_dynamic_model.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "- Typically, we are interested in soft classification. So computing $\\hat{y_t}$ consists of a softmax computation which provides a probability distribution over the possible output classes. \n",
    "\n",
    "$$\n",
    "\\hat{y}_t = \\text{softmax}(Vs_t + b_2)\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward pass in RNNs\n",
    "\n",
    "So in the forward pass we compute the new state and the output $\\hat{y_t}$ for all time steps in a sequence, as shown below.  \n",
    "\n",
    "$$\n",
    "s_t = g(Ws_{t-1} + Ux_t + b_1)\\\\\n",
    "\\hat{y}_t = \\text{softmax}(Vs_t + b_2)\n",
    "$$ \n",
    "\n",
    "![](img/RNN_dynamic_model.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_dynamic_model.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward pass in RNNs\n",
    "\n",
    "We compute this for the full sequence. \n",
    "\n",
    "- Given: $x$, network\n",
    "- $s_0 = 0$\n",
    "- for $t$ in 1 to length(input sequence $x$)\n",
    "    - $s_t = g(Ws_{t-1} + Ux_t + b_1$)\n",
    "    - $\\hat{y}_t = \\text{softmax}(Vs_t + b_2)$\n",
    "\n",
    "Note that the matrices $U$, $V$ and $W$ are **shared across time** and new values for $s_t$ and $\\hat{y_t}$ are calculated at each time step.\n",
    "\n",
    "![](img/RNN_dynamic_model.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_dynamic_model.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Forward pass with PyTorch\n",
    "\n",
    "- See the documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an RNN object \n",
    "\n",
    "We are creating an RNN with \n",
    "- only one hidden layer \n",
    "- input of size 20 (e.g., imagine word vectors of size 20)\n",
    "- hidden layer of size 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(20, 10, 1)  # input size, hidden_size, number of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input\n",
    "\n",
    "- The input is going to be sequences (e.g., sequences of words)\n",
    "- We need to provide the sequence length of the sequence and the size of each input vector. \n",
    "- For example, suppose you have the following sequence and you are representing each word with a 20-dimensional word vector, then your sequence length is going to be 5 and input size is going to be 20.  \n",
    "\n",
    "> Cherry blossoms are beautiful ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7574e-02, -2.2040e+00,  8.5885e-02, -3.7213e-01, -7.6761e-04,\n",
       "          7.2929e-01, -8.2384e-01, -1.8235e-01,  1.1587e+00,  1.6223e+00,\n",
       "         -2.2509e+00,  8.4252e-01, -1.3771e+00, -2.7338e+00,  1.4807e+00,\n",
       "          9.9048e-02, -1.0505e+00, -7.2140e-01,  1.0735e+00,  1.6458e+00],\n",
       "        [-1.2386e-01, -6.2394e-01,  2.5957e+00,  1.1262e+00,  1.1617e+00,\n",
       "         -2.2316e-02,  2.6256e-01, -8.1836e-01,  3.7086e-01,  5.3580e-01,\n",
       "          1.1683e-01, -7.1010e-01,  3.9168e-01,  3.3282e-01, -3.8471e-02,\n",
       "          3.7599e-01,  6.9960e-02, -8.3775e-02, -6.3650e-01,  3.0342e-01],\n",
       "        [-8.9973e-01,  2.4189e-01,  1.6836e+00,  1.7879e+00,  3.7536e-01,\n",
       "          1.2323e+00,  1.2282e+00,  1.0380e+00, -1.1312e+00,  1.5787e+00,\n",
       "          1.9125e-01, -1.7115e+00,  1.1831e+00,  1.5660e-01,  1.0187e+00,\n",
       "          8.4310e-02, -1.8809e-01, -1.2377e+00, -1.7061e+00,  3.4520e-01],\n",
       "        [-2.1737e-01,  8.0734e-02,  6.1478e-01, -1.7132e-01, -1.0812e+00,\n",
       "          7.2855e-01,  6.7692e-01, -4.7923e-01,  3.0847e-01, -6.4959e-01,\n",
       "         -8.3220e-01,  1.5893e+00, -6.5650e-02,  6.4784e-01, -1.2412e+00,\n",
       "         -8.7742e-01, -6.3395e-01,  1.9172e+00,  3.3983e-01,  1.1077e+00],\n",
       "        [-6.1814e-01, -4.3499e-01, -3.9480e-01,  1.4739e+00,  6.3557e-01,\n",
       "          1.1295e+00,  2.7308e-01,  2.3515e-01,  6.6725e-01,  8.1861e-01,\n",
       "          9.2862e-01,  1.0600e+00,  1.5811e+00,  3.9418e-01,  1.4936e+00,\n",
       "         -1.4536e+00,  2.4014e-01,  2.5343e-01,  1.2940e+00,  1.2727e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(5, 20)  # sequence length, input size\n",
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the 0th time step, we do not have anything to remember. So we initialize the hidden state randomly. \n",
    "- Let's initialize h0. \n",
    "- The shape of h0 is the number of hidden layers and hidden size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.randn(1, 10)  # number of hidden layers, hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8056,  0.6861,  1.3814,  0.9685, -1.0769,  1.2966, -1.1321, -1.2586,\n",
       "         -0.2973,  0.3546]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating new hidden states and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch calculates the output and new hidden states for us for all time steps.\n",
    "output, hn = rnn(inp, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1727,  0.6454, -0.4417, -0.4433, -0.6560, -0.6890,  0.6628, -0.8453,\n",
       "         -0.4844, -0.5396]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn  # hidden state for the last time step in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7415, -0.9241, -0.7748, -0.7862, -0.8525,  0.0716,  0.9822, -0.8240,\n",
       "          0.6622, -0.8995],\n",
       "        [-0.9070,  0.7495, -0.0297, -0.7718, -0.8986,  0.0978, -0.6883, -0.1934,\n",
       "         -0.6140,  0.9283],\n",
       "        [-0.6862,  0.5918,  0.2887, -0.7196, -0.5608, -0.0134,  0.2477,  0.1595,\n",
       "         -0.3826,  0.9602],\n",
       "        [ 0.7681,  0.4862, -0.0831,  0.8805, -0.7822, -0.2449, -0.2778,  0.2566,\n",
       "          0.6631,  0.6536],\n",
       "        [-0.1727,  0.6454, -0.4417, -0.4433, -0.6560, -0.6890,  0.6628, -0.8453,\n",
       "         -0.4844, -0.5396]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape  # For each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `tanh` activation function is used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapes of the weight matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight matrix $U$ between input to hidden layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 20])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.state_dict()[\"weight_ih_l0\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight matrix $W$ between hidden layer in time step $t$ to hidden layer in time step $t+1$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.state_dict()[\"weight_hh_l0\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `rnn` above is calculating the output of the hidden layer at each time step but we do not calculating $\\hat{y}$ in each time step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you\n",
    "\n",
    "iClicker cloud join link: https://join.iclicker.com/4QVT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 6.1: Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) RNNs pass along information between time steps through hidden layers.\n",
    "- (B) RNNs are appropriate only for text data.\n",
    "- (C) At each time step in an RNN, we use a unique hidden state (`h`), a unique input (`X`), but we reuse the same `W` matrix of weights.\n",
    "- (D) The number of parameters in an RNN language model would grow with the number of time steps.\n",
    "- (E) If you have `n` sequences, the input of an RNN is a three dimensional tensor. \n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 6.1: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "- (A) True\n",
    "- (B) False\n",
    "- (C) True\n",
    "- (D) False\n",
    "- (E) True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN is a **supervised machine learning model**. Similar to feedforward networks, we'll use a \n",
    "    - training set\n",
    "    - a loss function  \n",
    "    - backpropagation to obtain the gradients needed to adjust the weights in these networks \n",
    "\n",
    "- We have 3 sets of weights (and the corresponding bias terms) to update\n",
    "    - $U \\rightarrow $ the weight matrix between input layer and hidden layer\n",
    "    - $W \\rightarrow $ the weight matrix between previous hidden layer to current hidden layer\n",
    "    - $V \\rightarrow $ the weight matrix between hidden layer and output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to assess the error occurring at time step $t$.\n",
    "\n",
    "- To compute the loss function for the output at time $t$ we need the hidden layer from time $t-1$.\n",
    "- The hidden layer at time $t$ influences both the output at time $t$ and the hidden layer at time $t+1$. \n",
    "\n",
    "![](img/RNN_loss.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN_loss.png\" height=\"1500\" width=\"1500\">  -->\n",
    "\n",
    "[Credit](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To assess the error occurring to $h_t$, we need to know its influence on both the current output and the ones that follow.  \n",
    "- This is different than the usual backpropagation. We need to tailor backpropogation algorithm to this situation. In RNNs we use a generalized version of **Backpropogation called Backpropogation Through Time (BPTT)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "- The overall loss is the summation of losses at each time step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN code in 112 lines of Python\n",
    "\n",
    "- See [the code](https://gist.github.com/karpathy/d4dee566867f8291f086) for the above in ~112 lines of Python written by Andrej Karpathy. The code has only `numpy` dependency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN applications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### What can we do with RNNs?\n",
    "\n",
    "- We have seen the basic RNN architecture below. \n",
    "\n",
    "![](img/RNN_introduction.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_introduction.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- <center> -->\n",
    "\n",
    "- But a number of architectures are possible, which makes them a very rich family of models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN architectures\n",
    "\n",
    "- A number of possible RNN architectures\n",
    "\n",
    "![](img/RNN_architectures.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_architectures.png\" height=\"1500\" width=\"1500\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "[source](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how can we apply it to three different types of NLP tasks:\n",
    "- Sequence labeling (e.g., POS tagging)\n",
    "- Sequence classification (e.g. sentiment analysis or text classification)\n",
    "- Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sequence labeling \n",
    "\n",
    "- The task is to assign a label from a fixed set of labels to each element in the sequence.  \n",
    "    - Part-of-speech tagging \n",
    "    - Named entity recognition\n",
    "- Many-to-many architecture\n",
    "- Inputs are usually pre-trained word embeddings and outputs are tag probabilities generated by a softmax layer over the given tagset. \n",
    "- The RNN block is an abstraction representing an unrolled simple RNN consisting of an input layer, hidden layer and output layer at each time step and shared weight matrices $U$, $W$, and $V$. \n",
    "\n",
    "![](img/RNN_seq_labeling.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN_seq_labeling.png\" height=\"800\" width=\"800\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequence classification\n",
    "\n",
    "- We have done text classification such as sentiment analysis or spam identification before with traditional ML models, where we ignored the temporal nature of language.  \n",
    "- These are actually sequence classification tasks where we want to map a sequence of text to a label from a small set of labels (e.g., positive, negative, neutral). \n",
    "- To apply RNNs in this setting, we take the text to be classified and pass one word at a time generating a new hidden layer at each time step. We can then take the hidden layer from the last time step, $h_n$, which has the compressed representation of the entire sequence. We pass this representation through a feedforward neural network which chooses a class via a softmax.     \n",
    "- This is a many-to-one RNN architecture. \n",
    "\n",
    "![](img/RNN_classification.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN_classification.png\" height=\"800\" width=\"800\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similar to the sequence labeling example, we can also pass word embeddings as input. \n",
    "- Note that in this approach we do not have immediate outputs at each time step and we do not need to compute $\\hat{y}$ at each time step. We only have an output at the last time step. \n",
    "- So there won't be loss terms associated with each time step. \n",
    "- The loss function used to train the weights in the network is entirely based on the final text classification task. \n",
    "- We will compare the the output of the softmax layer of the feed-forward classifier and the actual $y$ to calculate the loss (e.g., cross-entropy loss) and this loss will drive the training. \n",
    "- The error signal is backpropagated all the way through the weights in the feed-forward classifier, through its input, which is the hidden layer of the last time step, through the three sets of RNN weights: $U$, $V$, and $W$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text generation\n",
    "\n",
    "- The idea is similar to text generation with Markov models. \n",
    "- We start with a seed. We then continue to sample words conditioned on our previous choices until we reach a pre-determined desired length of a sequence or end-of-sequence token is generated.\n",
    "- In the context of RNNs\n",
    "    - We start with a seed. In the example below, we are starting with a special beginning of sequence token \\<s\\>. \n",
    "    - We use embedding representation of this token and pass it to the RNN. \n",
    "    - We sample a word in the output from the softmax distribution.  \n",
    "    - We use this sampled word as the input in the next time step and then sample the next word in the same fashion. \n",
    "    - We continue this until the fixed length limit or the end of the sentence marker is reached. \n",
    "\n",
    "![](img/RNN_generation.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_generation.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center> -->\n",
    "    \n",
    "- The same idea can be used for music generation. \n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image captioning \n",
    "<br>\n",
    "\n",
    "- The same idea can be used for more complicated applications such as machine translation, summarization, or image captioning. \n",
    "- The idea is to prime the generation component with an appropriate context. \n",
    "- For example, in image captioning we can prime the the generation component with a meaningful  representation of an image given by the last layer in CNNs.  \n",
    "- We'll talk more about this application next week. \n",
    "\n",
    "![](img/image_captioning.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/image_captioning.png\" width=\"1000\" height=\"1000\"> -->\n",
    "<!-- </center> -->\n",
    "    \n",
    "[Source](https://cs.stanford.edu/people/karpathy/sfmltalk.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We know basics of RNNs. \n",
    "- Now we'll look at a toy example for character-level text generation using RNNs. \n",
    "- Recall that given a sequence of characters, character-level text generation is the task of modeling probability distribution of the next character in the sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A toy \"hello\" RNN \n",
    "- Suppose we want to train a character-level RNN on sequence \"hello\". \n",
    "- The vocabulary is 4 and we want our model to learn the following: \n",
    "    - \"e\" should be likely given \"h\" \n",
    "    - \"l\" should be likely given \"he\" \n",
    "    - \"l\" should be likely given \"hel\" \n",
    "    - \"o\" should be likely given \"hell\"     \n",
    "\n",
    "![](img/RNN_char_generation_train.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_char_generation_train.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- <center>     -->\n",
    "\n",
    "[Source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shapes of input, hidden, and output weight matrices\n",
    "- Shape of $W_{xh}$ ($U$) is going to be: $4 \\times 3$\n",
    "- Shape of $W_{hh}$ ($W$) is going to be: $3 \\times 3$\n",
    "- Shape of $W_{hy}$ ($V$) is going to be: $3 \\times 4$\n",
    "$$\n",
    "s_t = g(Ws_{t-1} + Ux_t + b_1)\\\\\n",
    "\\hat{y}_t = \\text{softmax}(Vs_t + b_2)\n",
    "$$ \n",
    "\n",
    "![](img/RNN_char_generation_train.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_char_generation_train.png\" height=\"600\" width=\"600\">  -->\n",
    "<!-- <center>   -->\n",
    "\n",
    "[Source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's build a simple RNN for this using `PyTorch`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x105efa370>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Let's define a mapping between indices and characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = [\"h\", \"e\", \"l\", \"o\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some representation for the input. Let's use one-hot representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_lookup = [\n",
    "    [1, 0, 0, 0],  # h\n",
    "    [0, 1, 0, 0],  # e\n",
    "    [0, 0, 1, 0],  # l\n",
    "    [0, 0, 0, 1],  # o\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next let's create one-hot representation of `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [0, 1, 2, 2]  # indices for the input \"hell\"\n",
    "X_one_hot = [one_hot_lookup[x] for x in X]\n",
    "inputs = torch.Tensor(X_one_hot)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1, 2, 2, 3]\n",
    "labels = torch.LongTensor(y)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining some variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4  # size of vocab\n",
    "EPOCHS = 10  # number of epochs\n",
    "input_size = 4  # size of vocab or one-hot size\n",
    "hidden_size = 3  # output from the RNN.\n",
    "batch_size = 1  # we are not batching in this toy example.\n",
    "sequence_length = 1  # we are processing characters one by one in this toy example\n",
    "num_layers = 1  # one-layer rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![](img/RNN_char_generation_train.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_char_generation_train.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- <center>     -->\n",
    "\n",
    "[Source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ToyRNN(nn.Module):\n",
    "    def __init__(self, debug=False):\n",
    "        super(ToyRNN, self).__init__()\n",
    "\n",
    "        # PyTorch core RNN module\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size, hidden_size=hidden_size, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layer for the output\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # Debugging flag\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, hidden, x):\n",
    "        x = x.view(batch_size, sequence_length, input_size)  # reshape the input\n",
    "        if self.debug:\n",
    "            print(\"\\n\\n\")\n",
    "            print(\"Input shape = \", x.size())\n",
    "\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        if self.debug:\n",
    "            print(\"out shape = \", out.size())\n",
    "            print(\"Hidden shape = \", hidden.size())\n",
    "\n",
    "        out = out.reshape(out.shape[0], -1)  # reshape to pass before the output layer\n",
    "        if self.debug:\n",
    "            print(\"out shape after reshaing = \", out.size())\n",
    "\n",
    "        out = self.fc(out)\n",
    "        if self.debug:\n",
    "            print(\"out shape after passing through fc = \", out.size())\n",
    "\n",
    "        return hidden, out\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(num_layers, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToyRNN(\n",
      "  (rnn): RNN(4, 3, batch_first=True)\n",
      "  (fc): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ToyRNN()\n",
    "print(model)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# Loss increases as the predicted probability diverges from the actual label.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 6.082, preidcted: oeoe\n",
      "Epoch: 2, loss: 5.008, preidcted: olol\n",
      "Epoch: 3, loss: 4.393, preidcted: llll\n",
      "Epoch: 4, loss: 4.155, preidcted: llll\n",
      "Epoch: 5, loss: 3.991, preidcted: llll\n",
      "Epoch: 6, loss: 3.697, preidcted: llll\n",
      "Epoch: 7, loss: 3.280, preidcted: llll\n",
      "Epoch: 8, loss: 2.864, preidcted: ello\n",
      "Epoch: 9, loss: 2.459, preidcted: ello\n",
      "Epoch: 10, loss: 2.020, preidcted: ello\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    pred = \"\"\n",
    "    for inp, label in zip(inputs, labels):\n",
    "        hidden, output = model(hidden, inp)\n",
    "        val, idx = output.max(1)\n",
    "        pred += idx2char[idx.data[0]]\n",
    "        loss += criterion(output, torch.LongTensor([label]))\n",
    "    print(\"Epoch: %d, loss: %1.3f, preidcted: %s\" % (epoch + 1, loss, pred))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/RNN_char_generation_train.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_char_generation_train.png\" height=\"600\" width=\"600\">  -->\n",
    "<!-- <center>     -->\n",
    "\n",
    "[Source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We have our toy RNN for text generation! \n",
    "- Usually we would do it on large text corpora (e.g., the whole Wikipedia or The New York Times articles from the last 20 years). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked and Bidirectional RNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have seen a simple RNN with one hidden layer. \n",
    "- But RNNs are quite flexible. \n",
    "- Two common ways to create complex networks by combining RNNs are:\n",
    "    - Stacked RNNs\n",
    "    - Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked RNNs \n",
    "\n",
    "- In the examples thus far, the input of RNNs was a sequence of word or character embeddings. We were passing the output of the RNN layer to the output layer and the outputs have been vectors useful for predicting next words, tags, or sequence labels.  \n",
    "\n",
    "![](img/RNN_seq_labeling.png)\n",
    "\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But nothing prevents us from using **the sequence of outputs from one RNN as an input sequence to another one**.\n",
    "- These are called **stacked RNNs** which consist of multiple networks where the output of one layer serves as the input to a subsequent layer. \n",
    "\n",
    "![](img/RNN_stacked.png)\n",
    "\n",
    "<!-- ![](img/RNN_stacked.png) -->\n",
    "\n",
    "<!-- <img src=\"img/RNN_stacked.png\" height=\"800\" width=\"800\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stacked RNNs generally outperform single-layer networks. \n",
    "- The network learns a different level of abstraction at each layer. \n",
    "- You can optimize your network for number of layers for your specific application and dataset.  \n",
    "- But remember that more layers means higher training cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs \n",
    "\n",
    "- The RNN uses information from the prior context to make predictions at time $t$. \n",
    "- But in many applications (e.g., POS tagging) we do have access to the entire input sequence and knowing the context on the right of time $t$ can be useful. \n",
    "- For example, suppose you are doing POS tagging and you are at the token **Teddy** in the sequence. It will be useful to know the right context in order to make the decision on whether it should be tagged as a _noun_ or a _proper noun_.  \n",
    "\n",
    "> He said , \" Teddy Roosevelt was a great president ! \"<br>\n",
    "\n",
    "> He said , \" Teddy bears are on sale ! \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can we use the words on the right of time step $t$ as context?  \n",
    "- In the left-to-right RNN, the hidden state at time $t$ represents everything the network knows about the sequence up to that point. \n",
    "- Suppose $h_t^f$ denotes a hidden state at time $t$ representing everything the network has gleaned from the sequence so far. \n",
    "$$h_t^f = RNN_{forward}(x_1, x_2, \\dots, x_t) $$\n",
    "- We can also train the network in the reverse direction, from right to left, to take advantage of the right context. \n",
    "- With this approach the hidden state at time $t$, $h_t^b$ represents all the information we have learned about the sequence from time $t$ to the end of the sequence. \n",
    "$$h_t^b = RNN_{backward}(x_t, x_{t+1}, \\dots, x_n) $$\n",
    "- (Somewhat similar to the $\\alpha$ and $\\beta$ values in the forward and backward algorithms in HMMs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **bidirectional RNN** combines two independent RNNs:\n",
    "- One where the input is processed from the start to the end\n",
    "- The other from the end to the start. \n",
    "- Each RNN will result in some representation of the input. \n",
    "- We then combine the two representations computed by two independent RNNs into a single vector which captures both the left and right contexts of an input at each point in time. \n",
    "- We can combine vectors by\n",
    "    - Concatenating them, as shown in the picture below or\n",
    "    - Element-wise addition \n",
    "    - Element-wise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/bidirectional_seq_labeling.png)\n",
    "<!-- <img src=\"img/bidirectional_seq_labeling.png\" height=\"800\" width=\"800\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also use bidirectional RNNs for sequence classification. \n",
    "- Recall that in sequence classification we pass the final hidden state of the RNN as input to a subsequent feedforward classifier. \n",
    "- The problem with this approach is that the final hidden state reflects more information about the end of the sequence than its beginning. \n",
    "- Bidirectional RNNs provide a simple solution to this problem. We can create a final hidden state by combining hidden states of forward and backward passes so that the hidden state reflects information about both the beginning and end of the sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/bidirectional_classification.png)\n",
    "<!-- <img src=\"img/bidirectional_classification.png\" height=\"800\" width=\"800\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final comments and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important ideas to know \n",
    "\n",
    "- RNNs are supervised neural network models to process sequential data.\n",
    "- The intuition is to put multiple feed-forward networks together and making connections between hidden layers.  \n",
    "- They have feedback connections in their structure to \"remember\" previous inputs, when reading in a sequence. \n",
    "- In simple RNNs sequences are processed one element at a time. The output of each neural unit at time $t$ is based on the current input at $t$ and the hidden layer at time $t-1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important ideas to know\n",
    "\n",
    "- In RNNs, the parameters are shared across different time steps.\n",
    "- A generalized version of backpropagation called backpropagation through time is used for training the network. \n",
    "- In practice truncated backpropagation through time is used where we work through chunks. \n",
    "- A number of RNNs architectures are possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important ideas to know\n",
    "\n",
    "- RNNs dail on capturing long-distance dependencies because of the problems like vanishing gradients.  \n",
    "- In practice, some other complicated variants such as LSTMs and GRUs are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coming up ...\n",
    "\n",
    "- LSTMs\n",
    "- Intuition of transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resources\n",
    "\n",
    "- [Sequence processing with Recurrent Neural Networks](https://web.stanford.edu/~jurafsky/slp3/9.pdf) (The notes above are heavily based on this resource.)\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [Coursera: NLP sequence models](https://www.coursera.org/lecture/nlp-sequence-models/recurrent-neural-network-model-ftkzt)\n",
    "- [RNN code in 112 lines of Python](https://gist.github.com/karpathy/d4dee566867f8291f086#file-min-char-rnn-py-L112)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:563]",
   "language": "python",
   "name": "conda-env-563-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
