{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/575_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 7: More RNNs, LSTMs\n",
    "\n",
    "UBC Master of Data Science program, 2021-22\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture plan, imports, LO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Lecture plan \n",
    "\n",
    "- Lab 4 announcement \n",
    "- RNN Toy example\n",
    "- Stacked and bidirectional RNNs\n",
    "- LSTM motivation \n",
    "- iClicker questions\n",
    "- Break\n",
    "- LSTMs\n",
    "- LSTM toy example\n",
    "- Final comments and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import RNN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learning outcomes\n",
    "\n",
    "From this lecture you will be able to \n",
    "\n",
    "- Broadly explain character-level text generation with RNNs;\n",
    "- Specify the shapes of weight matrices in RNNs;\n",
    "- Carry out forward pass with RNNs in `PyTorch`;\n",
    "- Explain stacked RNNs and bidirectional RNNs and the difference between the two;\n",
    "- Broadly explain the problem of vanishing gradients; \n",
    "- Broadly explain the idea of LSTMs at a high level;\n",
    "- Carry out forward pass with LSTMs in `PyTorch`;\n",
    "- Explain the shapes of input, output, hidden state and cell state in LSTMs;\n",
    "- Broadly explain numericalization, batching, and padding in text preprocessings;\n",
    "- Explain the motivation to use `torchtext`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributions\n",
    "\n",
    "This material is heavily based on [Jurafsky and Martin, Chapter 9]((https://web.stanford.edu/~jurafsky/slp3/9.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap and motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN Toy example with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From the last lecture\n",
    "\n",
    "- We know basics of RNNs. \n",
    "- Now we'll look at a toy example for character-level text generation using RNNs. \n",
    "- Recall that given a sequence of characters, character-level text generation is the task of modeling probability distribution of the next character in the sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A toy \"hello\" RNN \n",
    "- Suppose we want to train a character-level RNN on sequence \"hello\". \n",
    "- The vocabulary is 4 and we want our model to learn the following: \n",
    "    - \"e\" should be likely given \"h\" \n",
    "    - \"l\" should be likely given \"he\" \n",
    "    - \"l\" should be likely given \"hel\" \n",
    "    - \"o\" should be likely given \"hell\"     \n",
    "\n",
    "![](img/RNN_char_generation_train.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_char_generation_train.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- <center>     -->\n",
    "\n",
    "[Source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shapes of input, hidden, and output weight matrices\n",
    "- Shape of $W_{xh}$ ($U$) is going to be: $4 \\times 3$\n",
    "- Shape of $W_{hh}$ ($W$) is going to be: $3 \\times 3$\n",
    "- Shape of $W_{hy}$ ($V$) is going to be: $3 \\times 4$\n",
    "$$\n",
    "s_t = g(Ws_{t-1} + Ux_t + b_1)\\\\\n",
    "\\hat{y}_t = \\text{softmax}(Vs_t + b_2)\n",
    "$$ \n",
    "\n",
    "![](img/RNN_char_generation_train.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_char_generation_train.png\" height=\"600\" width=\"600\">  -->\n",
    "<!-- <center>   -->\n",
    "\n",
    "[Source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's build a simple RNN for this using `PyTorch`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x105efa370>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Let's define a mapping between indices and characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = [\"h\", \"e\", \"l\", \"o\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some representation for the input. Let's use one-hot representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_lookup = [\n",
    "    [1, 0, 0, 0],  # h\n",
    "    [0, 1, 0, 0],  # e\n",
    "    [0, 0, 1, 0],  # l\n",
    "    [0, 0, 0, 1],  # o\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next let's create one-hot representation of `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [0, 1, 2, 2]  # indices for the input \"hell\"\n",
    "X_one_hot = [one_hot_lookup[x] for x in X]\n",
    "inputs = torch.Tensor(X_one_hot)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1, 2, 2, 3]\n",
    "labels = torch.LongTensor(y)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining some variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4  # size of vocab\n",
    "EPOCHS = 10  # number of epochs\n",
    "input_size = 4  # size of vocab or one-hot size\n",
    "hidden_size = 3  # output from the RNN.\n",
    "batch_size = 1  # we are not batching in this toy example.\n",
    "sequence_length = 1  # we are processing characters one by one in this toy example\n",
    "num_layers = 1  # one-layer rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![](img/RNN_char_generation_train.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_char_generation_train.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- <center>     -->\n",
    "\n",
    "[Source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ToyRNN(nn.Module):\n",
    "    def __init__(self, debug=False):\n",
    "        super(ToyRNN, self).__init__()\n",
    "\n",
    "        # PyTorch core RNN module\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size, hidden_size=hidden_size, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layer for the output\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # Debugging flag\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, hidden, x):\n",
    "        x = x.view(batch_size, sequence_length, input_size)  # reshape the input\n",
    "        if self.debug:\n",
    "            print(\"\\n\\n\")\n",
    "            print(\"Input shape = \", x.size())\n",
    "\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        if self.debug:\n",
    "            print(\"out shape = \", out.size())\n",
    "            print(\"Hidden shape = \", hidden.size())\n",
    "\n",
    "        out = out.reshape(out.shape[0], -1)  # reshape to pass before the output layer\n",
    "        if self.debug:\n",
    "            print(\"out shape after reshaing = \", out.size())\n",
    "\n",
    "        out = self.fc(out)\n",
    "        if self.debug:\n",
    "            print(\"out shape after passing through fc = \", out.size())\n",
    "\n",
    "        return hidden, out\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(num_layers, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToyRNN(\n",
      "  (rnn): RNN(4, 3, batch_first=True)\n",
      "  (fc): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ToyRNN()\n",
    "print(model)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# Loss increases as the predicted probability diverges from the actual label.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 6.082, preidcted: oeoe\n",
      "Epoch: 2, loss: 5.008, preidcted: olol\n",
      "Epoch: 3, loss: 4.393, preidcted: llll\n",
      "Epoch: 4, loss: 4.155, preidcted: llll\n",
      "Epoch: 5, loss: 3.991, preidcted: llll\n",
      "Epoch: 6, loss: 3.697, preidcted: llll\n",
      "Epoch: 7, loss: 3.280, preidcted: llll\n",
      "Epoch: 8, loss: 2.864, preidcted: ello\n",
      "Epoch: 9, loss: 2.459, preidcted: ello\n",
      "Epoch: 10, loss: 2.020, preidcted: ello\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    pred = \"\"\n",
    "    for inp, label in zip(inputs, labels):\n",
    "        hidden, output = model(hidden, inp)\n",
    "        val, idx = output.max(1)\n",
    "        pred += idx2char[idx.data[0]]\n",
    "        loss += criterion(output, torch.LongTensor([label]))\n",
    "    print(\"Epoch: %d, loss: %1.3f, preidcted: %s\" % (epoch + 1, loss, pred))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/RNN_char_generation_train.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_char_generation_train.png\" height=\"600\" width=\"600\">  -->\n",
    "<!-- <center>     -->\n",
    "\n",
    "[Source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We have our toy RNN for text generation! \n",
    "- Usually we would do it on large text corpora (e.g., the whole Wikipedia or The New York Times articles from the last 20 years). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked and Bidirectional RNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have seen a simple RNN with one hidden layer. \n",
    "- But RNNs are quite flexible. \n",
    "- Two common ways to create complex networks by combining RNNs are:\n",
    "    - Stacked RNNs\n",
    "    - Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked RNNs \n",
    "\n",
    "- In the examples thus far, the input of RNNs was a sequence of word or character embeddings. We were passing the output of the RNN layer to the output layer and the outputs have been vectors useful for predicting next words, tags, or sequence labels.  \n",
    "\n",
    "![](img/RNN_seq_labeling.png)\n",
    "\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But nothing prevents us from using **the sequence of outputs from one RNN as an input sequence to another one**.\n",
    "- These are called **stacked RNNs** which consist of multiple networks where the output of one layer serves as the input to a subsequent layer. \n",
    "\n",
    "![](img/RNN_stacked.png)\n",
    "\n",
    "<!-- ![](img/RNN_stacked.png) -->\n",
    "\n",
    "<!-- <img src=\"img/RNN_stacked.png\" height=\"800\" width=\"800\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stacked RNNs generally outperform single-layer networks. \n",
    "- The network learns a different level of abstraction at each layer. \n",
    "- You can optimize your network for number of layers for your specific application and dataset.  \n",
    "- But remember that more layers means higher training cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs \n",
    "\n",
    "- The RNN uses information from the prior context to make predictions at time $t$. \n",
    "- But in many applications (e.g., POS tagging) we do have access to the entire input sequence and knowing the context on the right of time $t$ can be useful. \n",
    "- For example, suppose you are doing POS tagging and you are at the token **Teddy** in the sequence. It will be useful to know the right context in order to make the decision on whether it should be tagged as a _noun_ or a _proper noun_.  \n",
    "\n",
    "> He said , \" Teddy Roosevelt was a great president ! \"<br>\n",
    "\n",
    "> He said , \" Teddy bears are on sale ! \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can we use the words on the right of time step $t$ as context?  \n",
    "- In the left-to-right RNN, the hidden state at time $t$ represents everything the network knows about the sequence up to that point. \n",
    "- Suppose $h_t^f$ denotes a hidden state at time $t$ representing everything the network has gleaned from the sequence so far. \n",
    "$$h_t^f = RNN_{forward}(x_1, x_2, \\dots, x_t) $$\n",
    "- We can also train the network in the reverse direction, from right to left, to take advantage of the right context. \n",
    "- With this approach the hidden state at time $t$, $h_t^b$ represents all the information we have learned about the sequence from time $t$ to the end of the sequence. \n",
    "$$h_t^b = RNN_{backward}(x_t, x_{t+1}, \\dots, x_n) $$\n",
    "- (Somewhat similar to the $\\alpha$ and $\\beta$ values in the forward and backward algorithms in HMMs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **bidirectional RNN** combines two independent RNNs:\n",
    "- One where the input is processed from the start to the end\n",
    "- The other from the end to the start. \n",
    "- Each RNN will result in some representation of the input. \n",
    "- We then combine the two representations computed by two independent RNNs into a single vector which captures both the left and right contexts of an input at each point in time. \n",
    "- We can combine vectors by\n",
    "    - Concatenating them, as shown in the picture below or\n",
    "    - Element-wise addition \n",
    "    - Element-wise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/bidirectional_seq_labeling.png)\n",
    "<!-- <img src=\"img/bidirectional_seq_labeling.png\" height=\"800\" width=\"800\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also use bidirectional RNNs for sequence classification. \n",
    "- Recall that in sequence classification we pass the final hidden state of the RNN as input to a subsequent feedforward classifier. \n",
    "- The problem with this approach is that the final hidden state reflects more information about the end of the sequence than its beginning. \n",
    "- Bidirectional RNNs provide a simple solution to this problem. We can create a final hidden state by combining hidden states of forward and backward passes so that the hidden state reflects information about both the beginning and end of the sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/bidirectional_classification.png)\n",
    "<!-- <img src=\"img/bidirectional_classification.png\" height=\"800\" width=\"800\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with RNNs \n",
    "\n",
    "- In practice, you'll hardly see people using vanilla RNNs because they are quite hard to train for tasks that require access to distant information. \n",
    "- Despite having access to the entire previous sequence, the information encoded in hidden states of RNNs is fairly local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the examples below in the context of language modeling. \n",
    "\n",
    "> The students in the exam where the fire alarm is ringing **are** really stressed. \n",
    "\n",
    "> The flies munching on the banana that is lying under the tree which is in full bloom **are** really happy. \n",
    "\n",
    "- Assigning high probability to **_is_** following *alarm* is straightforward since it provides a local context for singular agreement. \n",
    "- However, assigning a high probability to **_are_** following _ringing_ is quite difficult because not only the plural _students_ is distant, but also the intervening context involves singular constituents. \n",
    "- Ideally, we want the network to retain the distant information about the plural **_students_** until it's needed while still processing the intermediate parts of the sequence correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The hidden layer and the weights that determine the values in the hidden layer are asked to perform two tasks simultaneously:\n",
    "    - Providing information useful for current decision\n",
    "    - Updating and carrying forward information required for future decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem 2: Vanishing gradients\n",
    "\n",
    "- Another difficulty with training RNNs arises from the need to backpropagate the error signal back through time. \n",
    "- Recall that we learn RNNs with \n",
    "    - Forward pass \n",
    "    - Backward pass (backprop through time)\n",
    "    \n",
    "- Computing new states and output in RNNs\n",
    "\n",
    "$$\n",
    "s_t = g(Ws_{t-1} + Ux_t + b_1)\\\\\n",
    "\\hat{y}_t = \\text{softmax}(Vs_t + b_2)\n",
    "$$ \n",
    "\n",
    "![](img/RNN_dynamic_model.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_dynamic_model.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall: Backpropagation through time\n",
    "\n",
    "- When we do backprop with feedforward neural networks\n",
    "    - Take the gradient (derivative) of the loss with respect to the parameters. \n",
    "    - Change parameters to minimize the loss. \n",
    "\n",
    "- In RNNs we use a generalized version of backprop called Backpropogation Through Time (BPTT)\n",
    "    - Calculating gradient at each output depends upon the current time step as well as the previous time steps. \n",
    "\n",
    "![](img/RNN_loss.png)\n",
    "\n",
    "<!-- <center>     -->\n",
    "<!-- <img src=\"img/RNN_loss.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center> -->\n",
    "    \n",
    "[Credit](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- So in the backward pass of RNNs, we have to multiply many derivatives together, which very often results in\n",
    "    - vanishing gradients (gradients becoming very small and eventually driven to zero) in case of long sequences\n",
    "- If we have a vanishing gradient, we might not be able to update our weights reliably. \n",
    "- Only able to capture short-term dependencies, which kind of defeats the whole purpose of using RNNs.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To address these issues more complex network architectures have been designed with the goal of maintaining relevant context over time by enabling the network to learn to forget the information that is no longer needed and to remember information required for decisions still to come. \n",
    "- One of the most commonly used such models are \n",
    "    - The Long short-term memory network (LSTM)\n",
    "    - Gated Recurrent Units (GRU)\n",
    "- Let's look at some example applications of LSTMs.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM for image captioning \n",
    "\n",
    "![](img/RNN_LSTM_image_captioning.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_LSTM_image_captioning.png\" height=\"2000\" width=\"2000\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "\n",
    "(Credit: [LSTMs for image captioning](https://arxiv.org/pdf/1411.4555.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Neural Storyteller](https://github.com/ryankiros/neural-storyteller)\n",
    "\n",
    "![](img/RNN_example.jpg)\n",
    "<!-- <img src=\"img/RNN_example.jpg\" width=\"500\" height=\"500\"> -->\n",
    "\n",
    "\n",
    "<blockquote>        \n",
    "<p style=\"font-size:30px\">We were barely able to catch the breeze at the beach , and it felt as if someone stepped out of my mind . She was in love with him for the first time in months , so she had no intention of escaping . The sun had risen from the ocean , making her feel more alive than normal . She 's beautiful , but the truth is that I do n't know what to do ...</p>\n",
    "</blockquote>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTMs for video captioning \n",
    "\n",
    "![](img/RNN_video_captioning2.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_video_captioning2.png\" height=\"1000\" width=\"1000\">  -->\n",
    "<!-- </center> -->\n",
    "\n",
    "(Credit: [LSTMs for video captioning](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Venugopalan_Sequence_to_Sequence_ICCV_2015_paper.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTMs for executing Python programs \n",
    "\n",
    "- In 2014, Google researchers built an LSTM that learns to execute simple\n",
    "Python programs!\n",
    "\n",
    "![](img/RNN_learning_to_execute.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN_learning_to_execute.png\" width=\"1500\" height=\"1500\"> -->\n",
    "\n",
    "(Credit: [Learning to execute](https://arxiv.org/pdf/1410.4615.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you\n",
    "\n",
    "iClicker cloud join link: https://join.iclicker.com/4QVT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 7.1: Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "Suppose you are training a Vanilla RNN with one hidden layer. Your input representation is of size 200, output layer is of size 200, and you have 100 hidden units in your hidden layer. \n",
    "\n",
    "- (A) The shape of matrix $W$ or $W_{hh}$ between hidden layers in consecutive time steps is going to be $200 \\times 200$. \n",
    "- (B) The output of the hidden layer is going to be a $100$ dimensional vector.  \n",
    "- (C) In bidirectional RNNs, if we want to combine the outputs of two RNNs with pointwise addition, the hidden sizes of the two RNNs have to be the same.  \n",
    "- (D) Word2vec skipgram model is likely to suffer from the problem of vanishing gradients. \n",
    "- (E) In the forward pass, in each time step in RNNs, you calculate the output of the hidden layer by multiplying the input $x$ by the weight matrix $U$ or $W_{xh}$ and applying non-linearity. \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 7.1: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "- (A) False. It's going to be $100 times 100$ \n",
    "- (B) True\n",
    "- (C) True\n",
    "- (D) False\n",
    "- (E) False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break\n",
    "\n",
    "![](img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (Semi-optional) Long short-term memory networks (LSTMs) \n",
    "\n",
    "- Recommendation: Go through these notes once to get a general idea of LSTMs. But try not to get bogged down in all the details. \n",
    "- I want you to get the general intuition of these models so that you have a general idea of their purpose and how they work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long Short Term Memory networks (LSTMs)\n",
    "\n",
    "- [Invented in 1997](https://www.bioinf.jku.at/publications/older/2604.pdf) by Hochreiter and Schmidhuber. \n",
    "- Designed so that model can remember things for a long time (hundreds of time steps)! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple (Vanilla) RNN \n",
    "\n",
    "- In a simple RNN, we have a series of these repeating modules.\n",
    "- How does the information flow in one cell of RNN? \n",
    "\n",
    "![](img/RNN_alternative_representation.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_alternative_representation.png\" height=\"1000\" width=\"1000\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long Short Term Memory networks (LSTMs)\n",
    "\n",
    "- In an LSTM, the repeating module is more complicated. \n",
    "- LSTMs add an explicit context layer to the architecture for context management. \n",
    "- It's connected to the hiddent layer via specialized neural units which selectively control the flow of information using gates. \n",
    "\n",
    "![](img/RNN_alternative_representation.png)\n",
    "![](img/LSTM0.png)\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_alternative_representation.png\" height=\"600\" width=\"600\">  -->\n",
    "\n",
    "<!-- <img src=\"img/LSTM0.png\" height=\"700\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTMs: Controlling the flow of information using gates\n",
    "\n",
    "- LSTMs divide context management into two sub problems: removing information no longer needed from the context and adding information likely to be needed for later decision making. \n",
    "- The information is added and removed through a structure called gates. \n",
    "- The gates share a common design pattern: each consists of a feedforward layer followed by a sigmoid activation function, followed by a pointwise multiplication with the layer being gated. \n",
    "    - The sigmoid layer pushes its output to either 0 or 1, deciding how much of each component should be let through.\n",
    "    - Combining this with pointwise multiplication has the effect of a binary mask; the values that align with 1 in the mask are passed through unchanged while the values corresponding to lower values are erased.\n",
    "\n",
    "![](img/RNN_LSTM2.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_LSTM2.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "   \n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTMs: The core idea \n",
    "\n",
    "- The core idea in LSTMs is using a context or cell state (memory cell)\n",
    "- Information can flow along the memory unchanged. \n",
    "- Information can be removed or written to the cells regulated by gates. \n",
    "\n",
    "![](img/RNN_LSTM0.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_LSTM0.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does LSTM work?\n",
    "- Four operations: forget, store (input), update, output\n",
    "\n",
    "![](img/RNN_LSTM0.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_LSTM0.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forget operation\n",
    "\n",
    "- The purpose of this gate is to delete information from the context that is no longer needed. \n",
    "- A sigmoid layer, **forget gate**, decides which values of the memory cell to reset. \n",
    "- Decides what part of the history is worth forgetting.\n",
    "- $f_t = \\sigma(W_fh_{t-1} + U_fx_t + b_f)$\n",
    "\n",
    "![](img/RNN_LSTM3.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_LSTM3.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Store operation \n",
    "\n",
    "- Decides what part of the new information is worth storing. \n",
    "- Two parts: \n",
    "    - A sigmoid layer, **input gate.**\n",
    "    - $i_t = \\sigma(W_ih_{t-1} + U_ix_t + b_i)$\n",
    "\n",
    "![](img/RNN_LSTM4.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_LSTM4.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector of new candidate values\n",
    "\n",
    "- A $tanh$ layer creates a vector of new candidate values $\\tilde{c}_t$ to write to the memory cell. \n",
    "- $\\tilde{c}_t = tanh(W_{c}h_{t-1} + U_{c}x_t + b_c)$ \n",
    "\n",
    "![](img/RNN_LSTM5.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_LSTM5.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Update operation (memory cell update)\n",
    "\n",
    "- The previous steps decided which values of the memory cell to reset and overwrite. \n",
    "- Now the LSTM applies the decisions to the memory cells.\n",
    "- $c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$ \n",
    "\n",
    "![](img/RNN_LSTM6.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_LSTM6.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Output operation\n",
    "\n",
    "- The sigmoid layer, **output gate**, decides which values should be sent to the network in the next time step. \n",
    "- $o_t = \\sigma(W_{o}h_{t-1} + U_ox_t + b_o)$\n",
    "\n",
    "![](img/RNN_LSTM7.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_LSTM7.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Output update\n",
    "\n",
    "- The memory cell goes through $tanh$ and is multiplied by the output gate\n",
    "- $h_t = o_t \\odot \\tanh(c_t)$\n",
    "\n",
    "![](img/RNN_LSTM8.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_LSTM8.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- So an LSTM unit at one time step takes as input \n",
    "    - the context layer\n",
    "    - the hidden layer from previous time step\n",
    "    - the current input vector \n",
    "- As output it generates\n",
    "    - Updated context vector\n",
    "    - Updated hidden vector\n",
    "- The nice thing is that the complexity in LSTMs is encapsulated within a the basic processing unit allowing us to maintain modularity and experiment with different architectures quite easily. \n",
    "- This modularity is the key to the power and widespread applicability of LSTMs!     \n",
    "- Similar to RNNs it's possible to stack LSTM layers or use it in a bidirectional setting. \n",
    "- There are other variants such as Gated Recurrent Units (GRU) which follow similar idea but are less complicated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTMs: Confusing diagrams!!!\n",
    "\n",
    "- LSTMs are not very intuitive.  \n",
    "- Complicated combination of state in the past, observation at the moment and different ways to either forget the observation or keep it around. \n",
    "- Famous for confusing illustrative diagrams. \n",
    "\n",
    "![](img/RNN_confusing_LSTMs.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_confusing_LSTMs.png\" height=\"1000\" width=\"1000\">  -->\n",
    "<!-- </center>     -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's build LSTMs!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating some data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0929e+00, -8.1991e-01, -4.2104e-01, -9.6200e-01,  1.2825e+00,\n",
       "           8.7684e-01,  1.6221e+00, -1.4779e+00, -1.7018e+00, -7.4980e-01],\n",
       "         [-1.1285e+00,  4.1351e-01,  2.8917e-01,  2.2473e+00, -8.0364e-01,\n",
       "          -2.8084e-01, -4.2036e-01,  1.3111e+00, -2.1993e-01,  2.1895e-01],\n",
       "         [ 2.0451e-01,  5.1463e-01,  9.9376e-01, -2.5873e-01,  1.5118e-01,\n",
       "           1.0364e-01, -2.1996e+00, -8.8490e-02, -5.6120e-01,  6.7155e-01],\n",
       "         [ 6.9330e-01, -9.4872e-01, -1.1440e+00,  2.4362e-01, -5.6734e-02,\n",
       "           3.7841e-01,  1.6863e+00,  2.5529e-01, -5.4963e-01,  1.0042e+00]],\n",
       "\n",
       "        [[ 3.5068e-01,  1.5434e+00,  1.4058e-01,  1.0617e+00, -9.9292e-01,\n",
       "          -1.6025e+00, -1.0764e+00,  9.0315e-01, -1.6461e+00,  1.0720e+00],\n",
       "         [ 1.5026e+00, -8.1899e-01,  2.6860e-01, -2.2150e+00, -1.3193e+00,\n",
       "          -2.0915e+00,  1.2767e+00, -9.9480e-01,  1.2176e+00, -2.2817e-01],\n",
       "         [ 1.3382e+00,  1.9929e+00,  1.3708e+00, -5.0087e-01, -2.3244e+00,\n",
       "           1.2311e+00, -1.0973e+00, -9.6690e-01,  6.7125e-01, -9.4053e-01],\n",
       "         [-4.6806e-01,  1.0322e+00, -8.9568e-01,  1.1124e+00, -4.1684e-01,\n",
       "          -1.7106e+00, -3.2902e-01,  1.3966e+00, -9.9491e-01, -1.5822e-03]],\n",
       "\n",
       "        [[-5.5601e-01, -2.7202e+00,  5.4215e-01, -1.1541e+00,  7.7631e-01,\n",
       "          -2.5822e-01, -2.0407e+00, -8.0156e-01, -6.0270e-01, -4.7965e-02],\n",
       "         [ 5.3490e-01,  1.1031e+00,  1.3334e+00, -1.4053e+00, -5.9217e-01,\n",
       "          -2.5479e-01, -8.9886e-01,  8.1377e-01,  6.5323e-01,  6.5572e-01],\n",
       "         [-1.4056e+00, -1.2743e+00,  4.5128e-01, -2.2801e-01,  1.6014e+00,\n",
       "          -2.2577e+00, -1.8009e+00,  7.0147e-01,  5.7028e-01,  1.8790e+00],\n",
       "         [-9.1925e-01,  1.0318e+00,  1.4353e+00,  8.8307e-02, -1.2037e+00,\n",
       "           1.0964e+00,  2.4210e+00,  2.4489e-01,  1.8118e+00, -4.4241e-01]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequence_len, batch_size, input_size\n",
    "X = torch.randn(3, 4, 10)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining the LSTM module \n",
    "An LSTM is defined by `torch.nn.LSTM` module with the following parameters. \n",
    "- `input_size`: the number of features in x (e.g., size of the word embedding)\n",
    "- `hidden_size`: the number of nodes in the hidden layer\n",
    "- `num_layers`: number of recurrent layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_rnn = nn.LSTM(input_size=10, hidden_size=5, num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Initializing the cell state and hidden state at time step 0\n",
    "\n",
    "- first dimension: number of LSTM layers\n",
    "- second dimension: batch_size\n",
    "- third dimension: hidden_size / number of nodes in a hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "c0 = torch.randn(2, 4, 5)\n",
    "h0 = torch.randn(2, 4, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward propagation of LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike RNN, LSTM module takes three inputs \n",
    "- the initial hidden state for each element in the batch (t=0)\n",
    "- initial cell state for each element in the batch     \n",
    "- the input features (e.g., embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h0 and c0 is optional input, defaults to tensor of 0's when not provided\n",
    "output, (hn, cn) = lstm_rnn(X, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  torch.Size([3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# output = seq_len, batch_size, hidden_size (output features from last layer of LSTM)\n",
    "print(\"output size: \", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### This was not that that complicated but when we want to use it on text data it's a quite involved process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text classification using `PyTorch` and `torchtext` \n",
    "\n",
    "- A lot of work in building LSTMs for text data goes into data preprocessing and getting the text into the suitable format. \n",
    "- There are a number of steps involved in data preprocessing. \n",
    "    - [ ] Data splitting (train, valid, test)\n",
    "    - [ ] Loading the data files\n",
    "    - [ ] Tokenization\n",
    "    - [ ] Creating a vocabulary: Creating a list of unique words \n",
    "    - [ ] Numericalization: Converting text to a set of integers. \n",
    "    - [ ] Word vectors\n",
    "    - [ ] Embedding lookup \n",
    "    - [ ] Batching   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text preprocessing\n",
    "\n",
    "![](img/lstm-preprocess.png)\n",
    "\n",
    "<!-- <img src=\"img/lstm-preprocess.png\" height=\"800\" width=\"800\">  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `batch_first=True`\n",
    "\n",
    "![](img/batch_first_true.png)\n",
    "<!-- <img src=\"img/batch_first_true.png\" height=\"800\" width=\"800\">  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `batch_first=False` (default)\n",
    "\n",
    "![](img/batch_first_false.png)\n",
    "<!-- <img src=\"img/batch_first_false.png\" height=\"800\" width=\"800\">  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [`torchtext`](https://pytorch.org/text/stable/index.html)\n",
    "- There is a library called `torchtext` to help us with these tasks. \n",
    "- To use it You will first need to [install `torchtext`](https://pypi.org/project/torchtext/) in the environment.\n",
    "\n",
    "```\n",
    "pip install torchtext\n",
    "```\n",
    "\n",
    "- I've included an adapted version of [`torchtext` tutorial](AppendixB-torchtext-tutorial.ipynb) from MDS-CL as an appendix. \n",
    "- You can also included a demo of [text classification using LSTMs](AppendixC-LSTM-spam-classification.ipynb) as an appendix.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs vs. LSTMs \n",
    "\n",
    "- RNNs suffer from the problem of vanishing gradients. \n",
    "    - Vanishing gradients lead to difficulty in training.  \n",
    "- We saw that LSTMs mitigate this problem by introducing a cell state and managing the context better. \n",
    "- They are better suited for sequence modeling and capturing long-distance dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/RNN_alternative_representation.png)\n",
    "![](img/LSTM0.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN_alternative_representation.png\" height=\"400\" width=\"400\">  -->\n",
    "\n",
    "<!-- <img src=\"img/LSTM0.png\" height=\"400\" width=\"400\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are tons of applications of LSTMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wiki page on LSTM applications \n",
    "\n",
    "![](img/lstm_applications_wiki.png)\n",
    "\n",
    "<!-- <img src=\"img/lstm_applications_wiki.png\" width=\"1500\" height=\"1500\"> -->\n",
    "\n",
    "(Credit: [Learning to execute](https://arxiv.org/pdf/1410.4615.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM for image captioning \n",
    "\n",
    "![](img/RNN_LSTM_image_captioning.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_LSTM_image_captioning.png\" height=\"2000\" width=\"2000\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "\n",
    "(Credit: [LSTMs for image captioning](https://arxiv.org/pdf/1411.4555.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### music2dance\n",
    "\n",
    "- [Dance with Melody: An LSTM-autoencoder Approach to Music-oriented Dance Synthesis](https://hcsi.cs.tsinghua.edu.cn/Paper/Paper18/MM18-TANGTAORAN.pdf)\n",
    "- [Follow the Music: Dance Motion Synthesis Corresponding to Arbitrary Music](https://www.cs.ubc.ca/~rozentil/data/music2dance.pdf) (UBC researchers)\n",
    "\n",
    "![](img/music2dance.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/music2dance.png\" height=\"1500\" width=\"1500\">  -->\n",
    "<!-- </center>     -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you\n",
    "\n",
    "iClicker cloud join link: https://join.iclicker.com/4QVT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 8.1: Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) Input to an RNN or LSTM is a 3-D tensor with three components: sequence length, batch size, and input size.\n",
    "- (B) In the forward pass of LSTMs, in addition to the previous hidden state we also pass the previous cell state.\n",
    "- (C) In LSTMs, creating batches with sequences of similar lengths might minimize padding.\n",
    "- (D) HMMs can handle variable length sequences but LSTMs cannot.   \n",
    "- (E) It's almost always a good idea to use LSTMs to model text data over bag-of-words models, as they encode sequential information and are likely to beat bag-of-words models. \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 8.1: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "- (A) True\n",
    "- (B) True\n",
    "- (C) True\n",
    "- (D) False\n",
    "- (E) False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.2: Questions for discussion\n",
    "\n",
    "1. Suppose you are using pre-trained embeddings to train a sentiment analysis model with LSTMs on a small corpus shown below. Now given a test sentence \"amazing\", would you expect the model to classify the sentence correctly? Why or why not? State your assumptions.  \n",
    "\n",
    "| text        | sentiment           |\n",
    "| ------------- |:-------------:|\n",
    "| what a wonderful movie ! | positive|\n",
    "| boring ... fell asleep a couple of times | negative |\n",
    "| astonishing ! ! | positive |\n",
    "\n",
    "2. In text generation using LSTMs or RNNs, why it might be a good idea to sample from the distribution rather than picking the next word with highest probability? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 8.2: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "1.  Most likely yes. Although the word **amazing** does not occur in the training corpus, we would be incorporating the information in the model, via pre-trained word embeddings, that the word amazing is similar to wonderful and astonishing which are associated with positive sentiment.\n",
    "2. To add novelty.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final comments and summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important ideas to know\n",
    "\n",
    "- RNNS and LSTMs are well suited for sequence modeling tasks. \n",
    "- They are widely used models to process sequential data in deep learning community and have a wide range of applications. \n",
    "- Training RNNs is hard because of vanishing and exploding gradients.  \n",
    "- LSTMs mitigate the problem by introducing a mechanism to selectively control the flow of information in the network.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important ideas to know\n",
    "\n",
    "- LSTMs use **gates to control the flow of information**. \n",
    "- They maintain **a separate cell or context state** in addition to the hidden state. \n",
    "- Maintaining cell or context state allows for efficient training with back-propagation through time.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important ideas to know\n",
    "\n",
    "- The shapes of weight matrices ($U, V, W$) in vanilla RNNs.\n",
    "- Forward pass of RNNs and LSTMs.  \n",
    "- The shapes of input, output, and hidden and cell states of LSTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important ideas to know\n",
    "\n",
    "- If you want to use LSTMs for text data a number of steps are involved before passing text data to LSTMs including\n",
    "    - tokenization\n",
    "    - embedding lookup \n",
    "    - numericalization\n",
    "    - batching \n",
    "    - padding \n",
    "- We'll use a library called [`torchtext`](https://pytorch.org/text/stable/index.html) to help us with some of the preprocessing tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN resources\n",
    "\n",
    "A lot of material is available on the web. Here are some resources that were useful for me. \n",
    "\n",
    "\n",
    "- [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [An illustration of LSTMs with nice visualizations](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "- Geoff Hinton's [short talk](https://www.youtube.com/watch?v=93rzMHtYT_0) and [lecture](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec7.pdf) on LSTMs\n",
    "- [Yoshua Bengio's lecture](https://www.youtube.com/watch?v=AYku9C9XoB8&t=884s)\n",
    "- [Ali Ghodsi's lecture on RNNs](https://www.youtube.com/results?search_query=ali+ghodsi+RNNs)\n",
    "- Richard Socher's [slides](https://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf) and [lecture](https://www.youtube.com/watch?v=Keqep_PKrY8) on RNNs\n",
    "- [A list of RNN resources](https://github.com/ajhalthor/awesome-rnn)\n",
    "- [MIT 6.S191 (2020): Recurrent Neural Networks](https://www.youtube.com/watch?v=SEnXr6v2ifU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:563]",
   "language": "python",
   "name": "conda-env-563-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
