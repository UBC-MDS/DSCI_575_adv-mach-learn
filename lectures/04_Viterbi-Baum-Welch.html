

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Lecture 4: Decoding and Learning in HMMs &#8212; DSCI 575 Advanced Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/04_Viterbi-Baum-Welch';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture 5: Topic Modeling" href="05_topic-modeling.html" />
    <link rel="prev" title="Lecture 3: Introduction to Hidden Markov Models (HMMs)" href="03_HMMs-intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/mds-hex-sticker.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/mds-hex-sticker.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting ready</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="00_course-information.html">Course Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_Markov-models.html">Lecture 1: Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_LMs-text-preprocessing.html">Lecture 2: Applications of Markov Models  and Text Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_HMMs-intro.html">Lecture 3: Introduction to Hidden Markov Models (HMMs)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Lecture 4: Decoding and Learning in HMMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_topic-modeling.html">Lecture 5: Topic Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_intro-to-RNNs.html">Lecture 6: Introduction to Recurrent Neural Networks (RNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_intro-to-transformers.html">Lecture 7: Introduction to self-attention and transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_more-transformers.html">Lecture 8: More transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Class demos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="class_demos/text-generation.html">Class Demo: Recipe generator</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="AppendixA-PageRank.html">PageRank as a Markov model</a></li>
<li class="toctree-l1"><a class="reference internal" href="AppendixB-HMM-POS.html">HMM supervised POS tagging</a></li>
<li class="toctree-l1"><a class="reference internal" href="AppendixC-BaumWelch.html">Baum-Welch (BW) algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="AppendixD-LDA-details.html">LDA details</a></li>
<li class="toctree-l1"><a class="reference internal" href="AppendixE-LSTMs.html">AppendixD: More RNNs, LSTMs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Attribution</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../attribution.html">Attributions</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/UBC-MDS/DSCI_575_adv-mach-learn" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/04_Viterbi-Baum-Welch.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture 4: Decoding and Learning in HMMs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-plan-imports-lo">Lecture plan, imports, LO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recap">Recap</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hmm-ingredients">HMM ingredients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recap-the-forward-algorithm">Recap: The forward algorithm</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-the-viterbi-algorithm">1. Decoding: The Viterbi algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1.1 Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#viterbi-initialization">1.2 Viterbi: Initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#viterbi-induction">1.3 Viterbi: Induction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#viterbi-conclusion">1.4 Viterbi conclusion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#viterbi-with-hmmlearn-on-our-toy-hmm">1.5 Viterbi with  <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code> on our toy HMM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">â“â“ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-1-select-all-of-the-following-statements-which-are-true-iclicker">Exercise 4.1: Select all of the following statements which are <strong>True</strong> (iClicker)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-algorithm">2. The backward algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.1 Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-algorithm-intuition">2.2 The backward algorithm intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-algorithm-initialization-beta-3-and-beta-3">2.3 The backward algorithm: Initialization <span class="math notranslate nohighlight">\(\beta_ğŸ™‚(3)\)</span> and <span class="math notranslate nohighlight">\(\beta_ğŸ˜”(3)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-algorithm-induction">2.4 The backward algorithm: Induction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-algorithm-conclusion">2.5 The backward algorithm: Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#baum-welch-bw-algorithm-high-level-idea">3. Baum-Welch (BW) algorithm (high-level idea)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">3.1 Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-unsupervised-approach">3.2 Iterative unsupervised approach</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupervised-learning-of-our-toy-problem">3.3 Unsupervised learning of our toy problem</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-comments-and-summary">Final comments and summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#three-fundamental-questions-for-hmms">Three fundamental questions for HMMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-ideas-to-know">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-hmms">Continuous HMMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-useful-resources-and-links">Some useful resources and links</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">â“â“ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-exercise-4-2-more-practice-questions">(Optional) Exercise 4.2: More practice questions</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><img alt="" src="../_images/575_banner.png" /></p>
<section class="tex2jax_ignore mathjax_ignore" id="lecture-4-decoding-and-learning-in-hmms">
<h1>Lecture 4: Decoding and Learning in HMMs<a class="headerlink" href="#lecture-4-decoding-and-learning-in-hmms" title="Permalink to this heading">#</a></h1>
<p>UBC Master of Data Science program, 2023-24</p>
<p>Instructor: Varada Kolhatkar</p>
<section id="lecture-plan-imports-lo">
<h2>Lecture plan, imports, LO<a class="headerlink" href="#lecture-plan-imports-lo" title="Permalink to this heading">#</a></h2>
<p><br><br></p>
<section id="imports">
<h3>Imports<a class="headerlink" href="#imports" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os.path</span>
<span class="kn">import</span> <span class="nn">string</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">),</span> <span class="s2">&quot;code&quot;</span><span class="p">))</span>
<span class="kn">from</span> <span class="nn">plotting_functions</span> <span class="kn">import</span> <span class="o">*</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.
Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.
</pre></div>
</div>
</div>
</div>
<p><br><br></p>
</section>
</section>
<section id="learning-outcomes">
<h2>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Permalink to this heading">#</a></h2>
<p>From this lesson you will be able to</p>
<ul class="simple">
<li><p>Explain the general idea and purpose of the Viterbi algorithm.</p></li>
<li><p>Explain the three steps in the Viterbi algorithm and apply it given an HMM and an observation sequence.</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(\delta_i(t)\)</span> and <span class="math notranslate nohighlight">\(\psi_i(t)\)</span> for a given state <span class="math notranslate nohighlight">\(i\)</span> at time step <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p>Explain the general idea of the backward algorithm.</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(\beta_i(t)\)</span> for a given state <span class="math notranslate nohighlight">\(i\)</span> at time step <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p>Explain the purpose and the general idea of the Baum-Welch algorithm.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code> for liklihood, decoding, and HMM unsupervised training.</p></li>
</ul>
<p><br><br><br><br></p>
</section>
<section id="recap">
<h2>Recap<a class="headerlink" href="#recap" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Hidden Markov models (HMMs) model sequential data with latent factors.</p></li>
<li><p>There are tons of applications associated with them and they are more realistic than Markov models.</p></li>
</ul>
<p><img alt="" src="../_images/HMM_example_small.png" /></p>
<!-- <img src="img/HMM_example.png" height="500" width="500"> --><section id="hmm-ingredients">
<h3>HMM ingredients<a class="headerlink" href="#hmm-ingredients" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Hidden states (e.g., Happy, Sad)</p></li>
<li><p>Output alphabet or output symbols or observations (e.g., learn, study, cry, facebook)</p></li>
<li><p>Discrete initial state probability distribution</p></li>
<li><p>Transition probabilities</p></li>
<li><p>Emission probabilities</p></li>
</ul>
<p>The three fundamental questions for an HMM.</p>
<ul class="simple">
<li><p><strong>Likelihood</strong>
Given a model with parameters <span class="math notranslate nohighlight">\(\theta = &lt;\pi, A, B&gt;\)</span>, how do we efficiently compute the likelihood of a particular observation sequence <span class="math notranslate nohighlight">\(O\)</span>?</p></li>
<li><p><strong>Decoding</strong>
Given an observation sequence <span class="math notranslate nohighlight">\(O\)</span> and a model <span class="math notranslate nohighlight">\(\theta\)</span> how do we choose a state sequence <span class="math notranslate nohighlight">\(Q={q_0, q_1, \dots q_T}\)</span> that best explains the observation sequence?</p></li>
<li><p><strong>Learning</strong>
Training: Given a large observation sequence <span class="math notranslate nohighlight">\(O\)</span> how do we choose the best parameters <span class="math notranslate nohighlight">\(\theta\)</span> that explain the data <span class="math notranslate nohighlight">\(O\)</span>?</p></li>
</ul>
</section>
<section id="recap-the-forward-algorithm">
<h3>Recap: The forward algorithm<a class="headerlink" href="#recap-the-forward-algorithm" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The forward algorithm is a dynamic programming algorithm to efficiently estimate the probability of an observation sequence <span class="math notranslate nohighlight">\(P(O;\theta)\)</span> given an HMM.</p></li>
<li><p>For each state <span class="math notranslate nohighlight">\(i\)</span>, we calculated <span class="math notranslate nohighlight">\(\alpha_i(0), \alpha_i(1), \alpha_i(2), ...\alpha_i(t)\)</span>, which represent the probabilities of being in state <span class="math notranslate nohighlight">\(i\)</span> at times <span class="math notranslate nohighlight">\(t\)</span> knowing all the observations which came before and at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p>The trellis was computed left to right and top to bottom.</p></li>
<li><p>The forward algorithm stores the probabilities of all possible 1-state sequences (from the start), to store all possible 2-state sequences (from the start), to store all possible 3-state sequences (from the start) and so on.</p></li>
</ul>
<p><img alt="" src="lectures/img/hmm_alpha_values_small.png" /></p>
<ul class="simple">
<li><p>Sum over all possible final states:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(P(O;\theta) = \sum\limits_{i=1}^{n}\alpha_i(T-1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(E,L,F,C) = \alpha_ğŸ™‚(3) + \alpha_ğŸ˜”(3) = 0.00023 + 0.00207 = 0.0023\)</span></p></li>
</ul>
</li>
</ul>
<!-- <center> -->
<!-- <img src="img/hmm_alpha_values.png" height="500" width="500">  -->
<!-- </center> --><p>Recall the three fundamental questions for an HMM.</p>
<ul class="simple">
<li><p><strong>Likelihood</strong>
Given a model with parameters <span class="math notranslate nohighlight">\(\theta = &lt;\pi, A, B&gt;\)</span>, how do we efficiently compute the likelihood of a particular observation sequence <span class="math notranslate nohighlight">\(O\)</span>?</p></li>
<li><p><strong>Decoding</strong>
Given an observation sequence <span class="math notranslate nohighlight">\(O\)</span> and a model <span class="math notranslate nohighlight">\(\theta\)</span> how do we choose a state sequence <span class="math notranslate nohighlight">\(Q={q_0, q_1, \dots q_T}\)</span> that best explains the observation sequence?</p></li>
<li><p><strong>Learning</strong>
Training: Given a large observation sequence <span class="math notranslate nohighlight">\(O\)</span> how do we choose the best parameters <span class="math notranslate nohighlight">\(\theta\)</span> that explain the data <span class="math notranslate nohighlight">\(O\)</span>?</p></li>
</ul>
<p><br><br><br><br></p>
</section>
</section>
<section id="decoding-the-viterbi-algorithm">
<h2>1. Decoding: The Viterbi algorithm<a class="headerlink" href="#decoding-the-viterbi-algorithm" title="Permalink to this heading">#</a></h2>
<section id="introduction">
<h3>1.1 Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Given an observation sequence <span class="math notranslate nohighlight">\(O\)</span> and a model <span class="math notranslate nohighlight">\(\theta\)</span> how do we choose a state sequence <span class="math notranslate nohighlight">\(Q={q_0, q_1, \dots q_T}\)</span> that best explains the observation sequence?</p></li>
<li><p>Purpose: finding whatâ€™s most likely going on under the hood.</p></li>
<li><p>For example: It tells us the most likely part-of-speech tags given an English sentence.</p></li>
</ul>
<blockquote>
Will/MD the/DT chair/NN chair/VB the/DT meeting/NN from/IN that/DT chair/NN?
</blockquote>    <p>More formally,</p>
<ul class="simple">
<li><p>Given an HMM, choose the state sequence that maximizes the probability of the output sequence.</p></li>
</ul>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q^* = \arg \max\limits_Q P(O,Q;\theta)\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(P(O,Q;\theta) = \pi_{q_0}b_{q_0}(o_0) \prod\limits_{t=1}^{T}a_{q_{t-1}}a_{q_t}b_{q_t}(o_t)\)</span></p></li>
</ul>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="700" width="700">  -->
<!-- </center> --><p><strong>Can we use the forward algorithm for decoding?</strong></p>
<p>If we want to pick an optimal state sequence which maximizes the probability of the observation sequence, how about picking the state with maximum <span class="math notranslate nohighlight">\(\alpha\)</span> value at each time step?</p>
<p><img alt="" src="lectures/img/hmm_alpha_values_small.png" /></p>
<!-- <center> -->
<!-- <img src="img/hmm_alpha_values.png" height="500" width="500">  -->
<!-- </center> -->
<p>If we pick the most probable state at each time step based on the <span class="math notranslate nohighlight">\(\alpha\)</span> values, it might not end up as the best state sequence because it might be possible that the transition between two highly probable states in a sequence is very unlikely.</p>
<p>We need something else.</p>
<p><br><br></p>
<p><strong>The Viterbi algorithm: Overview</strong></p>
<ul class="simple">
<li><p>Dynamic programming algorithm.</p></li>
<li><p>We use a different kind of trellis.</p></li>
<li><p>Want: Given an HMM, choose the state sequence that maximizes the probability of the output sequence.</p></li>
</ul>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q^* = \arg \max\limits_Q P(O,Q;\theta)\)</span></p></li>
</ul>
<ul class="simple">
<li><p>We store <span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\psi\)</span> values at each node in the trellis</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_i(t)\)</span> represents the probability of the most probable path leading to the trellis node at state <span class="math notranslate nohighlight">\(i\)</span> and time <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\psi_i(t)\)</span> represents the best possible previous state if I am in state <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ul>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="700" width="700">  -->
<!-- </center> --><p>Letâ€™s go through the algorithm step by step.</p>
<p><br><br></p>
</section>
<section id="viterbi-initialization">
<h3>1.2 Viterbi: Initialization<a class="headerlink" href="#viterbi-initialization" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Initialize with <span class="math notranslate nohighlight">\(\delta_i(0) = \pi_i b_i(o_0)\)</span> for all states</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\delta_ğŸ™‚(0) = \pi_ğŸ™‚ b_ğŸ™‚(E) = 0.8 \times 0.2 = 0.16\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_ğŸ˜”(0) = \pi_ğŸ˜” b_ğŸ˜”(E) = 0.2 \times 0.1 = 0.02\)</span></p></li>
</ul>
</li>
<li><p>Initialize with <span class="math notranslate nohighlight">\(\psi_i(0) = 0 \)</span>, for all states</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\psi_ğŸ™‚(0) = 0, \psi_ğŸ˜”(0) = 0\)</span></p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="700" width="700">  -->
<!-- </center>     --><p><br><br></p>
</section>
<section id="viterbi-induction">
<h3>1.3 Viterbi: Induction<a class="headerlink" href="#viterbi-induction" title="Permalink to this heading">#</a></h3>
<p>The best path <span class="math notranslate nohighlight">\(\delta_t\)</span> to state <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> depends on the best path to each
possible previous state <span class="math notranslate nohighlight">\(\delta_i(t-1)\)</span> and their transitions to <span class="math notranslate nohighlight">\(j\)</span> (<span class="math notranslate nohighlight">\(a_{ij}\)</span>).</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta_j(t) = \max\limits_i \{\delta_i(t-1)a_{ij}\} b_j(o_t)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\psi_j(t) = \arg \max\limits_i \{\delta_i(t-1)a_{ij}\} \)</span></p></li>
</ul>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<p><img alt="" src="../_images/viterbi_explanation_small.png" /></p>
<!-- <center> -->
<!-- <img src="img/viterbi_explanation.png" height="150" width="150">  -->
<!-- </center> -->
<ul class="simple">
<li><p>There are two possible paths to state ğŸ™‚ at <span class="math notranslate nohighlight">\(T = 1\)</span>. Which is the best one?</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_ğŸ™‚(1) = \max \begin{Bmatrix} \delta_ğŸ™‚(0) \times a_{ğŸ™‚ğŸ™‚},\\ \delta_ğŸ˜”(0) \times a_{ğŸ˜”ğŸ™‚}\end{Bmatrix}  \times b_ğŸ™‚(L)\)</span></p></li>
<li><p>First take the max between <span class="math notranslate nohighlight">\(\delta_ğŸ™‚(0) \times a_{ğŸ™‚ğŸ™‚}\)</span> and <span class="math notranslate nohighlight">\(\delta_ğŸ˜”(0) \times a_{ğŸ˜”ğŸ™‚}\)</span> and then multiply the max by <span class="math notranslate nohighlight">\(b_ğŸ™‚(L)\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\psi_ğŸ™‚(1)\)</span> = the state at <span class="math notranslate nohighlight">\(T=0\)</span> from where the path to ğŸ™‚ at <span class="math notranslate nohighlight">\(T=1\)</span> was the best one.</p></li>
<li><p><strong>Note that we use parentheses to show two quantities for taking the max. (Not the best notation but I have seen it being used in this context.)</strong></p></li>
</ul>
<p><br><br></p>
<p><strong>Viterbi: Induction (T = 1)</strong></p>
<p><span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\psi\)</span> at state ğŸ™‚ and T = 1</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta_{ğŸ™‚}(1) = \max\limits_i \{\delta_i(0)a_{ij}\} b_j(o_t) = 
\max \begin{Bmatrix} 0.16 \times 0.7, \\ 0.02 \times 0.4\end{Bmatrix} \times 0.7 = 0.0784\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\psi_{ğŸ™‚}(1) = \arg \max\limits_i \{\delta_i(0)a_{ij}\} = ğŸ™‚\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\psi\)</span> at state ğŸ˜” and T = 1</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta_{ğŸ˜”}(1) = \max\limits_i \{\delta_i(0)a_{ij}\} b_j(o_t) =  \max \begin{Bmatrix} 0.16 \times 0.3, \\ 0.02 \times 0.6\end{Bmatrix} \times 0.1 = 0.0048\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\psi_{ğŸ˜”}(1) = \arg \max\limits_i \{\delta_i(0)a_{ij}\} = ğŸ™‚\)</span></p></li>
</ul>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="800" width="800">  -->
<!-- </center>     --><p><strong>Viterbi: Induction (T = 1)</strong></p>
<p><span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\psi\)</span> at state ğŸ™‚ and T = 1</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta_{ğŸ™‚}(1) = \max \begin{Bmatrix} \delta_ğŸ™‚(0) \times a_{ğŸ™‚ğŸ™‚}, \\ \delta_ğŸ˜”(0) \times a_{ğŸ˜”ğŸ™‚}\end{Bmatrix}  \times b_ğŸ™‚(L) = 
\max \begin{Bmatrix} 0.16 \times 0.7, \\ 0.02 \times 0.4\end{Bmatrix} \times 0.7 = 0.0784\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\psi_{ğŸ™‚}(1)  = ğŸ™‚\)</span></p></li>
</ul>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="800" width="800">  -->
<!-- </center>     --><p><strong>Viterbi: Induction (T = 1)</strong></p>
<p><span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\psi\)</span> at state ğŸ˜” and T = 1</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta_{ğŸ˜”}(1) = \max\limits_i \{\delta_i(0)a_{ij}\} b_j(o_t) =  \max \begin{Bmatrix} 0.16 \times 0.3 ,\\ 0.02 \times 0.6\end{Bmatrix} \times 0.1 = 0.0048\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\psi_{ğŸ˜”}(1) = \arg \max\limits_i \{\delta_i(0)a_{ij}\} = ğŸ™‚\)</span></p></li>
</ul>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="800" width="800">  -->
<!-- </center>     --><p><strong>Viterbi: Induction (T = 2)</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\psi\)</span> at state ğŸ™‚ and T = 2</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\delta_{ğŸ™‚}(2) = \max\limits_i \{\delta_i(1)a_{ij}\} b_j(o_t) =  \max \begin{Bmatrix} 0.784 \times 0.7, \\ 0.0048 \times 0.4 \end{Bmatrix}\times 0 = 0
\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\psi_{ğŸ™‚}(2) = \arg \max\limits_i \{\delta_i(1)a_{ij}\} = ğŸ™‚\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\psi\)</span> at state ğŸ˜” and T = 2</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\delta_{ğŸ˜”}(2) = \max\limits_i \{\delta_i(1)a_{ij}\} b_j(o_t) =  \max \begin{Bmatrix} 0.784 \times 0.3, \\ 0.0048 \times 0.6 \end{Bmatrix}\times 0.2 = 4.704 \times 10^{-3}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\psi_{ğŸ˜”}(2) = \arg \max\limits_i \{\delta_i(1)a_{ij}\} = ğŸ™‚\)</span></p></li>
</ul>
</li>
</ul>
<br>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="400" width="400">  -->
<!-- </center>     --><p><strong>Viterbi: Induction (T = 3)</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\psi\)</span> at state ğŸ™‚ and T = 3</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\delta_{ğŸ™‚}(3) = \max\limits_i \{\delta_i(2)a_{ij}\} b_j(o_t) = \max \begin{Bmatrix} 0 \times 0.7, \\ 4.704 \times 10^{-3} \times 0.4 \end{Bmatrix} \times 0.1 = 1.88\times10^{-4}
\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\psi_{ğŸ™‚}(3) = \arg \max\limits_i \{\delta_i(2)a_{ij}\} = ğŸ˜”\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\psi\)</span> at state ğŸ˜” and T = 3</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\delta_{ğŸ˜”}(3) = \max\limits_i \{\delta_i(2)a_{ij}\} b_j(o_t) = \max \begin{Bmatrix} 0 \times 0.3, \\ 4.704 \times 10^{-3} \times 0.6 \end{Bmatrix} \times 0.6 = 1.69 \times 10^{-3}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\psi_{ğŸ˜”}(3) = \arg \max\limits_i \{\delta_i(2)a_{ij}\} = ğŸ˜”\)</span></p></li>
</ul>
</li>
</ul>
<br>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="400" width="400">  -->
<!-- </center> --></section>
<section id="viterbi-conclusion">
<h3>1.4 Viterbi conclusion<a class="headerlink" href="#viterbi-conclusion" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Choose the best final state: <span class="math notranslate nohighlight">\(q_t^* = \arg \max\limits_i \delta_i(t)\)</span></p></li>
<li><p>Recursively choose the best previous state: <span class="math notranslate nohighlight">\(q_{t-1}^* = \psi_{q_t^*}(t)\)</span></p>
<ul>
<li><p>The most likely state sequence for the observation sequence ELFC is ğŸ™‚ğŸ™‚ğŸ˜”ğŸ˜”.</p></li>
</ul>
</li>
<li><p>The probability of the state sequence is the probability of <span class="math notranslate nohighlight">\(q_t^*\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(P(ğŸ™‚ğŸ™‚ğŸ˜”ğŸ˜”) = 1.69 \times 10^{-3}\)</span></p></li>
</ul>
</li>
</ul>
<p><img alt="" src="lectures/img/HMM_viterbi_conclusion_small.png" /></p>
<!-- <img src="img/HMM_viterbi_conclusion.png" height="600" width="600">  --><p><br><br></p>
</section>
<section id="viterbi-with-hmmlearn-on-our-toy-hmm">
<h3>1.5 Viterbi with <a class="reference external" href="https://hmmlearn.readthedocs.io"> <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code></a> on our toy HMM<a class="headerlink" href="#viterbi-with-hmmlearn-on-our-toy-hmm" title="Permalink to this heading">#</a></h3>
<p><img alt="" src="../_images/HMM_example_small.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example.png" height="500" width="500">  -->
<!-- </center>     --><p>Letâ€™s get the optimal state sequence using Viterbi in our toy example.</p>
<ul class="simple">
<li><p>We assume that we already have the model, i.e., transition probabilities, emission probabilities, and initial state probabilities.</p></li>
<li><p>Our goal is to efficiently find the best state sequence for the given observation sequence.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">hmmlearn</span> <span class="kn">import</span> <span class="n">hmm</span>

<span class="c1"># Initializing an HMM</span>
<span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Happy&quot;</span><span class="p">,</span> <span class="s2">&quot;Sad&quot;</span><span class="p">]</span>
<span class="n">n_states</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>

<span class="n">observations</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Learn&quot;</span><span class="p">,</span> <span class="s2">&quot;Eat&quot;</span><span class="p">,</span> <span class="s2">&quot;Cry&quot;</span><span class="p">,</span> <span class="s2">&quot;Facebook&quot;</span><span class="p">]</span>
<span class="n">n_observations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>

<span class="c1"># Since we&#39;ve discrete observations, we&#39;ll use `CategoricalHMM`</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">CategoricalHMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_states</span><span class="p">)</span>

<span class="c1"># Set the initial state probabilities</span>
<span class="n">model</span><span class="o">.</span><span class="n">startprob_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>

<span class="c1"># Set the transition matrix</span>
<span class="n">model</span><span class="o">.</span><span class="n">transmat_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]])</span>

<span class="c1"># Set the emission probabilities of shape (n_components, n_symbols)</span>
<span class="n">model</span><span class="o">.</span><span class="n">emissionprob_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_state_seq</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">observation_seq</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="n">states</span><span class="p">,</span> <span class="n">symbols</span><span class="o">=</span><span class="n">observations</span><span class="p">):</span>
    <span class="n">logprob</span><span class="p">,</span> <span class="n">state_seq</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">observation_seq</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;viterbi&quot;</span><span class="p">)</span>
    <span class="n">o_seq</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">symbols</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">observation_seq</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">s_seq</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">state_seq</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;log probability of state sequence: &quot;</span><span class="p">,</span> <span class="n">logprob</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">s_seq</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">o_seq</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;state sequence&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Decoding example</span>
<span class="n">toy_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
<span class="n">get_state_seq</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">toy_seq</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>log probability of state sequence:  -6.3809933159177925
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>state sequence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Eat</th>
      <td>Happy</td>
    </tr>
    <tr>
      <th>Learn</th>
      <td>Happy</td>
    </tr>
    <tr>
      <th>Facebook</th>
      <td>Sad</td>
    </tr>
    <tr>
      <th>Cry</th>
      <td>Sad</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>This is how you find the best state sequence that explains the observation sequence using the Viterbi algorithm!</p></li>
<li><p>Much faster than the brute force approach of considering all possible state combinations, calculating probabilities for each of them and taking the one resulting in maximum probability.</p></li>
</ul>
<p><br><br><br><br></p>
</section>
</section>
<section id="questions-for-you">
<h2>â“â“ Questions for you<a class="headerlink" href="#questions-for-you" title="Permalink to this heading">#</a></h2>
<section id="exercise-4-1-select-all-of-the-following-statements-which-are-true-iclicker">
<h3>Exercise 4.1: Select all of the following statements which are <strong>True</strong> (iClicker)<a class="headerlink" href="#exercise-4-1-select-all-of-the-following-statements-which-are-true-iclicker" title="Permalink to this heading">#</a></h3>
<p><strong>iClicker join link: https://join.iclicker.com/ZTLY</strong></p>
<ul class="simple">
<li><p>(A) In Viterbi, <span class="math notranslate nohighlight">\(\delta_i(t)\)</span> is the probability of the best path (i.e., the path with highest probability) which accounts for the first <span class="math notranslate nohighlight">\(t\)</span> observations and ending at state <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p>(B) In Viterbi, suppose at <span class="math notranslate nohighlight">\(t-1\)</span>, state <span class="math notranslate nohighlight">\(i\)</span> has the highest <span class="math notranslate nohighlight">\(\delta_i(t-1)\)</span> among all states. Then at time step <span class="math notranslate nohighlight">\(t\)</span>, the path from <span class="math notranslate nohighlight">\(i\)</span> at <span class="math notranslate nohighlight">\(t-1\)</span> is going to give us the highest <span class="math notranslate nohighlight">\(\delta_j(t)\)</span> for all states <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p>(C) In Viterbi, the <span class="math notranslate nohighlight">\(\psi_j(t)\)</span> keeps track of the state from the previous time step which results in highest <span class="math notranslate nohighlight">\(\delta_i(t-1)a_{ij}\)</span> so that we can keep track of where we came from and we can recreate the path.</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">Exercise 4.1: Vâ€™s Solutions!</p>
<ul class="simple">
<li><p>(A) True</p></li>
<li><p>(B) False. This will also depend upon the transition probabilities between states.</p></li>
<li><p>(C) True</p></li>
</ul>
</div>
<p><br><br><br><br></p>
</section>
</section>
<section id="the-backward-algorithm">
<h2>2. The backward algorithm<a class="headerlink" href="#the-backward-algorithm" title="Permalink to this heading">#</a></h2>
<section id="id1">
<h3>2.1 Introduction<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>In the last lecture we talked about supervised training of HMMs where we assumed that we had mapping between observation sequences and hidden state sequences.</p></li>
<li><p>In real life we rarely have such mapping available.</p></li>
<li><p>For example, you can imagine how much manual effort it would be to come up with gold part-of-speech tag sequences on a large enough sample of text data, say Wikipedia, so that we have enough training data in order to learn initial state probabilities, transition probabilities, and emission probabilities.</p></li>
</ul>
<p><strong>Question we want to answer</strong></p>
<ul class="simple">
<li><p>Given a large observation sequence (or a set of observation sequences) <span class="math notranslate nohighlight">\(O\)</span> for training, but <strong>not</strong> the state sequence, how do we choose the â€œbestâ€ parameters <span class="math notranslate nohighlight">\(\theta\)</span> that explain the data <span class="math notranslate nohighlight">\(O\)</span>?</p></li>
<li><p>We want our parameters <span class="math notranslate nohighlight">\(\theta\)</span> to be set so that the available training data is maximally likely.</p></li>
<li><p>We do this using the forward-backward algorithm.</p></li>
</ul>
<p><strong>Recall: The forward algorithm</strong></p>
<ul class="simple">
<li><p>Computer probability of a given observation sequence.</p></li>
<li><p>Given a model with parameters <span class="math notranslate nohighlight">\(\theta = &lt;\pi, T, B&gt;\)</span>, how do we efficiently compute the probability of a particular observation sequence <span class="math notranslate nohighlight">\(O\)</span>?</p></li>
<li><p>Example: Whatâ€™s the probability of the sequence below?</p></li>
</ul>
<p><img alt="" src="lectures/img/HMM_example_activity_seq_small.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_activity_seq.png" height="400" width="400">     -->
<!-- </center>     --><p>Three steps of the forward algorithm.</p>
<ul class="simple">
<li><p>Initialization: Compute the <span class="math notranslate nohighlight">\(\alpha\)</span> values for nodes in the first column of the trellis <span class="math notranslate nohighlight">\((t = 0)\)</span>.</p></li>
<li><p>Induction: Iteratively compute the <span class="math notranslate nohighlight">\(\alpha\)</span> values for nodes in the rest of the trellis <span class="math notranslate nohighlight">\((1 \leq t &lt; T)\)</span>.</p></li>
<li><p>Conclusion: Sum over the <span class="math notranslate nohighlight">\(\alpha\)</span> values for nodes in the last column of the trellis <span class="math notranslate nohighlight">\((t = T)\)</span>.</p></li>
</ul>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="800" width="800">  -->
<!-- </center>     -->
<ul class="simple">
<li><p>Sum over all possible final states:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(P(O;\theta) = \sum\limits_{i=1}^{n}\alpha_i(T-1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(E,L,F,C) = \alpha_ğŸ™‚(3) + \alpha_ğŸ˜”(3) = 0.00023 + 0.00207 = 0.0023\)</span></p></li>
</ul>
</li>
</ul>
<p><img alt="" src="lectures/img/hmm_alpha_values_small.png" /></p>
<!-- <center> -->
<!-- <img src="img/hmm_alpha_values.png" height="700" width="700">  -->
<!-- </center>     --><p><strong>What are we doing in the forward algorithm?</strong></p>
<ul class="simple">
<li><p>In forward algorithm the point is to compute <span class="math notranslate nohighlight">\(P(O;\theta)\)</span>.</p></li>
<li><p>For each state <span class="math notranslate nohighlight">\(i\)</span>, we calculated <span class="math notranslate nohighlight">\(\alpha_i(0), \alpha_i(1), \alpha_i(2), ...\)</span></p></li>
<li><p>The trellis was computed left to right and top to bottom.</p></li>
<li><p>The forward algorithm stores the probabilities of all possible 1-state sequences (from the start), to store all possible 2-state sequences (from the start), to store all possible 3-state sequences (from the start) and so on.</p></li>
</ul>
<p><br><br></p>
</section>
<section id="the-backward-algorithm-intuition">
<h3>2.2 The backward algorithm intuition<a class="headerlink" href="#the-backward-algorithm-intuition" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Less intuitively, we can also do that in reverse order, i.e., from  right to left and top to bottom.</p></li>
<li><p>Weâ€™ll still deal with the same observation sequence which evolves forward in time but we will store temporary results in the backward direction.</p></li>
</ul>
<ul class="simple">
<li><p>In the <span class="math notranslate nohighlight">\(i^{th}\)</span> node of the trellis at time <span class="math notranslate nohighlight">\(t\)</span>, we store the probability of starting in state <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> then observing everything that comes thereafter.
$<span class="math notranslate nohighlight">\(\beta_{i}(t) = P(b_{t+1:T-1})\)</span>$</p></li>
<li><p>The trellis is computed <strong>right-to-left</strong> and <strong>top-to-bottom</strong>.</p></li>
</ul>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="700" width="700">  -->
<!-- </center>     --><p><strong>The backward algorithm: steps</strong></p>
<p>Three steps of the backward procedure.</p>
<ul class="simple">
<li><p>Initialization: Initialize <span class="math notranslate nohighlight">\(\beta\)</span> values for nodes in the last column of the trellis.
$<span class="math notranslate nohighlight">\(\beta_i(T-1) = 1\)</span>$</p></li>
<li><p>Induction: Iteratively compute the <span class="math notranslate nohighlight">\(\beta\)</span> values for nodes in the rest of the trellis <span class="math notranslate nohighlight">\((1 \leq t &lt; T)\)</span> as the probability of being in state <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> and reading everything to follow.
$<span class="math notranslate nohighlight">\(\beta_i(t) = \sum_{j=1}^N a_{ij}b_{j}(o_{t+1}) \beta_j(t+1)\)</span>$</p></li>
<li><p>Conclusion: Sum over the <span class="math notranslate nohighlight">\(\beta\)</span> values for nodes in the first column of the trellis <span class="math notranslate nohighlight">\((t = 0)\)</span> (i.e., all initial states).</p></li>
</ul>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="500" width="500">  -->
<!-- </center>     --></section>
<section id="the-backward-algorithm-initialization-beta-3-and-beta-3">
<h3>2.3 The backward algorithm: Initialization <span class="math notranslate nohighlight">\(\beta_ğŸ™‚(3)\)</span> and <span class="math notranslate nohighlight">\(\beta_ğŸ˜”(3)\)</span><a class="headerlink" href="#the-backward-algorithm-initialization-beta-3-and-beta-3" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Initialize the nodes in the last column of the trellis <span class="math notranslate nohighlight">\((T = 3)\)</span>.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\beta_ğŸ™‚(3) = 1.0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_ğŸ˜”(3) = 1.0\)</span></p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="800" width="800">  -->
<!-- </center> --></section>
<section id="the-backward-algorithm-induction">
<h3>2.4 The backward algorithm: Induction<a class="headerlink" href="#the-backward-algorithm-induction" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Iteratively compute the nodes in the rest of the trellis <span class="math notranslate nohighlight">\((1 \leq t &lt; T)\)</span>.</p></li>
<li><p>To compute <span class="math notranslate nohighlight">\(\beta_j(t)\)</span> we can compute <span class="math notranslate nohighlight">\(\beta_{i}(t+1)\)</span> for all possible states <span class="math notranslate nohighlight">\(i\)</span> and then use our knowledge of <span class="math notranslate nohighlight">\(a_{ij}\)</span> and <span class="math notranslate nohighlight">\(b_j(o_{t+1})\)</span>
$<span class="math notranslate nohighlight">\(\beta_i(t) = \sum_{j=1}^N a_{ij}b_{j}(o_{t+1}) \beta_j(t+1)\)</span>$</p></li>
</ul>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="800" width="800">  -->
<!-- </center>     -->
<p><strong>The backward algorithm: Induction <span class="math notranslate nohighlight">\(\beta_ğŸ™‚(2)\)</span></strong></p>
<div class="math notranslate nohighlight">
\[\beta_i(t) = \sum_{j=1}^N a_{ij}b_{j}(o_{t+1}) \beta_j(t+1)\]</div>
<ul class="simple">
<li><p>Probability of being at state ğŸ™‚ at <span class="math notranslate nohighlight">\(t=2\)</span> and observing everything to follow.</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-cdb5558c-a884-4b8a-a8b0-9562adf4fb86">
<span class="eqno">(13)<a class="headerlink" href="#equation-cdb5558c-a884-4b8a-a8b0-9562adf4fb86" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{split}
\beta_ğŸ™‚(2) = &amp; a_{ğŸ™‚ğŸ™‚}b_ğŸ™‚(C)\beta_ğŸ™‚(3) + a_{ğŸ™‚ğŸ˜”}b_ğŸ˜”(C)\beta_ğŸ˜”(3)\\
             = &amp; 0.7 \times 0.1 \times 1.0 + 0.3 \times 0.6 \times 1.0\\ 
             = &amp; 0.25&amp; \\
\end{split}
\end{equation}\]</div>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="700" width="700">  -->
<!-- </center>     -->
<p><strong>The backward algorithm: Induction <span class="math notranslate nohighlight">\(\beta_ğŸ˜”(2)\)</span></strong></p>
<div class="math notranslate nohighlight">
\[\beta_i(t) = \sum_{j=1}^N a_{ij}b_{j}(o_{t+1}) \beta_j(t+1)\]</div>
<ul class="simple">
<li><p>Probability of being at state ğŸ˜” at <span class="math notranslate nohighlight">\(t=2\)</span> and observing everything to follow.</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-ee616658-88bb-49e0-8360-a536d510f987">
<span class="eqno">(14)<a class="headerlink" href="#equation-ee616658-88bb-49e0-8360-a536d510f987" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{split}
\beta_ğŸ˜”(2) = &amp; a_{ğŸ˜”ğŸ™‚}b_ğŸ™‚(C)\beta_ğŸ™‚(3) + a_{ğŸ˜”ğŸ˜”}b_ğŸ˜”(C)\beta_ğŸ˜”(3)\\
             = &amp; 0.4 \times 0.1 \times 1.0 + 0.6 \times 0.6 \times 1.0\\ 
             = &amp; 0.4&amp; \\
\end{split}
\end{equation}\]</div>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="700" width="700">  -->
<!-- </center>     -->
<p><strong>Carry out rest of the steps as home work.</strong></p>
<p><br><br></p>
</section>
<section id="the-backward-algorithm-conclusion">
<h3>2.5 The backward algorithm: Conclusion<a class="headerlink" href="#the-backward-algorithm-conclusion" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Sum over all possible initial states to get the probability of an observation sequence in the reverse direction.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[P(O;\theta) = \sum_{i=1}^{N} \pi_i b_i(O_0)\beta_i(0)\]</div>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <center> -->
<!-- <img src="img/HMM_example_trellis.png" height="700" width="700">  -->
<!-- </center>     --><ul class="simple">
<li><p>Weâ€™re not doing this just for fun.</p></li>
<li><p>We are going to use it for unsupervised HMM training!</p></li>
<li><p>In general, we can combine <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> at any point in time to represent the probability of an entire sequence.</p></li>
<li><p>This is going to be vital for training of unsupervised HMMs.</p></li>
</ul>
<p><br><br><br><br></p>
</section>
</section>
<section id="baum-welch-bw-algorithm-high-level-idea">
<h2>3. Baum-Welch (BW) algorithm (high-level idea)<a class="headerlink" href="#baum-welch-bw-algorithm-high-level-idea" title="Permalink to this heading">#</a></h2>
<section id="id2">
<h3>3.1 Introduction<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>Given a large observation sequence (or a set of observation sequences) <span class="math notranslate nohighlight">\(O\)</span> for training, but <strong>not</strong> the state sequence, how do we choose the â€œbestâ€ parameters <span class="math notranslate nohighlight">\(\theta\)</span> that explain the data <span class="math notranslate nohighlight">\(O\)</span>?</p>
<p>We want our parameters <span class="math notranslate nohighlight">\(\theta\)</span> to be set so that the available training data is maximally likely.</p>
<p><strong>Can we use MLE?</strong></p>
<ul class="simple">
<li><p>If the training data contained state sequences, we could simply do maximum likelihood estimation, as we did in the last lecture, to get transition probabilities and the emission probabilities.</p></li>
<li><p>But when we are only given observations, we <strong>cannot</strong> count the following:</p>
<ul>
<li><p>How often we move from <span class="math notranslate nohighlight">\(q_{i-1}\)</span> to <span class="math notranslate nohighlight">\(q_i\)</span> normalized by how often we move from <span class="math notranslate nohighlight">\(q_{i-1}\)</span> to anything:
<span class="math notranslate nohighlight">\(p(q_i|q_{i-1}) = \frac{Count(q_{i-1} q_i)}{Count(q_{i-1} \text{ANY STATE })}\)</span></p></li>
<li><p>Whatâ€™s the proportion of <span class="math notranslate nohighlight">\(q_i\)</span> emitting the observation <span class="math notranslate nohighlight">\(o_i\)</span> .<br />
<span class="math notranslate nohighlight">\(p(o_i|q_{i}) = \frac{Count(o_i \text{ and } q_i)}{Count(q_{i})}\)</span></p></li>
</ul>
</li>
<li><p>In many cases, the mapping between hidden states and observations is unknown and so we canâ€™t use MLE.</p></li>
<li><p>How to deal with the incomplete data?</p>
<ul>
<li><p>Use unsupervised learning</p></li>
</ul>
</li>
</ul>
<p><br><br></p>
</section>
<section id="iterative-unsupervised-approach">
<h3>3.2 Iterative unsupervised approach<a class="headerlink" href="#iterative-unsupervised-approach" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>We guess the parameters and iteratively update them.</p></li>
<li><p>Unsupervised HMM training is done using a combination of the forward and the backward algorithms.</p></li>
<li><p>The idea is that we can combine <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> at any point in time to represent the probability of an entire observation sequence.</p></li>
<li><p>The forward algorithm computes the <span class="math notranslate nohighlight">\(\alpha\)</span> values, which represent the probability of being in a particular state at a particular time, given the observation sequence up to that time.</p></li>
<li><p>The backward algorithm computes the <span class="math notranslate nohighlight">\(\beta\)</span> values, which represent the probability of observing the rest of the sequence after that time.</p></li>
<li><p>We define <span class="math notranslate nohighlight">\(\gamma_i(t)\)</span>, which represents the probability of being in state <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> given the entire observation sequence <span class="math notranslate nohighlight">\(O\)</span>. We calculate it by combining <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> values calculated by the forward and backward algorithms.</p></li>
<li><p>We define another probability <span class="math notranslate nohighlight">\(\xi_{ij}(t)\)</span> of landing in state <span class="math notranslate nohighlight">\(s_i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> and transitioning to state <span class="math notranslate nohighlight">\(s_j\)</span> at time <span class="math notranslate nohighlight">\(t+1\)</span> regardless of the previous states and future states given the observations.</p></li>
<li><p>These probabilities are used to compute the expected sufficient statistics</p>
<ul>
<li><p>the expected number of times each state is visited</p></li>
<li><p>the expected number of times each transition is made, given the observation sequence.</p></li>
</ul>
</li>
</ul>
<p><strong>Expectation maximization</strong></p>
<ul class="simple">
<li><p>We will start with a randomly initialized model.</p></li>
<li><p>We use the model to calculate new <span class="math notranslate nohighlight">\(\alpha_i(t), \beta_i(t), \gamma_i(t), \xi_{ij}(t)\)</span>.</p></li>
<li><p>We update the model.</p></li>
<li><p>We can do this iteratively until convergence or stopping condition.</p></li>
</ul>
<p><img alt="" src="../_images/em.png" /></p>
<!-- <center> -->
<!-- <img src="img/em.png" height="700" width="700">        -->
<!-- </center>    --><p>See <span class="xref myst">AppendixB</span> for more details on Baum Welch algorithm.</p>
<p><br><br></p>
</section>
<section id="unsupervised-learning-of-our-toy-problem">
<h3>3.3 Unsupervised learning of our toy problem<a class="headerlink" href="#unsupervised-learning-of-our-toy-problem" title="Permalink to this heading">#</a></h3>
<p>Now that we know how to do decoding and unsupervised learning of HMMs in Python, letâ€™s learn about how do they work. Letâ€™s try it out with <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code> before learning about the details.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span> 
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">t</span> <span class="o">=</span> <span class="mi">4000</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

<span class="n">range_min</span><span class="o">=</span><span class="mi">10</span>
<span class="n">range_max</span><span class="o">=</span><span class="mi">30</span>
<span class="n">seqlens</span> <span class="o">=</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">range_min</span><span class="p">,</span> <span class="n">range_max</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
<span class="n">seqlens</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">t</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">seqlens</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0],
       [2],
       [3],
       ...,
       [2],
       [1],
       [2]])
</pre></div>
</div>
</div>
</div>
<p>Letâ€™s train an unsupervised HMM on these sampled sequences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_states</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">unsup_model</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">CategoricalHMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_states</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">unsup_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">seqlens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "â–¸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "â–¾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>CategoricalHMM(n_components=2, n_features=4,
               random_state=RandomState(MT19937) at 0x14ACDFD40)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;CategoricalHMM<span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>CategoricalHMM(n_components=2, n_features=4,
               random_state=RandomState(MT19937) at 0x14ACDFD40)</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_hmm</span><span class="p">(</span><span class="n">unsup_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nodes:
[&#39;s0&#39;, &#39;s1&#39;]
</pre></div>
</div>
<img alt="../_images/47425944b3d502986aad0ee3ea6e12eb32409b8ff80b6b04e3a101550cb8c590.svg" src="../_images/47425944b3d502986aad0ee3ea6e12eb32409b8ff80b6b04e3a101550cb8c590.svg" /></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">unsup_model</span><span class="o">.</span><span class="n">startprob_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.53718145, 0.46281855])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">unsup_model</span><span class="o">.</span><span class="n">transmat_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[9.03358928e-01, 9.66410721e-02],
       [9.99095951e-01, 9.04049078e-04]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">unsup_model</span><span class="o">.</span><span class="n">emissionprob_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.447484  , 0.15438547, 0.31955587, 0.07857466],
       [0.5069931 , 0.12223317, 0.24067389, 0.13009984]])
</pre></div>
</div>
</div>
</div>
<p>Compare it with our toy HMM</p>
<p><img alt="" src="../_images/HMM_example_trellis.png" /></p>
<!-- <img src="img/HMM_example_trellis.png" height="600" width="600">  --><ul class="simple">
<li><p>The model doesnâ€™t look very close to the real model from which we have sampled the sequences.</p></li>
<li><p>But itâ€™s unsupervised and with more data the probabilities would probably make more sense.</p></li>
<li><p>Also, note that itâ€™s an unsupervised model and it doesnâ€™t give you interpretation of the states. You have to do it on your own.</p></li>
</ul>
<p><br><br><br><br></p>
</section>
</section>
<section id="final-comments-and-summary">
<h2>Final comments and summary<a class="headerlink" href="#final-comments-and-summary" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Hidden Markov Models (HMMs) provide a probabilistic framework to model sequences.</p></li>
<li><p>They are much more practical compared to Markov models and are widely used.</p></li>
<li><p>Speech recognition is a success story for HMMs.</p></li>
</ul>
<section id="three-fundamental-questions-for-hmms">
<h3>Three fundamental questions for HMMs<a class="headerlink" href="#three-fundamental-questions-for-hmms" title="Permalink to this heading">#</a></h3>
<p><strong>Likelihood</strong>: Given a model with parameters <span class="math notranslate nohighlight">\(\theta = &lt;\pi, A, B&gt;\)</span>, how do we efficiently compute the likelihood of a particular observation sequence <span class="math notranslate nohighlight">\(O\)</span>?</p>
<p><strong>Decoding</strong>
Given an observation sequence <span class="math notranslate nohighlight">\(O\)</span> and a model <span class="math notranslate nohighlight">\(\theta\)</span> how do we choose a state sequence <span class="math notranslate nohighlight">\(Q={q_0, q_1, \dots q_T}\)</span> that best explains the observation sequence?</p>
<p><strong>Learning</strong>
Given a large observation sequence <span class="math notranslate nohighlight">\(O\)</span> how do we choose the best parameters <span class="math notranslate nohighlight">\(\theta\)</span> that explain the data <span class="math notranslate nohighlight">\(O\)</span>?</p>
</section>
<section id="important-ideas-to-know">
<h3>Important ideas to know<a class="headerlink" href="#important-ideas-to-know" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The definition of an HMM</p></li>
<li><p>The conditional independence assumptions of an HMM</p></li>
<li><p>The purpose of the forward algorithm and the backward algorithm.</p>
<ul>
<li><p>How to compute <span class="math notranslate nohighlight">\(\alpha_i(t)\)</span> and <span class="math notranslate nohighlight">\(\beta_i(t)\)</span></p></li>
</ul>
</li>
<li><p>The purpose of the Viterbi algorithm.</p>
<ul>
<li><p>How to compute <span class="math notranslate nohighlight">\(\delta_i(t)\)</span> and <span class="math notranslate nohighlight">\(\psi_i(t)\)</span></p></li>
</ul>
</li>
<li><p>The purpose of the Baum-Welch algorithm.</p></li>
</ul>
</section>
<section id="continuous-hmms">
<h3>Continuous HMMs<a class="headerlink" href="#continuous-hmms" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>If the observations are drawn from a continuous space (e.g., speech), the probabilities must be continuous as well.</p></li>
<li><p>HMMs generalize to continuous probability distributions.</p></li>
<li><p>In the lab your observations are mfcc feature vectors for time frames which are continuous observations.</p></li>
<li><p>In <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code> you can use <code class="docutils literal notranslate"><span class="pre">GaussianHMM</span></code> or <code class="docutils literal notranslate"><span class="pre">GMMHMM</span></code> for continuous observations.</p></li>
</ul>
<p><img alt="" src="../_images/continuous_hmms.png" /></p>
<!-- <center> -->
<!-- <img src="img/continuous_hmms.png" height="400" width="400">        -->
<!-- </center>    --></section>
<section id="id3">
<h3>Important ideas to know<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>Using <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code></p>
<ul class="simple">
<li><p>For unsupervised training of HMMs.</p></li>
<li><p>For likelihood (<code class="docutils literal notranslate"><span class="pre">model.score</span></code>)</p></li>
<li><p>For decoding (<code class="docutils literal notranslate"><span class="pre">model.decode</span></code>)</p></li>
<li><p>For discrete observations (<code class="docutils literal notranslate"><span class="pre">MultinomialHMM</span></code>)</p></li>
<li><p>For continuous observations (<code class="docutils literal notranslate"><span class="pre">GaussianHMM</span></code> or <code class="docutils literal notranslate"><span class="pre">GMMHMM</span></code>)</p></li>
<li><p>For sequences with varying lengths.</p></li>
</ul>
<p><br><br><br><br></p>
</section>
<section id="some-useful-resources-and-links">
<h3>Some useful resources and links<a class="headerlink" href="#some-useful-resources-and-links" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="http://www.cs.toronto.edu/~frank/csc401/lectures2020/5_HMMs.pdf">Frank Rudziczâ€™s slides on HMM</a></p></li>
<li><p><a class="reference external" href="https://people.cs.umass.edu/~mccallum/courses/inlp2004a/lect10-hmm2.pdf">Andrew McCallumâ€™s slides on HMM</a></p></li>
</ul>
<p><br><br><br><br></p>
</section>
</section>
<section id="id4">
<h2>â“â“ Questions for you<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h2>
<section id="optional-exercise-4-2-more-practice-questions">
<h3>(Optional) Exercise 4.2: More practice questions<a class="headerlink" href="#optional-exercise-4-2-more-practice-questions" title="Permalink to this heading">#</a></h3>
<p>Discuss the following questions with your neighbour.</p>
<p>Consider the sentence below:</p>
<blockquote>
    Will the chair chair the meeting from this chair ?
</blockquote>
<p>and a simple part-of-speech tagset:</p>
<blockquote>
{noun, verb, determiner, preposition, punctuation}
</blockquote>    
<p>The table below shows the possible assignments for words and part-of-speech tags. The symbol <code class="docutils literal notranslate"><span class="pre">x</span></code> denotes that the word and part-of-speech tag combination is possible. For instance, the word <em>chair</em> is unlikely to be used as a determiner and so we do not have an <code class="docutils literal notranslate"><span class="pre">x</span></code> there.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><i></i></p></th>
<th class="head text-center"><p>Will</p></th>
<th class="head text-center"><p>the</p></th>
<th class="head text-center"><p>chair</p></th>
<th class="head text-center"><p>chair</p></th>
<th class="head text-center"><p>the</p></th>
<th class="head text-center"><p>meeting</p></th>
<th class="head text-center"><p>from</p></th>
<th class="head text-center"><p>this</p></th>
<th class="head text-center"><p>chair</p></th>
<th class="head text-center"><p>?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>noun</p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p><i></i></p></td>
</tr>
<tr class="row-odd"><td><p>verb</p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p><i></i></p></td>
</tr>
<tr class="row-even"><td><p>determiner</p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
</tr>
<tr class="row-odd"><td><p>preposition</p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p>x</p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
</tr>
<tr class="row-even"><td><p>punctuation</p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p><i></i></p></td>
<td class="text-center"><p>x</p></td>
</tr>
</tbody>
</table>
<p>Given this information, answer the following questions:</p>
<ul class="simple">
<li><p>(A) With this simple tagset of part-of-speech tags, how many possible part-of-speech tag sequences (i.e, hidden state sequences) are there for the given sentence (observation sequence)?</p></li>
<li><p>(B) Restricting to the possibilities shown above with <code class="docutils literal notranslate"><span class="pre">x</span></code>, how many possible part-of-speech tag sequences are there?</p></li>
<li><p>(C) Given an HMM with states as part-of-speech tags and observations as words, one way to decode the observation sequence is using the brute force method below. What is the time complexity of this method in terms of the number of states (<span class="math notranslate nohighlight">\(n\)</span>) and the length of the output sequence (<span class="math notranslate nohighlight">\(T\)</span>)? You may ignore constants.</p>
<ul>
<li><p>enumerate all possible hidden state sequences (i.e., enumerate all solutions)</p></li>
<li><p>for each hidden state sequence, calculate the probability of the observation sequence given the hidden state sequence (i.e., score each solution)</p></li>
<li><p>pick the hidden state sequence which gives the highest probability for the observation sequence (i.e., pick the best solution)</p></li>
</ul>
</li>
<li><p>(D) If you decode the sequence using the Viterbi algorithm instead, what will be the time complexity in terms of the number of states (<span class="math notranslate nohighlight">\(n\)</span>) and the length of the output sequence (<span class="math notranslate nohighlight">\(T\)</span>)? You may ignore constants.</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">Exercise 4.2: Vâ€™s Solutions!</p>
<ul class="simple">
<li><p>(A) 9.7 million!</p></li>
<li><p>(B) 128</p></li>
<li><p>(C) The time complexity of this approach is <span class="math notranslate nohighlight">\(\mathcal{O}(N^T)\)</span>.</p></li>
<li><p>(D) <span class="math notranslate nohighlight">\(\mathcal{O}(N^2T)\)</span></p></li>
</ul>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-env-575-py"
        },
        kernelOptions: {
            name: "conda-env-575-py",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-env-575-py'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="03_HMMs-intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Lecture 3: Introduction to Hidden Markov Models (HMMs)</p>
      </div>
    </a>
    <a class="right-next"
       href="05_topic-modeling.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 5: Topic Modeling</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-plan-imports-lo">Lecture plan, imports, LO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recap">Recap</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hmm-ingredients">HMM ingredients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recap-the-forward-algorithm">Recap: The forward algorithm</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-the-viterbi-algorithm">1. Decoding: The Viterbi algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1.1 Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#viterbi-initialization">1.2 Viterbi: Initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#viterbi-induction">1.3 Viterbi: Induction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#viterbi-conclusion">1.4 Viterbi conclusion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#viterbi-with-hmmlearn-on-our-toy-hmm">1.5 Viterbi with  <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code> on our toy HMM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">â“â“ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-1-select-all-of-the-following-statements-which-are-true-iclicker">Exercise 4.1: Select all of the following statements which are <strong>True</strong> (iClicker)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-algorithm">2. The backward algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.1 Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-algorithm-intuition">2.2 The backward algorithm intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-algorithm-initialization-beta-3-and-beta-3">2.3 The backward algorithm: Initialization <span class="math notranslate nohighlight">\(\beta_ğŸ™‚(3)\)</span> and <span class="math notranslate nohighlight">\(\beta_ğŸ˜”(3)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-algorithm-induction">2.4 The backward algorithm: Induction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-algorithm-conclusion">2.5 The backward algorithm: Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#baum-welch-bw-algorithm-high-level-idea">3. Baum-Welch (BW) algorithm (high-level idea)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">3.1 Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-unsupervised-approach">3.2 Iterative unsupervised approach</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupervised-learning-of-our-toy-problem">3.3 Unsupervised learning of our toy problem</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-comments-and-summary">Final comments and summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#three-fundamental-questions-for-hmms">Three fundamental questions for HMMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-ideas-to-know">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-hmms">Continuous HMMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Important ideas to know</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-useful-resources-and-links">Some useful resources and links</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">â“â“ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-exercise-4-2-more-practice-questions">(Optional) Exercise 4.2: More practice questions</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Varada Kolhatkar
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      Â© Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>