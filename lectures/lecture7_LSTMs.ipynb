{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DSCI 575: Advanced Machine Learning (in the context of Natural Language Processing (NLP) applications)\n",
    "\n",
    "UBC Master of Data Science program, 2019-20\n",
    "\n",
    "Instructor: Varada Kolhatkar [ʋəɾəda kɔːlɦəʈkər]\n",
    "\n",
    "\n",
    "## Lecture 7: Long Short-Term Memory Networks (LSTMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kvarada/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kvarada/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kvarada/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kvarada/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kvarada/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kvarada/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys, re\n",
    "import tensorflow \n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning outcomes\n",
    "\n",
    "From this lecture you will be able to \n",
    "\n",
    "- specify the problem of vanishing and exploding gradients\n",
    "- explain the idea of LSTMs at a high level\n",
    "- implement an LSTM for character-based text generation using `Keras` and `Tensorflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation \n",
    "\n",
    "- Extremely popular \n",
    "- Used in all kinds of interesting applications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM for image captioning \n",
    "\n",
    "<img src=\"images/RNN_LSTM_image_captioning.png\" height=\"2000\" width=\"2000\"> \n",
    "\n",
    "\n",
    "(Credit: [LSTMs for image captioning](https://arxiv.org/pdf/1411.4555.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Neural Storyteller](https://github.com/ryankiros/neural-storyteller)\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "<img src=\"images/RNN_example.jpg\" width=\"800\" height=\"800\">\n",
    "\n",
    "<p style=\"font-size:30px\">We were barely able to catch the breeze at the beach , and it felt as if someone stepped out of my mind . She was in love with him for the first time in months , so she had no intention of escaping . The sun had risen from the ocean , making her feel more alive than normal . She 's beautiful , but the truth is that I do n't know what to do ...</p>\n",
    "</blockquote>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTMs for video captioning \n",
    "\n",
    "<img src=\"images/RNN_video_captioning2.png\" height=\"1500\" width=\"1500\"> \n",
    "\n",
    "\n",
    "(Credit: [LSTMs for video captioning](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Venugopalan_Sequence_to_Sequence_ICCV_2015_paper.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTMs for executing Python programs \n",
    "\n",
    "- In 2014, Google researchers built an LSTM that learns to execute simple\n",
    "Python programs!\n",
    "\n",
    "<img src=\"images/RNN_learning_to_execute.png\" width=\"1500\" height=\"1500\">\n",
    "\n",
    "(Credit: [Learning to execute](https://arxiv.org/pdf/1410.4615.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall: How do we learn RNNs?\n",
    "\n",
    "- Forward pass \n",
    "- Backward pass (backprop through time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Recall: Forward pass in RNNs\n",
    "- Computing new states and output in RNNs\n",
    "\n",
    "$$\n",
    "s_t = tanh(Ws_{t-1} + Ux_t + b_1)\\\\\n",
    "\\hat{y}_t = \\text{softmax}(Vs_t + b_2)\n",
    "$$ \n",
    "\n",
    "<img src=\"images/RNN_dynamic_model.png\" height=\"800\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recall: Backpropagation through time\n",
    "\n",
    "- When we do backprop with feedforward neural networks\n",
    "    - Take the gradient (derivative) of the loss with respect to the parameters. \n",
    "    - Change parameters to minimize the loss. \n",
    "\n",
    "- In RNNs we use a generalized version of backprop called Backpropogation Through Time (BPTT)\n",
    "    - Calculating gradient at each output depends upon the current time step as well as the previous time steps. \n",
    "\n",
    "    \n",
    "<img src=\"images/RNN_loss.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "    \n",
    "[Credit](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vanishing and exploding gradients\n",
    "\n",
    "- So in the backward pass of RNNs, we have to multiply many derivatives together, which very often results in\n",
    "    - vanishing gradients (gradients becoming very small) \n",
    "    - exploding gradients (gradients becoming too big)\n",
    "- One of reasons why people were not able to train these networks for a long time \n",
    "    \n",
    "<center>\n",
    "<img src=\"images/RNN_loss.png\" height=\"800\" width=\"800\"> \n",
    "<center>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why this is a problem? \n",
    "\n",
    "- Let's consider the case of vanishing gradients\n",
    "    - Caused by multiplying many small numbers together \n",
    "- Suppose we are trying to predict the next word after 'a'. In this case, it is dependent upon 'law', which occurs more than 10 words away. \n",
    "<blockquote>\n",
    "    I am studying law at the University of British Columbia in Canada because I want to work as a ___. \n",
    "</blockquote>\n",
    "\n",
    "- Time steps that are further back away have smaller and smaller gradient. \n",
    "\n",
    "- If we have a vanishing gradient, we might not be able to update our weights reliably. \n",
    "- Only able to capture short-term dependencies, which kind of defeats the whole purpose of using RNNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's look into it a bit more closely\n",
    "\n",
    "- Let $W_{hh}$ be the weight matrix between hidden layers\n",
    "- Let $W_{hx}$ be the weight matrix between input and hidden layers\n",
    "- Let $\\begin{bmatrix}W_{hh} & W_{hx}\\end{bmatrix}$ be $W$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "h_t =& tanh(W_{hh}h_{t-1} + W_{hx}x_t)\\\\\n",
    "    =& tanh( \\begin{bmatrix}W_{hh} & W_{hx}\\end{bmatrix} \\begin{bmatrix}h_{t-1}\\\\x_t\\end{bmatrix})\\\\\n",
    "    =& tanh( W \\begin{bmatrix}h_{t-1}\\\\x_t\\end{bmatrix})\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "<img src=\"images/RNN_gradient_flow.png\" height=\"400\" width=\"400\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient flow in RNNs\n",
    "\n",
    "- During the backward pass\n",
    "    - we have the derivative of loss with respect $h_t$ \n",
    "    - we want to compute the derivative of loss with respect to $h_{t-1}$\n",
    "\n",
    "<img src=\"images/RNN_gradient_flow1.png\" height=\"400\" width=\"400\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient flow in RNNs\n",
    "\n",
    "- Computing gradient of loss with respect to $h_0$ involves repeated multiplications of same quantities \n",
    "- If many values > 1.0 then we have exploding gradient. \n",
    "    - Possible solution: gradient clipping to scale big gradients.     \n",
    "- If many values < 1.0 then we have vanishing gradient.\n",
    "    - Possible solution: weight initialization, activation function    \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/RNN_gradient_flow2.png\" height=\"1300\" width=\"1300\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Vanishing and exploding gradients: Simple example \n",
    "\n",
    "- Multiplying many numbers together either results in a very small or very large numbers. \n",
    "- Suppose we have $T$ time steps.  \n",
    "- Suppose all numbers in the product are scalars and have some value $\\alpha$\n",
    "- As $T \\rightarrow \\infty$\n",
    "    - $\\alpha^T \\rightarrow \\infty$ if $\\alpha > 1$\n",
    "    - $\\alpha^T \\rightarrow 0$ if $\\alpha < 1$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A robust solution to this problem is \n",
    "\n",
    "- **Use a more complex recurrent unit with gates**\n",
    "    - Gated Recurrent Units (GRUs)    \n",
    "    - **Long Short Term Memory networks (LSTMs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Echo state networks\n",
    "\n",
    "- Idea: Initialize the  weight matrices carefully. \n",
    "- For example, Set $U$, $W$ to identity matrix and only learn $V$. \n",
    "    - Each state is a summation of previous states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long Short Term Memory networks (LSTMs)\n",
    "\n",
    "- [Invented in 1997](https://www.bioinf.jku.at/publications/older/2604.pdf) by Hochreiter and Schmidhuber. \n",
    "- Designed so that model can remember things for a long time (hundreds of time steps)! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple (Vanilla) RNN \n",
    "\n",
    "- In a simple RNN, repeating module contain a simple computation nodes.  \n",
    "\n",
    "\n",
    "<img src=\"files/images/RNN_alternative_representation.png\" height=\"1000\" width=\"1000\"> \n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long Short Term Memory networks (LSTMs)\n",
    "\n",
    "- In an LSTM, the repeating module is more complicated. \n",
    "- It selectively controls the flow of information using gates. \n",
    "\n",
    "\n",
    "<img src=\"files/images/RNN_alternative_representation.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "\n",
    "<img src=\"files/images/LSTM0.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTMs: Controlling the flow of information using gates\n",
    "\n",
    "- The information is added and removed through a structure called gates. \n",
    "- They optionally let the information through via sigmoid layer and pointwise multiplication\n",
    "    - The sigmoid layer outputs a number between 0 and 1, deciding how much of each component should be let through.\n",
    "    - A pointwise multiplication operation applies the decision. \n",
    "\n",
    "<img src=\"images/RNN_LSTM2.png\" height=\"800\" width=\"800\"> \n",
    "   \n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### LSTMs\n",
    "\n",
    "- In addition to usual hidden units, LSTMs have memory cells. \n",
    "- Purpose of memory cells is to remember things for a long time.\n",
    "\n",
    "<img src=\"images/RNN_LSTM1.png\" height=\"1000\" width=\"1000\"> \n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTMs: The core idea \n",
    "\n",
    "- The core idea in LSTMs is using a cell state (memory cell)\n",
    "- Information can flow along the memory unchanged. \n",
    "- Information can be removed or written to the cells regulated by gates. \n",
    "\n",
    "<img src=\"images/RNN_LSTM0.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does LSTM work?\n",
    "- Four operations: forget, store (input), update, output\n",
    "\n",
    "<img src=\"images/RNN_LSTM0.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forget operation\n",
    "\n",
    "- A sigmoid layer, **forget gate**, decides which values of the memory cell to reset. \n",
    "- Decides what part of the history is worth forgetting.\n",
    "- $f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f)$\n",
    "\n",
    "<img src=\"images/RNN_LSTM3.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Store operation \n",
    "\n",
    "- Decides what part of the new information is worth storing. \n",
    "- Two parts: \n",
    "    - A sigmoid layer, **input gate.**\n",
    "    - $i_t = \\sigma(W_i[h_{t-1}, x_t] + b_i)$\n",
    "\n",
    "<img src=\"images/RNN_LSTM4.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector of new candidate values\n",
    "\n",
    "- A $tanh$ layer creates a vector of new candidate values $\\tilde{c}_t$ to write to the memory cell. \n",
    "- $\\tilde{c}_t = tanh(W_{c}[h_{t-1}, x_t] + b_c)$ \n",
    "\n",
    "<img src=\"images/RNN_LSTM5.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Update operation (memory cell update)\n",
    "\n",
    "- The previous steps decided which values of the memory cell to reset and overwrite. \n",
    "- Now the LSTM applies the decisions to the memory cells.\n",
    "- $c_t = f_t \\times c_{t-1} + i_t \\times \\tilde{c}_t$ \n",
    "\n",
    "<img src=\"images/RNN_LSTM6.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Output operation\n",
    "\n",
    "- The sigmoid layer, **output gate**, decides which values should be sent to the network in the next time step. \n",
    "- $o_t = \\sigma(W_{o}[h_{t-1}, x_t] + b_o)$\n",
    "\n",
    "<img src=\"images/RNN_LSTM7.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Output update\n",
    "\n",
    "- The memory cell goes through $tanh$ and is multiplied by the output gate\n",
    "- $h_t = o_t \\cdot \\tanh(c_t)$\n",
    "\n",
    "<img src=\"images/RNN_LSTM8.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "\n",
    "(Credit: [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does it help RNN training?\n",
    "\n",
    "- You might be wondering why this solves the problem of vanishing and exploding gradients.\n",
    "- Turns out that the cell state we create in LSTMs allows an uninterrupted flow of gradients through time which mitigates our problem of vanishing and exploding gradients! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTMs: Confusing diagrams!!!\n",
    "\n",
    "- LSTMs are not very intuitive.  \n",
    "- Complicated combination of state in the past, observation at the moment and different ways to either forget the observation or keep it around. \n",
    "- Famous for confusing illustrative diagrams. \n",
    "\n",
    "<img src=\"images/RNN_confusing_LSTMs.png\" height=\"1000\" width=\"1000\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multilayer RNNs/LSTMs\n",
    "\n",
    "- Stack multiple layers of RNNs or LSTMs on the top of each other. \n",
    "\n",
    "\n",
    "<img src=\"images/RNN_stacked.png\" height=\"600\" width=\"600\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bidirectional RNNs/LSTMs\n",
    "\n",
    "- Problem with RNNs/LSTMs is that the prediction at a certain time uses information only from the previous time steps and not the later timesteps. Example: \n",
    "<blockquote>\n",
    "He said, \"Teddy Roosevelt was a great president!\"<br>\n",
    "He said, \"Teddy bears are on sale!\"\n",
    "</blockquote>    \n",
    "\n",
    "\n",
    "<img src=\"images/RNN_bidirectional_LSTM.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "[Source](https://www.i2tutorials.com/technology/deep-dive-into-bidirectional-lstm/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's build LSTMs!!! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generation with LSTMs \n",
    "\n",
    "LSTMs are expensive to train and, we'll be training them on [Google colab](https://colab.research.google.com/notebooks/welcome.ipynb). This will allow us to train on a GPU and assess the benefits of training neural networks on GPUs.\n",
    "\n",
    "You can follow the steps below.\n",
    "\n",
    "- Go to [Google colab](https://colab.research.google.com/). \n",
    "- Make an account if you don't have one.\n",
    "- Select \"UPLOAD\" and upload [this notebook](code/LSTM-character-based-text-generation.ipynb) in Google Colab. \n",
    "- Runtime --> change runtime type --> Select GPU.\n",
    "- Run the notebook. s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Number of Parameter in LSTMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 30, 10)            360       \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 36)                1836      \n",
      "=================================================================\n",
      "Total params: 14,396\n",
      "Trainable params: 14,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build and compile network=\n",
    "vocab_size = 36\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length = 30))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explanation of parameters\n",
    "\n",
    "- Embedding layer (embedding_1)\n",
    "    - 360 &rarr; size of the embedding matrix. We have 36 words in the vocabulary and each word has a 10 dimensional word embedding. \n",
    "\n",
    "- LSTM layer (lstm_1)\n",
    "    - For each gate in the LSTM cell, we have a weight matrix of size:  (num_units + input_size + 1) * num_units = $(50 + 10 + 1) \\times 50 = 3050$\n",
    "\n",
    "        - num_units &rarr; number of hidden units in the LSTM cell (50 in the example above)\n",
    "        - input_size &rarr; size of the input vector (10 in the example above)\n",
    "        - 1 &rarr; for bias\n",
    "    \n",
    "    - We have 4 such weight matrices: for forget gate, input gate, output gate, and cell memory.\n",
    "    - The number of parameters = ( (num_units + input_size + 1) * num_units ) * 4 =  $((50 + 10 + 1) \\times 50) \\times 4 = 12200$\n",
    "- Dense layer (dense_1)\n",
    "    - (num_units + 1) * size_of_vocab = (50 + 1) * 36 = 1836\n",
    "    - 1 &rarr; for bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- Training RNNs is hard because of vanishing and exploding gradients.  \n",
    "- LSTMs mitigate the problem by introducing a mechanism to selectively control the flow of information in the network  \n",
    "- They are widely used models to process sequential data in deep learning community and have a wide range of applications\n",
    "- Input to LSTMs is a 3-dimensional data matrix (called tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN resources\n",
    "\n",
    "A lot of material is available on the web. Here are some resources that were useful for me. \n",
    "\n",
    "\n",
    "- [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- Geoff Hinton's [short talk](https://www.youtube.com/watch?v=93rzMHtYT_0) and [lecture](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec7.pdf) on LSTMs\n",
    "- [Yoshua Bengio's lecture](https://www.youtube.com/watch?v=AYku9C9XoB8&t=884s)\n",
    "- [Ali Ghodsi's lecture on RNNs](https://www.youtube.com/results?search_query=ali+ghodsi+RNNs)\n",
    "- Richard Socher's [slides](https://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf) and [lecture](https://www.youtube.com/watch?v=Keqep_PKrY8) on RNNs\n",
    "- [A list of RNN resources](https://github.com/ajhalthor/awesome-rnn)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
