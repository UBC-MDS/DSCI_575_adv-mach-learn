{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/575_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 7: More transformers\n",
    "\n",
    "UBC Master of Data Science program, 2024-25\n",
    "\n",
    "> [Attention is all you need!](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, LO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learning outcomes\n",
    "\n",
    "From this lecture you will be able to \n",
    "\n",
    "- Broadly explain how transformers are used in a language model.\n",
    "  \n",
    "- Broadly explain how transformers are used for autoregressive text generation. \n",
    "- Broadly explain how bi-directional attention works. \n",
    "- Broadly explain masking and masked language models.\n",
    "- Explain the difference between causal language model and bi-directional language model.\n",
    "- Use PyTorch's [`TransformerDecoderLayer`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html) layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributions\n",
    "\n",
    "This material is heavily based on [Jurafsky and Martin, Chapter 10](https://web.stanford.edu/~jurafsky/slp3/10.pdf) and [Chapter 11](https://web.stanford.edu/~jurafsky/slp3/11.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap and introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1: Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) Unlike RNNs, self-attention allows the model to process all tokens in the sequence simultaneously without recurrent connections.\n",
    "- (B) Self-attention enables each input token to influence the representation of every other token in the sequence through learned attention weights.\n",
    "- (C) Self-attention computes a weighted sum of value representation based on the similarity between query and key representations of tokens.\n",
    "  \n",
    "- (D) The computational complexity of self-attention is quadratic in the sequence length, due to pairwise attention score calculations.\n",
    "  \n",
    "- (E) The output of a self-attention layer is always a scalar per token.\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 7.1: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "- A, B, C, D\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 7.2: Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) Self-attention can model long-range dependencies better than RNNs because it does not rely on sequential information flow.\n",
    "- (B) Transformers use positional embeddings because self-attention alone does not encode the order of tokens.\n",
    "- (C) Multihead self-attention allows the model to attend to different types of information from different subspaces.\n",
    "  \n",
    "- (D) Self-attention enables Transformers to model relationships between different parts of the input by computing weighted combinations of all tokens, where weights are based on learned similarity scores.\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 7.2: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "- A, B, C, D\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3: Discuss the following questions with your neighbour\n",
    "\n",
    "- How are the query ($Q$), key ($K$), and value ($V$) matrices used in self-attention?\n",
    "  \n",
    "- What are the typical components of a Transformer block?\n",
    "- What architectural feature enables Transformers to process data in parallel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know some fundamentals of transformers. In this lesson we'll focus on: \n",
    "\n",
    "- Three types of language models\n",
    "- Coding example of text generation and `TransformerDecoderLayer` using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lecture, we looked at this architecture for language modeling. This is a decoder-only language model. \n",
    "\n",
    "<br><br>\n",
    "![](../img/transformers-single-layer-LM.png)\n",
    "\n",
    "<!-- <img src=\"img/transformers-single-layer-LM.png\" width=\"700\" height=\"700\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models can be broadly categorized into three types:\n",
    "\n",
    "- **Decoder-only models (e.g., GPT-3)**: These models focus on generating text based on prior input. They are typically used for tasks like text generation.\n",
    "  \n",
    "- **Encoder-only models (e.g., BERT, RoBERTa)**: These models are designed to analyze and understand input text, making them suitable for tasks such as sentiment analysis and question answering.\n",
    "  \n",
    "- **Encoder-decoder models (e.g., T5, BART)**: These models combine the functionalities of both encoder and decoder architectures to handle tasks that involve transforming an input into an output, such as translation or summarization. \n",
    "  \n",
    "Let's try to understand the distinctions between these architectures. Later, we will look at a code example of the decoder-only architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decoder-only architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training a language model with transformers**\n",
    "\n",
    "To train a language model, we begin by segmenting a large corpus of text into fixed-length **input/output sequence pairs**. For example:\n",
    "\n",
    "> **Input**: So long and thanks for  \n",
    "> **Target (Gold output)**: long and thanks for all\n",
    "\n",
    "- Here, the goal is for the model to predict the next word at each position in the target sequence based on the corresponding context in the input.\n",
    "\n",
    "- At each time step, the final layer of the Transformer takes into account **all preceding tokens** in the input sequence and predicts a **probability distribution** over the entire vocabulary for the next token.\n",
    "\n",
    "- During training, we use **cross-entropy loss** to compare the model's predicted distribution with the actual next word in the sequence. The objective is to **maximize the likelihood** of the correct next token at each position.\n",
    "\n",
    "- The total loss for a training sequence is computed as the **mean of the cross-entropy losses** across all positions in that sequence.\n",
    "\n",
    "![Training a language model](../img/lm-training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causal Masking in decoder-only models**\n",
    "\n",
    "- In decoder-only language models, we must ensure that the model does not attend to future tokens when computing self-attention. This is essential for preserving the causal structure of text generation, where each token is predicted based only on the preceding tokens.\n",
    "\n",
    "- To enforce this, we apply a **causal mask** during the attention computation. The mask sets the upper triangle of the attention score matrix (which corresponds to future tokens) to a very large negative value, typically $-\\infty$.\n",
    "\n",
    "- When the attention scores are passed through the softmax function, these large negative values become effectively zero. As a result, the model ignores future tokens when predicting the current one.\n",
    "\n",
    "- This mechanism maintains the left-to-right (causal) nature of the model, ensuring that predictions are based only on past context.\n",
    "\n",
    "<img src=\"../img/self_attention_calc_partial.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autoregressive text generation**\n",
    "\n",
    "- Once we have a trained model, we can generate new text **autoregressively**, similar to how RNN-based models work.\n",
    "\n",
    "- In autoregressive generation (also known as **causal language model generation**), text is generated one token at a time by repeatedly sampling the next token conditioned on all previously generated tokens.\n",
    "\n",
    "- The sampling process in neural text generation resembles that of **Markov models** in its sequential nature. However, Transformer-based models capture **long-range dependencies** and **richer contextual information** beyond immediate history.\n",
    "\n",
    "- Models in the **GPT family** are examples of autoregressive language models, primarily designed for generating coherent and contextually appropriate text.\n",
    "\n",
    "![Autoregressive generation](../img/transformer_autoregressive_text_generation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Recipe generation demo](../demo/transformers-recipe-generation.ipynb)**\n",
    "\n",
    "- In this demo, we will implement transformer-based text generation using **PyTorch**.\n",
    "\n",
    "- We'll walk through the key components needed to build a simple autoregressive text generator and apply it to generate **recipe-style text**.\n",
    "\n",
    "- This example will help you understand how the underlying architecture translates into working code and how generation unfolds step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break\n",
    "\n",
    "![](../img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoder-only architecture\n",
    "\n",
    "- Models such as [**BERT**](https://en.wikipedia.org/wiki/BERT_(language_model)) and its variant **RoBERTa** are examples of **bidirectional Transformer models** with an **encoder-only architecture**.\n",
    "\n",
    "- These models are primarily designed for a variety of **natural language understanding tasks**, such as text classification, sentence similarity, and question answering.\n",
    "\n",
    "- Remember the [**sentence transformers**](https://www.sbert.net/) you used in DSCI 563 Lab 1 to compute sentence embeddings? Those embeddings are based on **BERT**, fine-tuned for semantic similarity tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bidirectional self-attention**\n",
    "\n",
    "- So far, we've looked at **backward-looking self-attention**, also known as a **causal** or **left-to-right Transformer model**.\n",
    "\n",
    "- In causal attention, each output is computed using **only the tokens that come before** the current position in the sequence.\n",
    "\n",
    "- This setup is ideal for **autoregressive text generation**, where predicting the next token depends solely on prior context.\n",
    "\n",
    "- However, in tasks like **sequence classification** or **sequence labeling**, causal models have a major limitation: they **cannot access future tokens**, which may contain important context.\n",
    "\n",
    "- The hidden state at each position is computed using only the **current and preceding tokens**, and it **ignores useful information** from the right side of the sequence.\n",
    "\n",
    "- **Bidirectional self-attention** overcomes the limitations of causal attention by allowing each token to attend to **all tokens in the input sequence**, regardless of their position.\n",
    "\n",
    "![Causal vs. Bidirectional Attention](../img/causal-bi-directional-self-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- As in causal attention, the model maps sequences of input embeddings $(x_1, \\dots, x_n)$ to sequences of output embeddings $(y_1, \\dots, y_n)$ of the same length.\n",
    "\n",
    "- This bidirectional flow of information makes it particularly well-suited for **sequence classification**, **named entity recognition**, and other **language understanding tasks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With **bidirectional encoders**, we obtain **contextual representations** of each token by incorporating information from both the left and right context. These representations are generally useful for a wide range of **downstream NLP tasks**.\n",
    "\n",
    "- The core computations in the self-attention mechanism remain the same as in causal models.\n",
    "\n",
    "- However, in bidirectional attention, we no longer mask out the future tokens. The attention score matrix includes **all pairwise dot products** $q_i \\cdot k_j$ without setting the upper triangle to $-\\infty$.\n",
    "\n",
    "<img src=\"../img/self_attention_calc_all.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "- But this raises a challenge: **How do we train a bidirectional encoder?**\n",
    "\n",
    "- In causal models, training is straightforward. We simply predict the next word based on past context.\n",
    "\n",
    "- Can we apply the same strategy here?\n",
    "\n",
    "  - **No**, because in a bidirectional model, the representation of a token already includes information from both sides.  \n",
    "  - Predicting the next token would be **cheating**, since the model has access to the token it's supposed to predict.\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Fill-in-the-blank\" task**\n",
    "\n",
    "- To train **bidirectional encoder models**, we use a **fill-in-the-blank** task, also known as the **cloze task**, instead of predicting the next word.\n",
    "\n",
    "- In this setup, the model is given an input sequence with one or more tokens masked (hidden), and it must **predict the missing tokens**.\n",
    "\n",
    "> **I am studying science at UBC because I want to ___ as a data scientist.**  \n",
    "> **The ___ in the exam where the fire alarm is ___ are really stressed.**\n",
    "\n",
    "- During training, one or more tokens in the input sequence are **masked out**, and the model learns to predict a **probability distribution** over the vocabulary for each masked position.\n",
    "\n",
    "- The model uses **cross-entropy loss** at each masked position to guide the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Masking**\n",
    "\n",
    "There are several ways to deprive the model of one or more tokens in the input sequence during training:\n",
    "\n",
    "- **Masking**: Replace a token with a special `[MASK]` token and train the model to recover the original token.\n",
    "\n",
    "- **Random corruption**: Replace a token with a **random token** from the vocabulary and train the model to recover the correct one.\n",
    "\n",
    "- This approach is known as **masked language modeling**, and it was used in the training of **BERT**.\n",
    "\n",
    "- In BERT, **15% of the input tokens** in each training sequence are selected for masking:\n",
    "\n",
    "  - **80%** of the selected tokens are replaced with `[MASK]`\n",
    "  \n",
    "  - **10%** are replaced with a randomly selected token\n",
    "  - **10%** are **left unchanged**\n",
    "\n",
    "This strategy helps the model learn to make predictions in varied and realistic contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/BERT-MLM.png)\n",
    "\n",
    "<!-- <img src=\"img/BERT-MLM.png\" width=\"600\" height=\"600\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contextual embeddings**\n",
    "\n",
    "The representations created by Transformer models are called **contextual embeddings**.\n",
    "\n",
    "- Earlier methods like **word2vec** learned a **single static vector** for each word $w$ in the vocabulary, regardless of its context.\n",
    "\n",
    "- In contrast, **Transformer-based models**, whether causal (like GPT) or masked (like BERT), generate **contextual representations**, each word is represented by a **different vector** depending on its surrounding context.\n",
    "\n",
    "- The key difference:\n",
    "\n",
    "  - **Causal models** (e.g., GPT) use **left-side context** to build the representation.\n",
    "    \n",
    "  - **Masked language models** (e.g., BERT) use **both left and right context**, thanks to bidirectional attention.\n",
    "\n",
    "![Contextual representations](../img/contextual-representation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT model parameters**\n",
    "\n",
    "The original **BERT** model (Bidirectional Encoder Representations from Transformers) was a **bidirectional transformer encoder** with the following specifications:\n",
    "\n",
    "- **Training data**:  \n",
    "  - **800 million words** from a book corpus called **BooksCorpus**\n",
    "    \n",
    "  - **2.5 billion words** from the **English Wikipedia**  \n",
    "  - Total corpus size: **3.3 billion words**\n",
    "\n",
    "- **Hidden size**: 768  \n",
    "  - (Recall from DSCI 563 Lab 1: the sentence embeddings you used were also **768-dimensional**, as they came from a BERT-based model.)\n",
    "\n",
    "- **Transformer architecture**:\n",
    "  - **12 Transformer layers** (also called encoder blocks)\n",
    "    \n",
    "  - Each layer includes **12 attention heads** (multi-head attention)\n",
    "\n",
    "- The total model size is over **100 million parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoder-decoder architecture (high-level)\n",
    "\n",
    "- There are tasks such as machine translation or text summarization, where a combination of encoder and decoder architecture is beneficial.\n",
    "\n",
    "- For instance, when translating from English to Spanish, it would be useful to get contextual representations of the English sentence and then autoregressively generate the Spanish sentence. \n",
    "\n",
    "![](../img/encoder-decoder-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the encoder-decoder transformer architecture, the encoder uses the\n",
    "transformer blocks we saw in the previous lecture.\n",
    "\n",
    "- The decoder uses a more powerful block with an extra cross-attention layer that can attend to all the encoder words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/encoder-decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interim summary**\n",
    "\n",
    "The table below summarizes the key differences between the three main types of Transformer-based language models.\n",
    "\n",
    "| Feature                        | Decoder-Only (e.g., GPT-3)                                            | Encoder-Only (e.g., BERT, RoBERTa)                                       | Encoder-Decoder (e.g., T5, BART)                                |\n",
    "|-------------------------------|------------------------------------------------------------------------|---------------------------------------------------------------------------|------------------------------------------------------------------|\n",
    "| **Contextual Embedding Direction** | Unidirectional (left-to-right)                                      | Bidirectional                                                             | Bidirectional (encoder) + unidirectional (decoder)              |\n",
    "| **Output Computation Based On**    | Only previous (left) context                                         | Full context (both left and right)                                        | Encoded representation of the full input                        |\n",
    "| **Text Generation Capability**     | Naturally suited for text generation                                 | Not designed for direct text generation                                   | Can generate text (e.g., translation, summarization)            |\n",
    "| **Example**                         | *MDS Cohort 9 is the ___*                                            | *MDS Cohort 9 is the best!* → positive sentiment                          | *Input*: Translate to Mandarin: MDS Cohort 9 is the best!<br>*Output*: MDS 第九期是最棒的! |\n",
    "| **Typical Use Cases**              | Autoregressive generation, language modeling                         | Classification, sequence labeling, embedding extraction                   | Text-to-text tasks (e.g., translation, summarization, rewriting)|\n",
    "| **Contextual Embeddings**          | Unidirectional contextual embeddings and token distributions         | Bidirectional contextual embeddings                                       | Encoder: bidirectional<br>Decoder: unidirectional               |\n",
    "| **Sequence Processing**            | Given a prompt $X_{1:i}$, predicts $X_{i+1}$ to $X_L$                | Embeddings used for analysis, not generation                              | Encode input sequence, then decode output step-by-step          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is just a high-level introduction of common transformer architectures. \n",
    "- There are many things related to transformers which we have not covered. Refer to the linked resources from lecture 7 if you want to learn more.  \n",
    "- In particular, go through these chapters from Jurafsky & Martin book: [Chapter 10](https://web.stanford.edu/~jurafsky/slp3/10.pdf), [Chapter 11](https://web.stanford.edu/~jurafsky/slp3/11.pdf), [Chapter 12](https://web.stanford.edu/~jurafsky/slp3/12.pdf), [Chapter 14](https://web.stanford.edu/~jurafsky/slp3/14.pdf)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) is an excellent resource. \n",
    "- Transformers are not only for NLP. They have been successfully applied in many other domains often with state-of-the-art results. For example, \n",
    "    - [Vision Transformers](https://arxiv.org/pdf/2010.11929.pdf)\n",
    "    - Bioinformatics: See [this](https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology) and [this](http://people.csail.mit.edu/tommi/papers/Ingraham_etal_neurips19.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing transformer blocks with PyTorch**\n",
    "\n",
    "To implement Transformer models using PyTorch, the following classes are particularly useful:\n",
    "\n",
    "- [`TransformerEncoderLayer`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html):  \n",
    "    - Used for **encoder-only architectures**. Combines multi-head self-attention with a feedforward neural network, residual connections, and layer normalization.\n",
    "\n",
    "- [`TransformerDecoderLayer`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html):  \n",
    "  Used for both:\n",
    "  - **Decoder-only models**, where it includes **masked self-attention** and feedforward layers.\n",
    "    \n",
    "  - **Encoder-decoder models**, where it includes **masked self-attention**, **cross-attention** (attending to encoder outputs), and a feedforward layer.\n",
    "\n",
    "- [`TransformerDecoder`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html):  \n",
    "  Stacks multiple decoder layers to form a complete decoder module. In encoder-decoder architectures, it takes:\n",
    "  - A decoder input sequence\n",
    "    \n",
    "  - A `memory` input (i.e., the encoder’s output)\n",
    "\n",
    "- [`MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html):  \n",
    "  A lower-level component that computes attention across different heads. It enables the model to attend to information from multiple representation subspaces at different positions.  \n",
    "  Use this class if you're building a Transformer from scratch or modifying attention behaviour manually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final comments and summary\n",
    "\n",
    "The Transformer architecture is modular and adaptable across a wide range of NLP tasks, from classification to generation.\n",
    "\n",
    "- We explored three types of language models built with Transformers:\n",
    "  - **Decoder-only** (e.g., GPT): Good for text generation with unidirectional attention.\n",
    "    \n",
    "  - **Encoder-only** (e.g., BERT): Used for classification and embedding tasks with bidirectional attention.\n",
    "  - **Encoder-decoder** (e.g., T5, BART): Ideal for tasks that require input-to-output transformations, like translation.\n",
    "\n",
    "- **Training objectives**:\n",
    "  - Causal models use **next-word prediction**.\n",
    "    \n",
    "  - Encoder-only models like BERT use **masked language modeling (MLM)** or the **cloze task**.\n",
    "\n",
    "- **Contextual embeddings**:\n",
    "  - All Transformer-based models produce **contextual embeddings**—the vector for a word depends on its context.\n",
    "    \n",
    "  - BERT uses **bidirectional context**, GPT uses **left-side context only**.\n",
    "\n",
    "- **PyTorch tools for implementation**:\n",
    "  - `MultiheadAttention` for building custom attention.\n",
    "    \n",
    "  - `TransformerEncoderLayer`, `TransformerDecoderLayer`, and `TransformerDecoder` for composing full models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Resources\n",
    "Attention-mechanisms and transformers are quite new. But there are many resources on transformers. I'm listing a few resources here. \n",
    "\n",
    "- [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Transformers](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)\n",
    "- 3Blue1Brown has recently released some videos on transformers\n",
    "    - [But what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning](https://www.youtube.com/watch?v=wjZofJX0v4M)\n",
    "    - [Attention in transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Transformers documentation](https://huggingface.co/transformers/index.html)\n",
    "- [A funny video: I taught an AI to make pasta](https://www.youtube.com/watch?v=Y_NvR5dIaOY)\n",
    "- [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n",
    "- Relevant chapters from Jurafsky & Martin book: [Chapter 10](https://web.stanford.edu/~jurafsky/slp3/10.pdf), [Chapter 11](https://web.stanford.edu/~jurafsky/slp3/11.pdf), [Chapter 12](https://web.stanford.edu/~jurafsky/slp3/12.pdf), [Chapter 14](https://web.stanford.edu/~jurafsky/slp3/14.pdf)\n",
    "\n",
    "<br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
