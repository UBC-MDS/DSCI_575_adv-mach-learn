{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to `torchtext`\n",
    "\n",
    "**Attributions: This tutorial is directly taken from and adapted a bit from MDS-CL materials.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`torchtext`](https://pytorch.org/text/stable/index.html) is a python library which helps you easily process text data for NLP task such as read text file from disk, tokenize the text, convert the text to lists of integers, and pad sequences in a batch.\n",
    "\n",
    "You can then install torchtext using `pip`:\n",
    "\n",
    "`pip3 install torchtext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchtext` takes in raw data in the form of text files, such as CSV, TSV, or JSON files, and converts them to `torchtext.data.Datasets`. \n",
    "\n",
    "`torchtext` then passes the `Dataset` to an `Iterator`. Iterators handle numericalizing, batching, packaging, and moving the data to the given device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/torchtext.png\" height=\"800\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For demonstration purpose, let's consider a sample of size 100 from [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>ham</td>\n",
       "      <td>And how's your husband.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5287</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hey ! Don't forget ... You are MINE ... For ME...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4153</th>\n",
       "      <td>ham</td>\n",
       "      <td>Haf u eaten? Wat time u wan me 2 come?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>ham</td>\n",
       "      <td>But i'll b going 2 sch on mon. My sis need 2 t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>ham</td>\n",
       "      <td>I know a few people I can hit up and fuck to t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                                sms\n",
       "1837   ham                            And how's your husband.\n",
       "5287   ham  Hey ! Don't forget ... You are MINE ... For ME...\n",
       "4153   ham             Haf u eaten? Wat time u wan me 2 come?\n",
       "2000   ham  But i'll b going 2 sch on mon. My sis need 2 t...\n",
       "2184   ham  I know a few people I can hit up and fuck to t..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sms_df_all = pd.read_csv(\"data/spam.csv\", encoding=\"latin-1\")\n",
    "sms_df_all = sms_df_all.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "sms_df_all = sms_df_all.rename(columns={\"v1\": \"label\", \"v2\": \"sms\"})\n",
    "sms_df = sms_df_all.sample(100)\n",
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchtext` needs train, valid, and test data into three separate files. So let's use `sklearn` to split data set into train, validation, and test sets and write three CSV files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation_test = train_test_split(sms_df, test_size=0.2, random_state=123)\n",
    "validation, test = train_test_split(validation_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (80, 2)\n",
      "Validation set shape: (10, 2)\n",
      "Test set shape: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set shape:\", train.shape)\n",
    "print(\"Validation set shape:\", validation.shape)\n",
    "print(\"Test set shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new directory to save split datasets.\n",
    "import os\n",
    "\n",
    "data_path = \"./data/split/\"\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =['sms','label']\n",
    "train.to_csv(data_path + \"train.csv\", columns = cols, index=False)\n",
    "validation.to_csv(data_path + \"val.csv\", columns = cols, index=False)\n",
    "test.to_csv(data_path + \"test.csv\", columns = cols, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring the fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchtext` takes a declarative approach to load its data: you tell `torchtext` how you want the data to look like, and `torchtext` handles it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import related packages\n",
    "import nltk\n",
    "import torch\n",
    "import torchtext\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from torchtext.legacy.data import (\n",
    "    BucketIterator,\n",
    "    Field,\n",
    "    Iterator,\n",
    "    LabelField,\n",
    "    TabularDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define `tokenize_nltk` function for tokenization. It will be passed to `TorchText` and take in the sentence as a string and return the sentence as a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_nltk(text):\n",
    "    \"\"\"\n",
    "    Simple tokenization on white spaces.\n",
    "    \"\"\"\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', '!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_nltk(\"This is a test! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchtext's` Fields handle how data should be processed. You can read all of the possible arguments [here](https://torchtext.readthedocs.io/en/latest/data.html#field). These arguments load and process the input data appropriately.\n",
    "\n",
    "- **For the sms text**, we pass in the preprocessing we want the field to do as keyword arguments. We give it the tokenizer we want the field to use, tell it to convert the input to lowercase, and also tell it the input is sequential.\n",
    "\n",
    "- **For the label input**, it is not sequential input and doesn't need unknown token for out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, tokenize=tokenize_nltk, lower=True)\n",
    "LABEL = Field(sequential=False, unk_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields know what to do when given raw data. Now, we need to tell the fields what data it should work on. This is where we use Datasets. To process the tsv file, we use `TabularDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = TabularDataset.splits(\n",
    "    path=data_path,  # the root directory where the data lies\n",
    "    train=\"train.csv\",\n",
    "    validation=\"val.csv\",\n",
    "    test=\"test.csv\", \n",
    "    format=\"csv\",\n",
    "    skip_header=True,\n",
    "    fields=[(\"sms\", TEXT), (\"label\", LABEL)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the TabularDataset, we pass in a list of `(name, field)` pairs as the fields argument. The fields we pass in must be in the same order as the columns.\n",
    "\n",
    "The splits method creates three datasets for the train, validation, and test data by applying the same processing. We give the the root directory of datasets and corresponding names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset can be treated as a list-like dataset and can be indexed and iterated over like normal lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.legacy.data.example.Example object at 0x7f8ba13c91c0>\n"
     ]
    }
   ],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an `Example object`. Each `Example object` includes the attributes of a single data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sms', 'label'])\n",
      "['x', 'course', 'it', '2yrs', '.', 'just', 'so', 'her', 'messages', 'on', 'messenger', 'lik', 'you', 'r', 'sending', 'me']\n",
      "ham\n"
     ]
    }
   ],
   "source": [
    "print(train[0].__dict__.keys())\n",
    "print(train[0].sms)\n",
    "print(train[0].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to build our vocabulary to map words to integers. We'll use vocabulary from GloVe embeddings. When you run the following for the first time, it'll download the word embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, min_freq=2, vectors='glove.6B.100d')\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let us take a look at the vocabulary of sms texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 66),\n",
       " ('i', 50),\n",
       " ('...', 34),\n",
       " ('you', 27),\n",
       " ('to', 27),\n",
       " (',', 19),\n",
       " ('a', 18),\n",
       " ('u', 17),\n",
       " ('?', 14),\n",
       " ('!', 14),\n",
       " (':', 14),\n",
       " ('&', 13),\n",
       " ('call', 13),\n",
       " ('my', 13),\n",
       " ('2', 13),\n",
       " (';', 12),\n",
       " ('*', 12),\n",
       " (\"'m\", 11),\n",
       " ('the', 11),\n",
       " ('me', 10)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the vocabulary of labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show an example to convert text strings to tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed sent1:  ['i', 'like', 'to', 'watch', 'the', 'sunset', '.']\n",
      "preprocessed sent2:  ['be', 'calm', '.']\n",
      "tensor([[ 3, 46],\n",
      "        [ 0,  0],\n",
      "        [ 5,  2],\n",
      "        [ 0,  1],\n",
      "        [20,  1],\n",
      "        [ 0,  1],\n",
      "        [ 2,  1]])\n",
      "torch.Size([7, 2])\n"
     ]
    }
   ],
   "source": [
    "sent1 = TEXT.preprocess(\"I like to watch the sunset.\")\n",
    "print('preprocessed sent1: ', sent1)\n",
    "sent2 = TEXT.preprocess(\"Be calm.\")\n",
    "print('preprocessed sent2: ', sent2)\n",
    "# convert tokens to tensor\n",
    "tensor = TEXT.process([sent1, sent2])\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output tensor, second sentence is padded because it's shorter than the first one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the `Iterator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of preparing data is to create the iterators. \n",
    "Dataset can be iterated by iterator. At each step, the iterator generates a batch of data which will have a text attribute (the PyTorch tensors containing a batch of numericalized text) and a label attribute (the PyTorch tensors containing a batch of numericalized labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code for how you would initialize the Iterators for the train, validation, and test data, repsectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import BucketIterator, Iterator\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "    (\n",
    "        train,\n",
    "        val,\n",
    "        test,\n",
    "    ),  # we pass in the datasets we want the iterator to draw data from\n",
    "    batch_sizes=(4, 64, 64),  # batch size for Train, dev and Test, respectively.\n",
    "    sort_key=lambda x: len(x.sms),\n",
    "    sort=True,\n",
    "    # A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding.\n",
    "    sort_within_batch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `sort_within_batch` argument, when set to True, sorts the data within each minibatch in decreasing order according to the sort_key.\n",
    " \n",
    "The [`BucketIterator`](https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator) automatically shuffles and buckets the input sequences into sequences of similar length. In each batch,  we need to pad the input sequences to be of the same length to enable batch processing. The amount of padding necessary is determined by the longest sequence in the batch. Therefore, padding is most efficient when the sequences are of similar lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `Iterator`\n",
    "We use for-loop to load batch in a iterator. The iterator returns a custom datatype called `torchtext.data.batch.Batch`. The `Batch` object is a similar API to generate a batch samples from each field in the dataset as attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 4]\n",
      "\t[.sms]:[torch.LongTensor of size 5x4]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "    print(batch)\n",
    "    messages = batch.sms\n",
    "    labels = batch.label\n",
    "    break  # we use first batch as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages` and `labels` are tensors, you can use them to train any machine model with [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The messages tensor size is `[maximum sequence length * batch_size]`.\n",
    "\n",
    "The label tensor size is `[batch_size]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how `messages` tensor looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 51, 176,   0,   0],\n",
       "        [  0, 145,   0, 115],\n",
       "        [ 31,   0,   2,   1],\n",
       "        [ 24,   2,   1,   1],\n",
       "        [  0,   1,   1,   1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `label` tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We also can convert indexes to text tokens.**\n",
    "\n",
    "We convert these four samples back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed sentence: \n",
      "0  smaple: ['just', '<unk>', '..', 'and', '<unk>']\n",
      "1  smaple: ['yup', 'next', '<unk>', '.', '<pad>']\n",
      "2  smaple: ['<unk>', '<unk>', '.', '<pad>', '<pad>']\n",
      "3  smaple: ['<unk>', 'babe', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(\"processed sentence: \")\n",
    "for j in range(messages.shape[1]):  # sample loop\n",
    "    tmp = []  # create a output container\n",
    "    for i in range(messages.shape[0]):  # token loop\n",
    "        tmp.append(TEXT.vocab.itos[messages[i, j]])\n",
    "    print(j, \" smaple:\", tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the first 3 samples are padded to length of 7. The out-of-vocabulary tokens are replaced with `<unk>` token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do the orignal samples look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in range(len(train)):\n",
    "    samples.append(train.examples[i].sms)\n",
    "samples.sort(key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\\\\alright', 'babe'],\n",
       " ['say', 'thanks2', '.'],\n",
       " ['yup', 'next', 'stop', '.'],\n",
       " ['just', 'sleeping', '..', 'and', 'surfing']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "* https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "* https://medium.com/@adam.wearne/lets-get-sentimental-with-pytorch-dcdd9e1ea4c9\n",
    "* https://spacy.io/\n",
    "* https://www.aclweb.org/anthology/P17-1067.pdf\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "* https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "* https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "* https://pytorch.org/tutorials/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
