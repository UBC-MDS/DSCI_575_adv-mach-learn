{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/575_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 5: Topic Modeling\n",
    "\n",
    "UBC Master of Data Science program, 2021-22\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture plan, imports, LO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Lecture plan \n",
    "\n",
    "- Activity and motivation (~10 mins)\n",
    "- Introduction and intuition (~15 mins)\n",
    "- LDA (~15 mins)\n",
    "- Break (~5 mins)\n",
    "- iClicker and Q&A (~5 mins)\n",
    "- Topic modeling with Python (~15 mins)\n",
    "- Selecting number of topics (~5 mins)\n",
    "- Final comments and summary (~2 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "import string\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"code/.\")\n",
    "from utils import *\n",
    "\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "from gensim import corpora, matutils, models\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learning outcomes\n",
    "\n",
    "From this lesson you will be able to\n",
    "\n",
    "- Explain the general idea of topic modeling.\n",
    "- Name the two main approaches for topic modeling. \n",
    "- Explain the difference between document clustering and topic modeling.  \n",
    "- Broadly explain the difference between LSA and LDA.\n",
    "- Broadly explain the data generation process given an LDA topic model.  \n",
    "- Explain at a high level how topic assignments are updated in the LDA model. \n",
    "- Explain the importance of preprocessing in topic modeling. \n",
    "- Carry out topic modeling by training an [LDA model with gensim](https://radimrehurek.com/gensim/models/ldamodel.html). \n",
    "- Visualize topics identified by topic modeling using `pyLDAvis`. \n",
    "- Interpret the output of topic modeling. \n",
    "- Name a few possible ways to evaluate a topic model. \n",
    "- Explore coherence scores to pick the number of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling introduction activity (~5 mins)\n",
    "\n",
    "- Consider the following documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fashion model at famous probabilistic topic model conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresh elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>famous elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probabilistic conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>creative probabilistic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model diet apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>probabilistic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kiwi health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fresh apple kiwi health diet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fresh apple kiwi juice nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>probabilistic topic model conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>probabilistic topi model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            text\n",
       "0                                           famous fashion model\n",
       "1                                          elegant fashion model\n",
       "2   fashion model at famous probabilistic topic model conference\n",
       "3                                    fresh elegant fashion model\n",
       "4                                  famous elegant fashion model \n",
       "5                                       probabilistic conference\n",
       "6                                   creative probabilistic model\n",
       "7                                model diet apple kiwi nutrition\n",
       "8                                            probabilistic model\n",
       "9                                          kiwi health nutrition\n",
       "10                                 fresh apple kiwi health diet \n",
       "11                                              health nutrition\n",
       "12                              fresh apple kiwi juice nutrition\n",
       "13                          probabilistic topic model conference\n",
       "14                                      probabilistic topi model"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_clustering = pd.read_csv(\"data/toy_clustering.csv\")\n",
    "toy_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss the following questions with your neighbour and write your answers in this [Google doc]( \n",
    "https://docs.google.com/document/d/1iEZyA6YkcsSNLbmCnuZLUfyF8EBqPC4YE_R8lkXOdpM/edit?usp=sharing\n",
    ")**. \n",
    "1. Suppose you are asked to cluster these documents manually. How many clusters would you identify?\n",
    "2. What would be the prominent words in each cluster?\n",
    "3. Which words would occur in more than one cluster? \n",
    "4. Do we have to have only one cluster assignment per document? Are there documents which are a mixture of multiple clusters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why topic modeling? \n",
    "\n",
    "- Suppose you have a large collection of documents on a variety of topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: A corpus of news articles \n",
    "\n",
    "![](img/TM_NYT_articles.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_NYT_articles.png\" height=\"2000\" width=\"2000\">  -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: A corpus of food magazines \n",
    "\n",
    "![](img/TM_food_magazines.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_food_magazines.png\" height=\"2000\" width=\"2000\">  -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A corpus of scientific articles\n",
    "\n",
    "![](img/TM_science_articles.png)\n",
    "\n",
    "<!-- <img src=\"img/TM_science_articles.png\" height=\"2000\" width=\"2000\">  -->\n",
    "\n",
    "(Credit: [Dave Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling motivation\n",
    "\n",
    "- Humans are pretty good at reading and understanding a document and answering questions such as \n",
    "    - What is it about?  \n",
    "    - Which documents is it related to?     \n",
    "- But for a large collection of documents it would take years to read all documents and organize and categorize them so that they are easy to search.\n",
    "- You need an automated way\n",
    "    - to get an idea of what's going on in the data or \n",
    "    - to pull documents related to a certain topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling\n",
    "\n",
    "- Topic modeling gives you an ability to summarize the major themes in a large collection of documents (corpus). \n",
    "    - Example: The major themes in a collection of news articles could be \n",
    "        - **politics**\n",
    "        - **entertainment**\n",
    "        - **sports**\n",
    "        - **technology**\n",
    "        - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Topic modeling is a great EDA tool to get a sense of what's going on in a large corpus. \n",
    "- Some examples\n",
    "    - If you want to pull documents related to a particular lawsuit. \n",
    "    - You want to examine people's sentiment towards a particular candidate and/or political party and so you want to pull tweets or Facebook posts related to election.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: Input \n",
    "\n",
    "![](img/TM_science_articles.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_science_articles.png\" height=\"2000\" width=\"2000\">  -->\n",
    "<!-- </center>     -->\n",
    "    \n",
    "Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: output\n",
    "\n",
    "![](img/TM_topics.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_topics.png\" height=\"900\" width=\"900\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "\n",
    "(Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: output with interpretation\n",
    "- Assigning labels is a human thing. \n",
    "\n",
    "![](img/TM_topics_with_labels.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_topics_with_labels.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA topics in Yale Law Journal\n",
    "\n",
    "![](img/TM_yale_law_journal.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_yale_law_journal.png\" height=\"1500\" width=\"1500\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [David Blei's paper](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA topics in social media\n",
    "\n",
    "![](img/TM_health_topics_social_media.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_health_topics_social_media.png\" height=\"1300\" width=\"1300\">  -->\n",
    "<!-- </center> -->\n",
    "\n",
    "(Credit: [Health topics in social media](https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0103408.g002))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition and introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: Input and output\n",
    "\n",
    "- Input\n",
    "    - A large collection of documents\n",
    "    - A value for the hyperparameter $K$ (e.g., $K = 3$)\n",
    "- Output\n",
    "    1. Topic-words association \n",
    "        - For each topic, what words describe that topic? \n",
    "    2. Document-topics association\n",
    "        - For each document, what topics are expressed by the document? \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: Example\n",
    "\n",
    "- Topic-words association \n",
    "    - For each topic, what words describe that topic?  \n",
    "    - **A topic is a mixture of words.** \n",
    "\n",
    "![](img/topic_modeling_word_topics.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_word_topics.png\" height=\"1000\" width=\"1000\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: Example\n",
    "\n",
    "- Document-topics association \n",
    "    - For each document, what topics are expressed by the document?\n",
    "    - **A document is a mixture of topics.** \n",
    "    \n",
    "![](img/topic_modeling_doc_topics.png)\n",
    "\n",
    "<!-- <center>     -->\n",
    "<!-- <img src=\"img/topic_modeling_doc_topics.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: Input and output\n",
    "\n",
    "- Input\n",
    "    - A large collection of documents\n",
    "    - A value for the hyperparameter $K$ (e.g., $K = 3$)\n",
    "- Output\n",
    "    - For each topic, what words describe that topic?  \n",
    "    - For each document, what topics are expressed by the document?\n",
    "\n",
    "![](img/topic_modeling_output.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_output.png\" height=\"1000\" width=\"1000\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you do topic modeling? \n",
    "\n",
    "- A common tool to solve such problems is unsupervised ML methods.\n",
    "- Given the hyperparameter $K$, the idea of topic modeling is to describe a set of documents using $K$ \"topics\"\n",
    "- Two main approaches \n",
    "    - Topic modeling as matrix factorization (LSA)\n",
    "    - Latent Dirichlet Allocation (LDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling as matrix factorization\n",
    "\n",
    "- We have seen this before in DSCI 563! \n",
    "- You can think of topic modeling as a matrix factorization problem. \n",
    "\n",
    "$$X_{n \\times d} \\approx Z_{n \\times k}W_{k \\times d}$$\n",
    "\n",
    "- Where \n",
    "    - $n \\rightarrow $ Number of documents\n",
    "    - $k \\rightarrow $ Number of topics\n",
    "    - $d \\rightarrow $ Number of features (e.g., the size of vocabulary)\n",
    "\n",
    "- $Z$ gives us document-topic assignments and $W$ gives us topic-word assignments.    \n",
    "- This is LSA and we used [`TruncatedSVD`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) for this matrix factorization in `sklearn` \n",
    "- The idea is to reduce the dimensionality of the data to some semantically meaningful components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fashion model at famous probabilistic topic model conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresh elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>famous elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probabilistic conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>creative probabilistic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model diet apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>probabilistic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kiwi health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fresh apple kiwi health diet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fresh apple kiwi juice nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>probabilistic topic model conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>probabilistic topi model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            text\n",
       "0                                           famous fashion model\n",
       "1                                          elegant fashion model\n",
       "2   fashion model at famous probabilistic topic model conference\n",
       "3                                    fresh elegant fashion model\n",
       "4                                  famous elegant fashion model \n",
       "5                                       probabilistic conference\n",
       "6                                   creative probabilistic model\n",
       "7                                model diet apple kiwi nutrition\n",
       "8                                            probabilistic model\n",
       "9                                          kiwi health nutrition\n",
       "10                                 fresh apple kiwi health diet \n",
       "11                                              health nutrition\n",
       "12                              fresh apple kiwi juice nutrition\n",
       "13                          probabilistic topic model conference\n",
       "14                                      probabilistic topi model"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df = pd.read_csv(\"data/toy_clustering.csv\")\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get BOW representation of the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>conference</th>\n",
       "      <th>creative</th>\n",
       "      <th>diet</th>\n",
       "      <th>elegant</th>\n",
       "      <th>famous</th>\n",
       "      <th>fashion</th>\n",
       "      <th>fresh</th>\n",
       "      <th>health</th>\n",
       "      <th>juice</th>\n",
       "      <th>kiwi</th>\n",
       "      <th>model</th>\n",
       "      <th>nutrition</th>\n",
       "      <th>probabilistic</th>\n",
       "      <th>topi</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>famous fashion model</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elegant fashion model</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fashion model at famous probabilistic topic model conference</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fresh elegant fashion model</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>famous elegant fashion model</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probabilistic conference</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creative probabilistic model</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model diet apple kiwi nutrition</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probabilistic model</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kiwi health nutrition</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fresh apple kiwi health diet</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health nutrition</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fresh apple kiwi juice nutrition</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probabilistic topic model conference</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probabilistic topi model</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              apple  \\\n",
       "text                                                                  \n",
       "famous fashion model                                              0   \n",
       "elegant fashion model                                             0   \n",
       "fashion model at famous probabilistic topic model conference      0   \n",
       "fresh elegant fashion model                                       0   \n",
       "famous elegant fashion model                                      0   \n",
       "probabilistic conference                                          0   \n",
       "creative probabilistic model                                      0   \n",
       "model diet apple kiwi nutrition                                   1   \n",
       "probabilistic model                                               0   \n",
       "kiwi health nutrition                                             0   \n",
       "fresh apple kiwi health diet                                      1   \n",
       "health nutrition                                                  0   \n",
       "fresh apple kiwi juice nutrition                                  1   \n",
       "probabilistic topic model conference                              0   \n",
       "probabilistic topi model                                          0   \n",
       "\n",
       "                                                              conference  \\\n",
       "text                                                                       \n",
       "famous fashion model                                                   0   \n",
       "elegant fashion model                                                  0   \n",
       "fashion model at famous probabilistic topic model conference           1   \n",
       "fresh elegant fashion model                                            0   \n",
       "famous elegant fashion model                                           0   \n",
       "probabilistic conference                                               1   \n",
       "creative probabilistic model                                           0   \n",
       "model diet apple kiwi nutrition                                        0   \n",
       "probabilistic model                                                    0   \n",
       "kiwi health nutrition                                                  0   \n",
       "fresh apple kiwi health diet                                           0   \n",
       "health nutrition                                                       0   \n",
       "fresh apple kiwi juice nutrition                                       0   \n",
       "probabilistic topic model conference                                   1   \n",
       "probabilistic topi model                                               0   \n",
       "\n",
       "                                                              creative  diet  \\\n",
       "text                                                                           \n",
       "famous fashion model                                                 0     0   \n",
       "elegant fashion model                                                0     0   \n",
       "fashion model at famous probabilistic topic model conference         0     0   \n",
       "fresh elegant fashion model                                          0     0   \n",
       "famous elegant fashion model                                         0     0   \n",
       "probabilistic conference                                             0     0   \n",
       "creative probabilistic model                                         1     0   \n",
       "model diet apple kiwi nutrition                                      0     1   \n",
       "probabilistic model                                                  0     0   \n",
       "kiwi health nutrition                                                0     0   \n",
       "fresh apple kiwi health diet                                         0     1   \n",
       "health nutrition                                                     0     0   \n",
       "fresh apple kiwi juice nutrition                                     0     0   \n",
       "probabilistic topic model conference                                 0     0   \n",
       "probabilistic topi model                                             0     0   \n",
       "\n",
       "                                                              elegant  famous  \\\n",
       "text                                                                            \n",
       "famous fashion model                                                0       1   \n",
       "elegant fashion model                                               1       0   \n",
       "fashion model at famous probabilistic topic model conference        0       1   \n",
       "fresh elegant fashion model                                         1       0   \n",
       "famous elegant fashion model                                        1       1   \n",
       "probabilistic conference                                            0       0   \n",
       "creative probabilistic model                                        0       0   \n",
       "model diet apple kiwi nutrition                                     0       0   \n",
       "probabilistic model                                                 0       0   \n",
       "kiwi health nutrition                                               0       0   \n",
       "fresh apple kiwi health diet                                        0       0   \n",
       "health nutrition                                                    0       0   \n",
       "fresh apple kiwi juice nutrition                                    0       0   \n",
       "probabilistic topic model conference                                0       0   \n",
       "probabilistic topi model                                            0       0   \n",
       "\n",
       "                                                              fashion  fresh  \\\n",
       "text                                                                           \n",
       "famous fashion model                                                1      0   \n",
       "elegant fashion model                                               1      0   \n",
       "fashion model at famous probabilistic topic model conference        1      0   \n",
       "fresh elegant fashion model                                         1      1   \n",
       "famous elegant fashion model                                        1      0   \n",
       "probabilistic conference                                            0      0   \n",
       "creative probabilistic model                                        0      0   \n",
       "model diet apple kiwi nutrition                                     0      0   \n",
       "probabilistic model                                                 0      0   \n",
       "kiwi health nutrition                                               0      0   \n",
       "fresh apple kiwi health diet                                        0      1   \n",
       "health nutrition                                                    0      0   \n",
       "fresh apple kiwi juice nutrition                                    0      1   \n",
       "probabilistic topic model conference                                0      0   \n",
       "probabilistic topi model                                            0      0   \n",
       "\n",
       "                                                              health  juice  \\\n",
       "text                                                                          \n",
       "famous fashion model                                               0      0   \n",
       "elegant fashion model                                              0      0   \n",
       "fashion model at famous probabilistic topic model conference       0      0   \n",
       "fresh elegant fashion model                                        0      0   \n",
       "famous elegant fashion model                                       0      0   \n",
       "probabilistic conference                                           0      0   \n",
       "creative probabilistic model                                       0      0   \n",
       "model diet apple kiwi nutrition                                    0      0   \n",
       "probabilistic model                                                0      0   \n",
       "kiwi health nutrition                                              1      0   \n",
       "fresh apple kiwi health diet                                       1      0   \n",
       "health nutrition                                                   1      0   \n",
       "fresh apple kiwi juice nutrition                                   0      1   \n",
       "probabilistic topic model conference                               0      0   \n",
       "probabilistic topi model                                           0      0   \n",
       "\n",
       "                                                              kiwi  model  \\\n",
       "text                                                                        \n",
       "famous fashion model                                             0      1   \n",
       "elegant fashion model                                            0      1   \n",
       "fashion model at famous probabilistic topic model conference     0      2   \n",
       "fresh elegant fashion model                                      0      1   \n",
       "famous elegant fashion model                                     0      1   \n",
       "probabilistic conference                                         0      0   \n",
       "creative probabilistic model                                     0      1   \n",
       "model diet apple kiwi nutrition                                  1      1   \n",
       "probabilistic model                                              0      1   \n",
       "kiwi health nutrition                                            1      0   \n",
       "fresh apple kiwi health diet                                     1      0   \n",
       "health nutrition                                                 0      0   \n",
       "fresh apple kiwi juice nutrition                                 1      0   \n",
       "probabilistic topic model conference                             0      1   \n",
       "probabilistic topi model                                         0      1   \n",
       "\n",
       "                                                              nutrition  \\\n",
       "text                                                                      \n",
       "famous fashion model                                                  0   \n",
       "elegant fashion model                                                 0   \n",
       "fashion model at famous probabilistic topic model conference          0   \n",
       "fresh elegant fashion model                                           0   \n",
       "famous elegant fashion model                                          0   \n",
       "probabilistic conference                                              0   \n",
       "creative probabilistic model                                          0   \n",
       "model diet apple kiwi nutrition                                       1   \n",
       "probabilistic model                                                   0   \n",
       "kiwi health nutrition                                                 1   \n",
       "fresh apple kiwi health diet                                          0   \n",
       "health nutrition                                                      1   \n",
       "fresh apple kiwi juice nutrition                                      1   \n",
       "probabilistic topic model conference                                  0   \n",
       "probabilistic topi model                                              0   \n",
       "\n",
       "                                                              probabilistic  \\\n",
       "text                                                                          \n",
       "famous fashion model                                                      0   \n",
       "elegant fashion model                                                     0   \n",
       "fashion model at famous probabilistic topic model conference              1   \n",
       "fresh elegant fashion model                                               0   \n",
       "famous elegant fashion model                                              0   \n",
       "probabilistic conference                                                  1   \n",
       "creative probabilistic model                                              1   \n",
       "model diet apple kiwi nutrition                                           0   \n",
       "probabilistic model                                                       1   \n",
       "kiwi health nutrition                                                     0   \n",
       "fresh apple kiwi health diet                                              0   \n",
       "health nutrition                                                          0   \n",
       "fresh apple kiwi juice nutrition                                          0   \n",
       "probabilistic topic model conference                                      1   \n",
       "probabilistic topi model                                                  1   \n",
       "\n",
       "                                                              topi  topic  \n",
       "text                                                                       \n",
       "famous fashion model                                             0      0  \n",
       "elegant fashion model                                            0      0  \n",
       "fashion model at famous probabilistic topic model conference     0      1  \n",
       "fresh elegant fashion model                                      0      0  \n",
       "famous elegant fashion model                                     0      0  \n",
       "probabilistic conference                                         0      0  \n",
       "creative probabilistic model                                     0      0  \n",
       "model diet apple kiwi nutrition                                  0      0  \n",
       "probabilistic model                                              0      0  \n",
       "kiwi health nutrition                                            0      0  \n",
       "fresh apple kiwi health diet                                     0      0  \n",
       "health nutrition                                                 0      0  \n",
       "fresh apple kiwi juice nutrition                                 0      0  \n",
       "probabilistic topic model conference                             0      1  \n",
       "probabilistic topi model                                         1      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "bow = cv.fit_transform(toy_df[\"text\"]).toarray()\n",
    "bow_df = pd.DataFrame(bow, columns=cv.get_feature_names_out(), index=toy_df[\"text\"])\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can we extract these **latent features**, which are there in the data but haven't manifested yet? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lsa_pipe = make_pipeline(\n",
    "    CountVectorizer(stop_words=\"english\"), TruncatedSVD(n_components=3)\n",
    ")\n",
    "\n",
    "lsa_transformed = lsa_pipe.fit_transform(toy_df[\"text\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latent topic 1</th>\n",
       "      <th>latent topic 2</th>\n",
       "      <th>latent topic 3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>famous fashion model</th>\n",
       "      <td>1.3395</td>\n",
       "      <td>-0.1852</td>\n",
       "      <td>0.7211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elegant fashion model</th>\n",
       "      <td>1.2732</td>\n",
       "      <td>-0.1242</td>\n",
       "      <td>0.9512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fashion model at famous probabilistic topic model conference</th>\n",
       "      <td>2.8280</td>\n",
       "      <td>-0.5133</td>\n",
       "      <td>-0.3490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fresh elegant fashion model</th>\n",
       "      <td>1.3594</td>\n",
       "      <td>0.1891</td>\n",
       "      <td>1.0901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>famous elegant fashion model</th>\n",
       "      <td>1.5187</td>\n",
       "      <td>-0.1957</td>\n",
       "      <td>1.1627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probabilistic conference</th>\n",
       "      <td>0.5673</td>\n",
       "      <td>-0.2133</td>\n",
       "      <td>-0.8913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creative probabilistic model</th>\n",
       "      <td>1.1405</td>\n",
       "      <td>-0.1919</td>\n",
       "      <td>-0.6450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model diet apple kiwi nutrition</th>\n",
       "      <td>1.0110</td>\n",
       "      <td>1.6361</td>\n",
       "      <td>-0.1893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probabilistic model</th>\n",
       "      <td>1.0913</td>\n",
       "      <td>-0.1766</td>\n",
       "      <td>-0.5561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kiwi health nutrition</th>\n",
       "      <td>0.1729</td>\n",
       "      <td>1.2872</td>\n",
       "      <td>-0.1848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fresh apple kiwi health diet</th>\n",
       "      <td>0.3187</td>\n",
       "      <td>1.8663</td>\n",
       "      <td>-0.0410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health nutrition</th>\n",
       "      <td>0.0942</td>\n",
       "      <td>0.7554</td>\n",
       "      <td>-0.1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fresh apple kiwi juice nutrition</th>\n",
       "      <td>0.3187</td>\n",
       "      <td>1.8663</td>\n",
       "      <td>-0.0410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probabilistic topic model conference</th>\n",
       "      <td>1.4885</td>\n",
       "      <td>-0.3280</td>\n",
       "      <td>-1.0701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probabilistic topi model</th>\n",
       "      <td>1.1405</td>\n",
       "      <td>-0.1919</td>\n",
       "      <td>-0.6450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              latent topic 1   \\\n",
       "text                                                                            \n",
       "famous fashion model                                                   1.3395   \n",
       "elegant fashion model                                                  1.2732   \n",
       "fashion model at famous probabilistic topic model conference           2.8280   \n",
       "fresh elegant fashion model                                            1.3594   \n",
       "famous elegant fashion model                                           1.5187   \n",
       "probabilistic conference                                               0.5673   \n",
       "creative probabilistic model                                           1.1405   \n",
       "model diet apple kiwi nutrition                                        1.0110   \n",
       "probabilistic model                                                    1.0913   \n",
       "kiwi health nutrition                                                  0.1729   \n",
       "fresh apple kiwi health diet                                           0.3187   \n",
       "health nutrition                                                       0.0942   \n",
       "fresh apple kiwi juice nutrition                                       0.3187   \n",
       "probabilistic topic model conference                                   1.4885   \n",
       "probabilistic topi model                                               1.1405   \n",
       "\n",
       "                                                              latent topic 2  \\\n",
       "text                                                                           \n",
       "famous fashion model                                                 -0.1852   \n",
       "elegant fashion model                                                -0.1242   \n",
       "fashion model at famous probabilistic topic model conference         -0.5133   \n",
       "fresh elegant fashion model                                           0.1891   \n",
       "famous elegant fashion model                                         -0.1957   \n",
       "probabilistic conference                                             -0.2133   \n",
       "creative probabilistic model                                         -0.1919   \n",
       "model diet apple kiwi nutrition                                       1.6361   \n",
       "probabilistic model                                                  -0.1766   \n",
       "kiwi health nutrition                                                 1.2872   \n",
       "fresh apple kiwi health diet                                          1.8663   \n",
       "health nutrition                                                      0.7554   \n",
       "fresh apple kiwi juice nutrition                                      1.8663   \n",
       "probabilistic topic model conference                                 -0.3280   \n",
       "probabilistic topi model                                             -0.1919   \n",
       "\n",
       "                                                              latent topic 3  \n",
       "text                                                                          \n",
       "famous fashion model                                                  0.7211  \n",
       "elegant fashion model                                                 0.9512  \n",
       "fashion model at famous probabilistic topic model conference         -0.3490  \n",
       "fresh elegant fashion model                                           1.0901  \n",
       "famous elegant fashion model                                          1.1627  \n",
       "probabilistic conference                                             -0.8913  \n",
       "creative probabilistic model                                         -0.6450  \n",
       "model diet apple kiwi nutrition                                      -0.1893  \n",
       "probabilistic model                                                  -0.5561  \n",
       "kiwi health nutrition                                                -0.1848  \n",
       "fresh apple kiwi health diet                                         -0.0410  \n",
       "health nutrition                                                     -0.1220  \n",
       "fresh apple kiwi juice nutrition                                     -0.0410  \n",
       "probabilistic topic model conference                                 -1.0701  \n",
       "probabilistic topi model                                             -0.6450  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    np.round(lsa_transformed, 4),\n",
    "    columns=[\"latent topic 1 \", \"latent topic 2\", \"latent topic 3\"],\n",
    "    index=toy_df[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Latent topic 2 seems to be dominant in the fruit-related documents. \n",
    "- Latent topic 1 seems to be dominant in model-related documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the components learned by LSA. \n",
    "- How much variance is covered by these two components? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6388605814592865"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_pipe.named_steps[\"truncatedsvd\"].explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good to know! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our features are word counts. \n",
    "- Which features have higher weights in latent topic 1 vs. latent topic 2? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print these words in a nice format using `mglearn.tools.print_topics`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       \n",
      "--------      --------      --------      \n",
      "model         kiwi          fashion       \n",
      "fashion       nutrition     elegant       \n",
      "probabilistic apple         famous        \n",
      "famous        fresh         fresh         \n",
      "conference    health        model         \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(lsa_pipe.named_steps[\"truncatedsvd\"].components_, axis=1)[:, ::-1]\n",
    "\n",
    "feature_names = np.array(\n",
    "    lsa_pipe.named_steps[\"countvectorizer\"].get_feature_names_out()\n",
    ")\n",
    "print_topics(\n",
    "    topics=[0, 1, 2], feature_names=feature_names, sorting=sorting, n_words=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSA has learned three useful \"concepts\" or latent features from word count features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternative: Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "- A Bayesian, probabilistic, and generative approach  \n",
    "- Developed by [David Blei](http://www.cs.columbia.edu/~blei/) and colleagues in 2003. \n",
    "    * One of the most cited papers in the last 20 years.\n",
    "- DISCLAIMER    \n",
    "    - We won't go into the math because we do not have the time. \n",
    "    - My goal is to give you an intuition of the model and show you how to use it to solve topic modeling problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA high-level idea\n",
    "\n",
    "- Dirichlet distribution is a distribution of distributions. \n",
    "- In our case,\n",
    "    - Every document is a discrete probability distribution of topics. \n",
    "    - Every topic is a discrete probability distribution of words.\n",
    "    - So we are have distributions of distributions.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA: insight\n",
    "- Each document is a mixture of corpus-wide topics\n",
    "- Every topic is a mixture words\n",
    "\n",
    "![](img/TM_dist_topics_words_blei.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_dist_topics_words_blei.png\" height=\"1000\" width=\"1000\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## High-level intuition of LDA\n",
    "\n",
    "Attribution: Material and presentation in the next slides is adapted from [Jordan Boyd-Graber's excellent material on LDA](http://users.umiacs.umd.edu/~jbg/teaching/CMSC_726/16a.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generative, Bayesian, probabilistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generative story of LDA\n",
    "- The story that tells us how our data was generated. \n",
    "- The generative story of LDA to create Document 1 below:     \n",
    "    1. Pick a topic from the topic distribution for Document 1. \n",
    "    2. Pick a word from the selected topic's word distribution. \n",
    "- Not a realistic story but a mathematically useful story. \n",
    "\n",
    "![](img/topic_modeling_generative_story.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_generative_story.png\" height=\"600\" width=\"600\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal \n",
    "- Given a corpus of documents and the number of topics we want to learn \n",
    "    - the discrete probability distributions over topics for each document\n",
    "    - the discrete probability distributions over words for each topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Plate notation\n",
    "\n",
    "- Used in Bayesian inference for representing variables that repeat. \n",
    "- It shows the generative process of the LDA model. It also shows the dependency structure in the probability distribution.\n",
    "- We are not going into the details but I would like you to be familiar with this picture at a high-level because it's likely that you might see it in the context of topic modeling. \n",
    "\n",
    "![](img/topic_modeling_plate_diagram.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_plate_diagram.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "- $\\beta_k \\rightarrow$ Distribution over words for topic $k$\n",
    "- $\\theta_d \\rightarrow$ Distribution over topics for document $d$ \n",
    "- $w_n \\rightarrow$ word\n",
    "- $Z_n \\rightarrow$ topic\n",
    "- $N \\rightarrow$ Size of the vocabulary\n",
    "- $M \\rightarrow$ Number of documents\n",
    "- $\\lambda \\rightarrow$ Hyperparameter for word proportion\n",
    "- $\\alpha\\rightarrow$ Hyperparameter for topic proportion  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Optional) Mathematical presentation of the generative story (plate diagram)\n",
    "\n",
    "![](img/topic_modeling_plate_diagram.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_plate_diagram.png\" height=\"500\" width=\"500\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "\n",
    "- For each topic $k \\in \\{1, \\dots, K\\}$ draw a multinomial distribution $\\beta_k$ from a Dirichlet distribution with parameter $\\lambda$. \n",
    "- For each document $d \\in \\{1, \\dots, M\\}$, draw a multinomial distribution $\\theta_d$\n",
    "from a Dirichlet distribution with parameter $\\alpha$. \n",
    "- For each word position $n \\in \\{1, \\dots, N\\}$, select a hidden topic $Z_n$ from the multinomial distribution parameterized by $\\theta$.\n",
    "- Choose the observed word $w_n$ from the distribution $\\beta_{Z_n}$. \n",
    "\n",
    "[Source](http://users.umiacs.umd.edu/~jbg/teaching/CMSC_726/16a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Optional) LDA Inference\n",
    "\n",
    "- Infer the underlying topic structure in the documents. In particular, \n",
    "    - Learn the discrete probability distributions of topics in each document\n",
    "    - Learn the discrete probability distributions of words in each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Optional) LDA Inference\n",
    "\n",
    "- We are interested in the posterior distribution: $P(z, \\beta, \\theta| w_n, \\alpha, \\lambda)$\n",
    "- Observations: words. Everything else is hidden (latent). \n",
    "\n",
    "![](img/topic_modeling_plate_diagram.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_plate_diagram.png\" height=\"600\" width=\"600\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "\n",
    "- $\\lambda$: Hyperparameter for word proportion\n",
    "    - High $\\lambda$ &rarr; every topic contains a mixture of most of the words\n",
    "    - Low $\\lambda$ &rarr; every topic contains a mixture of only few words\n",
    "    \n",
    "- $\\alpha$: Hyperparameter for topic proportion  \n",
    "   - High $\\alpha$ &rarr; every document contains a mixture of most of the topics\n",
    "   - Low $\\alpha$ &rarr; every document is representative of only a few topics    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we find the posterior distribution? \n",
    "\n",
    "- We are interested in the posterior distribution of document topic assignments and word topic assignments. \n",
    "- How do we find it? \n",
    "    - **Gibbs sampling** (very accurate but very slow, appropriate for small datasets)\n",
    "    - Variational inference (faster but less accurate, extension of expectation maximization, appropriate for medium to large datasets)    \n",
    "    \n",
    "- Next let's look at an intuition of Gibbs sampling for topic modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm: Gibbs sampling \n",
    "\n",
    "- Sample topic assignments\n",
    "- Calculate conditional probability of single word topic assignment conditioned on the rest of the parameters. s \n",
    "- Two components\n",
    "    - How much this document likes the topic\n",
    "    - How much this topic likes the word\n",
    "\n",
    "![](img/topic_modeling_topic_word_assignment.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_topic_word_assignment.png\" height=\"700\" width=\"700\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Optional) Gibbs sampling equation: Calculating the conditional probability\n",
    "\n",
    "- Sets up a Markov chain that converges into the posterior distribution of the model parameters or word–topic assignments.\n",
    "- Two components\n",
    "    - How much this document likes topic $k$: \n",
    "    $$\\frac{n_{d,k} + \\alpha_k}{\\sum^K_i n_{d,i} + \\alpha_i}$$\n",
    "    - How much this topic likes word $w_{d,n}$: $$\\frac{V_{k, w_{d,n}} + \\lambda_{w_{d,n}}}{\\sum_i V_{k,i} + \\lambda_i}$$ \n",
    "- The conditional probability of word topic assignment given everything else in the model: \n",
    "\n",
    "$$\\frac{n_{d,k} + \\alpha_k}{\\sum^K_i n_{d,i} + \\alpha_i} \\frac{V_{k, w_{d,n}} + \\lambda_{w_{d,n}}}{\\sum_i V_{k,i} + \\lambda_i}$$\n",
    "\n",
    "- $n_{d,k} \\rightarrow$ number of times document $d$ uses topic $k$ \n",
    "- $V_{k, w_{d,n}} \\rightarrow$ number of times topic $k$ uses word type $w_{d,n}$\n",
    "- $\\alpha_k \\rightarrow$ Dirichlet parameter for document to topic distribution\n",
    "- $\\lambda_{w_{d,n}} \\rightarrow$ Dirichlet parameter for topic to word distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### (Optional) LDA algorithm \n",
    "\n",
    "- Suppose $K$ is number of topics\n",
    "- For each iteration $i$\n",
    "    - For each document $d$ and word $n$ currently assigned to topic $Z_{old}$\n",
    "        - Decrement $n_{d,Z_{old}}$ and $V_{Z_{old}, w_{d,n}}$\n",
    "        - Sample $Z_{new} = k$ with probability proportional to $\\frac{n_{d,k} + \\alpha_k}{\\sum^K_i n_{d,i} + \\alpha_i} \\frac{V_{k, w_{d,n}} + \\lambda_{w_{d,n}}}{\\sum_i V_{k,i} + \\lambda_i}$\n",
    "        - Increment $n_{d, Z_{new}} and V_{Z_{new}, w_{d,n}}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Random topic assignment\n",
    "\n",
    "- Randomly assign each word in each document to one of the topics. \n",
    "    - The same word in the vocabulary may have different topic assignments in different instances.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sample document and random topic assignment\n",
    "- Consider this sample document (Document 10) with random topic assignment\n",
    "\n",
    "![](img/topic_modeling_word_topic_assignment.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_word_topic_assignment.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "\n",
    "- With the current topic assignment, here are the topic counts in our document \n",
    "\n",
    "![](img/topic_modeling_doc_topic_counts.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_doc_topic_counts.png\" height=\"700\" width=\"700\">  -->\n",
    "<!-- </center>     -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Total topic counts\n",
    "\n",
    "- Find out how much each word in the document is liked by each topic.\n",
    "- For each word in our current document (Document 10), calculate how often that word occurs with each topic in all documents\n",
    "\n",
    "![](img/topic_modeling_word_topic_counts.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_word_topic_counts.png\" height=\"700\" width=\"700\">  -->\n",
    "<!-- </center>     -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to figure out a way to revise word-topic assignments. For that we sample a word-topic assignment and revise it assuming that we know everything else in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sample a word-topic assignment\n",
    "\n",
    "- Suppose our sampled word-topic assignment is the word _probabilistic_ in Document 10 with assigned topic 3. \n",
    "- How often does the Topic 3 occur in Document 10? Once. \n",
    "- How often does the word _probabilistic_ occur with Topic 3 in the corpus? Twice.  \n",
    "\n",
    "<!-- <center> -->\n",
    "![](img/topic_modeling_word_topic_assignment.png)\n",
    "<!-- <img src=\"img/topic_modeling_word_topic_assignment.png\" height=\"600\" width=\"600\">  -->\n",
    "\n",
    "![](img/topic_modeling_doc_topic_counts.png)\n",
    "<!-- <img src=\"img/topic_modeling_doc_topic_counts.png\" height=\"600\" width=\"600\">  -->\n",
    "\n",
    "![](img/topic_modeling_word_topic_counts.png)\n",
    "<!-- <img src=\"img/topic_modeling_word_topic_counts.png\" height=\"600\" width=\"600\">  -->\n",
    "<!-- </center>     -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Decrement counts\n",
    "\n",
    "- We want to update the word topic assignment of _probabilistic_ and Topic 3. \n",
    "- Decrement the count of the word from the word-topic counts.\n",
    "\n",
    "![](img/topic_modeling_count_decrement.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_count_decrement.png\" height=\"1200\" width=\"1200\">  -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Calculating conditional probability distribution \n",
    "\n",
    "- How much does Document 10 like each topic?\n",
    "- How much does each topic like word _probabilistic_ ? \n",
    "\n",
    "<!-- <center> -->\n",
    "![](img/topic_modeling_decremented_counts.png)\n",
    "<!-- <img src=\"img/topic_modeling_decremented_counts.png\" height=\"500\" width=\"500\">  -->\n",
    "\n",
    "![](img/topic_modeling_conditional_proba.png)\n",
    "<!-- <img src=\"img/topic_modeling_conditional_proba.png\" height=\"800\" width=\"800\"> -->\n",
    "<!-- </center>     -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Updating topic assignment\n",
    "\n",
    "- So update the topic of the current word _probabilistic_ in document 10 to **topic 1**\n",
    "- Update the document-topic and word-topic counts accordingly. \n",
    "\n",
    "![](img/topic_modeling_update_count.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_update_count.png\" height=\"1200\" width=\"1200\">  -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Conclusion\n",
    "\n",
    "- In one pass, the algorithm repeats the above steps for each word in the corpus\n",
    "- If you do this for several passes, meaningful topics emerge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you\n",
    "\n",
    "iClicker cloud join link: https://join.iclicker.com/4QVT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 5.1: Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) Latent Dirichlet Allocation (LDA) is an unsupervised approach. \n",
    "- (B) The assumption in topic modeling is that different topics tend to use different words. \n",
    "- (C) We could use LSA for topic modeling, where $Z$ gives us the mixture of topics per document. \n",
    "- (D) In LDA topic model, a document is a mixture of multiple topics. \n",
    "- (E) If I train a topic model on a large collection of news articles with K = 10, I would get 10 topic labels (e.g., sports, culture, politics, finance) as output. \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 5.1: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "- (A) \n",
    "- (B) \n",
    "- (C) \n",
    "- (D) \n",
    "- (E) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Questions for class discussion \n",
    "\n",
    "1. What's the difference between topic modeling and document clustering using algorithms such as K-Means clustering? \n",
    "2. In 563 lab1, we carried out document clustering using K-Means and sentence embedding representation of documents. Would it be a good idea to use sentence embedding representation of documents to a topic model? \n",
    "3. Imagine that you are using LSA for topic modeling, where $X$ is bag-of-words representation of $N$ documents and $V$ features (number of words in the vocabulary). Explain which matrix would give you word distribution over topics and which matrix would give you topic distribution over documents.\n",
    "$$X_{N \\times V} = Z_{N \\times K} W_{K \\times V}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 5.2: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic modeling with Python\n",
    "\n",
    "- `scikit-learn` has implementation of LDA but we'll be using another library called [gensim](https://radimrehurek.com/gensim/models/ldamodel.html), which we used for `Word2Vec`, because it's more flexible.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Topic modeling pipeline \n",
    "\n",
    "- Preprocess your corpus. \n",
    "- Train LDA using `Gensim`.\n",
    "- Interpret your topics.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "Let's work with a slightly bigger corpus. We'll extract text from a few Wikipedia articles and apply topic modeling on these texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54862\n",
      "26059\n",
      "21609\n",
      "15856\n",
      "16999\n",
      "73034\n",
      "54448\n",
      "19035\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki query</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Artificial intelligence (AI) is intelligence—perceiving, synthesizing, and inferring information—demonstrated by machines, as opposed to intelligence displayed by non-human animals and humans. Exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supervised Machine Learning</td>\n",
       "      <td>Supervised learning (SL) is a machine learning paradigm for problems where the available data consists of labeled examples, meaning that each data point contains features (covariates) and an assoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Supreme Court of Canada</td>\n",
       "      <td>The Supreme Court of Canada (SCC; French: Cour suprême du Canada, CSC) is the highest court in the judicial system of Canada. It comprises nine justices, whose decisions are the ultimate applicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peace, Order, and Good Government</td>\n",
       "      <td>In many Commonwealth jurisdictions, the phrase \"peace, order, and good government\" (POGG) is an expression used in law to express the legitimate objects of legislative powers conferred by statute....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canadian constitutional law</td>\n",
       "      <td>Canadian constitutional law (French: droit constitutionnel du Canada) is the area of Canadian law relating to the interpretation and application of the Constitution of Canada by the courts. All la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ice hockey</td>\n",
       "      <td>Ice hockey (or simply hockey) is a team sport played on ice skates, usually on an ice skating rink with lines and markings specific to the sport. It belongs to a family of sports called hockey. In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Google company</td>\n",
       "      <td>Google LLC ( (listen)) is an American multinational technology company focusing on online advertising, search engine technology, cloud computing, computer software, quantum computing, e-commerce, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Google litigation</td>\n",
       "      <td>Google has been involved in multiple lawsuits over issues such as privacy, advertising, intellectual property and various Google services such as Google Books and YouTube. The company's legal depa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          wiki query  \\\n",
       "0            Artificial Intelligence   \n",
       "1        Supervised Machine Learning   \n",
       "2            Supreme Court of Canada   \n",
       "3  Peace, Order, and Good Government   \n",
       "4        Canadian constitutional law   \n",
       "5                         ice hockey   \n",
       "6                     Google company   \n",
       "7                  Google litigation   \n",
       "\n",
       "                                                                                                                                                                                                      text  \n",
       "0  Artificial intelligence (AI) is intelligence—perceiving, synthesizing, and inferring information—demonstrated by machines, as opposed to intelligence displayed by non-human animals and humans. Exa...  \n",
       "1  Supervised learning (SL) is a machine learning paradigm for problems where the available data consists of labeled examples, meaning that each data point contains features (covariates) and an assoc...  \n",
       "2  The Supreme Court of Canada (SCC; French: Cour suprême du Canada, CSC) is the highest court in the judicial system of Canada. It comprises nine justices, whose decisions are the ultimate applicati...  \n",
       "3  In many Commonwealth jurisdictions, the phrase \"peace, order, and good government\" (POGG) is an expression used in law to express the legitimate objects of legislative powers conferred by statute....  \n",
       "4  Canadian constitutional law (French: droit constitutionnel du Canada) is the area of Canadian law relating to the interpretation and application of the Constitution of Canada by the courts. All la...  \n",
       "5  Ice hockey (or simply hockey) is a team sport played on ice skates, usually on an ice skating rink with lines and markings specific to the sport. It belongs to a family of sports called hockey. In...  \n",
       "6  Google LLC ( (listen)) is an American multinational technology company focusing on online advertising, search engine technology, cloud computing, computer software, quantum computing, e-commerce, ...  \n",
       "7  Google has been involved in multiple lawsuits over issues such as privacy, advertising, intellectual property and various Google services such as Google Books and YouTube. The company's legal depa...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "queries = [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Supervised Machine Learning\",\n",
    "    \"Supreme Court of Canada\",\n",
    "    \"Peace, Order, and Good Government\",\n",
    "    \"Canadian constitutional law\",\n",
    "    \"ice hockey\",\n",
    "    \"Google company\",\n",
    "    \"Google litigation\",\n",
    "]\n",
    "wiki_dict = {\"wiki query\": [], \"text\": []}\n",
    "for i in range(len(queries)):\n",
    "    text = wikipedia.page(queries[i]).content\n",
    "    wiki_dict[\"text\"].append(text)\n",
    "    print(len(text))\n",
    "    wiki_dict[\"wiki query\"].append(queries[i])\n",
    "\n",
    "wiki_df = pd.DataFrame(wiki_dict)\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing the corpus \n",
    "\n",
    "- **Preprocessing is crucial!**\n",
    "- Tokenization, converting text to lower case\n",
    "- Removing punctuation and stopwords\n",
    "- Discarding words with length < threshold or word frequency < threshold        \n",
    "- Possibly lemmatization: Consider the lemmas instead of inflected forms. \n",
    "- Depending upon your application, restrict to specific part of speech;\n",
    "    * For example, only consider nouns, verbs, and adjectives\n",
    "    \n",
    "We'll use [`spaCy`](https://spacy.io/) for preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_spacy(\n",
    "    doc,\n",
    "    min_token_len=2,\n",
    "    irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text\n",
    "    and return a preprocessed string.\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    doc : (spaCy doc object)\n",
    "        the spacy doc object of the text\n",
    "    min_token_len : (int)\n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list)\n",
    "        a list of irrelevant pos tags\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    clean_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        if (\n",
    "            token.is_stop == False  # Check if it's not a stopword\n",
    "            and len(token) > min_token_len  # Check if the word meets minimum threshold\n",
    "            and token.pos_ not in irrelevant_pos\n",
    "        ):  # Check if the POS is in the acceptable POS tags\n",
    "            lemma = token.lemma_  # Take the lemma of the word\n",
    "            clean_text.append(lemma.lower())\n",
    "    return \" \".join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wiki_df[\"text_pp\"] = [preprocess_spacy(text) for text in nlp.pipe(wiki_df[\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki query</th>\n",
       "      <th>text</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Artificial intelligence (AI) is intelligence—perceiving, synthesizing, and inferring information—demonstrated by machines, as opposed to intelligence displayed by non-human animals and humans. Exa...</td>\n",
       "      <td>artificial intelligence intelligence perceive synthesizing infer information demonstrate machine oppose intelligence display non human animal human example task include speech recognition computer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supervised Machine Learning</td>\n",
       "      <td>Supervised learning (SL) is a machine learning paradigm for problems where the available data consists of labeled examples, meaning that each data point contains features (covariates) and an assoc...</td>\n",
       "      <td>supervised learning machine learn paradigm problem available datum consist label example mean datum point contain feature covariate associated label goal supervised learning algorithm learn functi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Supreme Court of Canada</td>\n",
       "      <td>The Supreme Court of Canada (SCC; French: Cour suprême du Canada, CSC) is the highest court in the judicial system of Canada. It comprises nine justices, whose decisions are the ultimate applicati...</td>\n",
       "      <td>supreme court canada scc french cour suprême canada csc high court judicial system canada comprise justice decision ultimate application canadian law grant permission litigant year appeal decision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peace, Order, and Good Government</td>\n",
       "      <td>In many Commonwealth jurisdictions, the phrase \"peace, order, and good government\" (POGG) is an expression used in law to express the legitimate objects of legislative powers conferred by statute....</td>\n",
       "      <td>commonwealth jurisdiction phrase peace order good government pogg expression law express legitimate object legislative power confer statute phrase appear imperial acts parliament letters patent co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canadian constitutional law</td>\n",
       "      <td>Canadian constitutional law (French: droit constitutionnel du Canada) is the area of Canadian law relating to the interpretation and application of the Constitution of Canada by the courts. All la...</td>\n",
       "      <td>canadian constitutional law french droit constitutionnel canada area canadian law relate interpretation application constitution canada court law canada provincial federal conform constitution law...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ice hockey</td>\n",
       "      <td>Ice hockey (or simply hockey) is a team sport played on ice skates, usually on an ice skating rink with lines and markings specific to the sport. It belongs to a family of sports called hockey. In...</td>\n",
       "      <td>ice hockey hockey team sport play ice skate ice skating rink line marking specific sport belong family sport call hockey ice hockey oppose team use ice hockey stick control advance shoot closed vu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Google company</td>\n",
       "      <td>Google LLC ( (listen)) is an American multinational technology company focusing on online advertising, search engine technology, cloud computing, computer software, quantum computing, e-commerce, ...</td>\n",
       "      <td>google llc listen american multinational technology company focus online advertising search engine technology cloud computing computer software quantum computing commerce artificial intelligence c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Google litigation</td>\n",
       "      <td>Google has been involved in multiple lawsuits over issues such as privacy, advertising, intellectual property and various Google services such as Google Books and YouTube. The company's legal depa...</td>\n",
       "      <td>google involve multiple lawsuit issue privacy advertising intellectual property google service google books youtube company legal department expand 100 lawyer year business 2014 grow 400 lawyer go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          wiki query  \\\n",
       "0            Artificial Intelligence   \n",
       "1        Supervised Machine Learning   \n",
       "2            Supreme Court of Canada   \n",
       "3  Peace, Order, and Good Government   \n",
       "4        Canadian constitutional law   \n",
       "5                         ice hockey   \n",
       "6                     Google company   \n",
       "7                  Google litigation   \n",
       "\n",
       "                                                                                                                                                                                                      text  \\\n",
       "0  Artificial intelligence (AI) is intelligence—perceiving, synthesizing, and inferring information—demonstrated by machines, as opposed to intelligence displayed by non-human animals and humans. Exa...   \n",
       "1  Supervised learning (SL) is a machine learning paradigm for problems where the available data consists of labeled examples, meaning that each data point contains features (covariates) and an assoc...   \n",
       "2  The Supreme Court of Canada (SCC; French: Cour suprême du Canada, CSC) is the highest court in the judicial system of Canada. It comprises nine justices, whose decisions are the ultimate applicati...   \n",
       "3  In many Commonwealth jurisdictions, the phrase \"peace, order, and good government\" (POGG) is an expression used in law to express the legitimate objects of legislative powers conferred by statute....   \n",
       "4  Canadian constitutional law (French: droit constitutionnel du Canada) is the area of Canadian law relating to the interpretation and application of the Constitution of Canada by the courts. All la...   \n",
       "5  Ice hockey (or simply hockey) is a team sport played on ice skates, usually on an ice skating rink with lines and markings specific to the sport. It belongs to a family of sports called hockey. In...   \n",
       "6  Google LLC ( (listen)) is an American multinational technology company focusing on online advertising, search engine technology, cloud computing, computer software, quantum computing, e-commerce, ...   \n",
       "7  Google has been involved in multiple lawsuits over issues such as privacy, advertising, intellectual property and various Google services such as Google Books and YouTube. The company's legal depa...   \n",
       "\n",
       "                                                                                                                                                                                                   text_pp  \n",
       "0  artificial intelligence intelligence perceive synthesizing infer information demonstrate machine oppose intelligence display non human animal human example task include speech recognition computer...  \n",
       "1  supervised learning machine learn paradigm problem available datum consist label example mean datum point contain feature covariate associated label goal supervised learning algorithm learn functi...  \n",
       "2  supreme court canada scc french cour suprême canada csc high court judicial system canada comprise justice decision ultimate application canadian law grant permission litigant year appeal decision...  \n",
       "3  commonwealth jurisdiction phrase peace order good government pogg expression law express legitimate object legislative power confer statute phrase appear imperial acts parliament letters patent co...  \n",
       "4  canadian constitutional law french droit constitutionnel canada area canadian law relate interpretation application constitution canada court law canada provincial federal conform constitution law...  \n",
       "5  ice hockey hockey team sport play ice skate ice skating rink line marking specific sport belong family sport call hockey ice hockey oppose team use ice hockey stick control advance shoot closed vu...  \n",
       "6  google llc listen american multinational technology company focus online advertising search engine technology cloud computing computer software quantum computing commerce artificial intelligence c...  \n",
       "7  google involve multiple lawsuit issue privacy advertising intellectual property google service google books youtube company legal department expand 100 lawyer year business 2014 grow 400 lawyer go...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training LDA with [gensim](https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "\n",
    "To train an LDA model with [gensim](https://radimrehurek.com/gensim/models/ldamodel.html), you need\n",
    "\n",
    "- Document-term matrix \n",
    "- Dictionary (vocabulary)\n",
    "- The number of topics ($K$): `num_topics`\n",
    "- The number of passes: `passes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Gensim`'s `doc2bow`\n",
    "- Let's first create a dictionary using [`corpora.Dictionary`](https://radimrehurek.com/gensim/corpora/dictionary.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134,777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167,038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4914</th>\n",
       "      <td>wilberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4915</th>\n",
       "      <td>window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4916</th>\n",
       "      <td>windows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4917</th>\n",
       "      <td>wiretap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4918</th>\n",
       "      <td>yuan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4919 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word\n",
       "0         1.6\n",
       "1     134,777\n",
       "2     167,038\n",
       "3        1863\n",
       "4        1943\n",
       "...       ...\n",
       "4914  wilberg\n",
       "4915   window\n",
       "4916  windows\n",
       "4917  wiretap\n",
       "4918     yuan\n",
       "\n",
       "[4919 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [doc.split() for doc in wiki_df[\"text_pp\"].tolist()]\n",
    "dictionary = corpora.Dictionary(corpus)  # Create a vocabulary for the lda model\n",
    "pd.DataFrame(\n",
    "    dictionary.token2id.keys(), index=dictionary.token2id.values(), columns=[\"Word\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Gensim`'s `doc2bow`\n",
    "- Now let's convert our corpus into document-term matrix for LDA using [`dictionary.doc2bow`](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2bow).\n",
    "- For each document, it stores the frequency of each token in the document in the format (token_id, frequency). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(57, 4),\n",
       " (68, 3),\n",
       " (74, 1),\n",
       " (80, 3),\n",
       " (88, 1),\n",
       " (89, 3),\n",
       " (114, 53),\n",
       " (123, 1),\n",
       " (132, 4),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (146, 1),\n",
       " (147, 1),\n",
       " (148, 4),\n",
       " (149, 3),\n",
       " (164, 1),\n",
       " (166, 1),\n",
       " (174, 2),\n",
       " (182, 2),\n",
       " (192, 4)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "doc_term_matrix[1][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we are ready to train an LDA model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "num_topics = 4\n",
    "\n",
    "lda = models.LdaModel(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examine the topics and topic distribution for a document in our LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"algorithm\" + 0.009*\"intelligence\" + 0.008*\"learning\" + 0.008*\"\\\\displaystyle\"'),\n",
       " (1, '0.001*\"google\" + 0.000*\"hockey\" + 0.000*\"ice\" + 0.000*\"court\"'),\n",
       " (2, '0.063*\"google\" + 0.011*\"company\" + 0.010*\"search\" + 0.005*\"billion\"'),\n",
       " (3, '0.023*\"hockey\" + 0.015*\"court\" + 0.014*\"ice\" + 0.011*\"team\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_words=4)  # Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document label:  Google litigation\n",
      "Topic assignment for document:  [(2, 0.4452362), (3, 0.554476)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Document label: \", wiki_df.iloc[7][0])\n",
    "print(\"Topic assignment for document: \", lda[doc_term_matrix[7]])  # Topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.554476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.445236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic id  probability\n",
       "0         3     0.554476\n",
       "1         2     0.445236"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_assignment = sorted(lda[doc_term_matrix[7]], key=lambda x: x[1], reverse=True)\n",
    "df = pd.DataFrame(topic_assignment, columns=[\"topic id\", \"probability\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'court yesterday lawyer work google lawsuit go ice hockey game'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"After the court yesterday lawyers working on Google lawsuits went to a ice hockey game.\"\n",
    "pp_new_doc = preprocess_spacy(nlp(new_doc))\n",
    "pp_new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = dictionary.doc2bow(pp_new_doc.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.600286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.349240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.025438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.025035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic id  probability\n",
       "0         3     0.600286\n",
       "1         2     0.349240\n",
       "2         0     0.025438\n",
       "3         1     0.025035"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_assignment = sorted(lda[bow_vector], key=lambda x: x[1], reverse=True)\n",
    "df = pd.DataFrame(topic_assignment, columns=[\"topic id\", \"probability\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualize topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can also visualize the topics using `pyLDAvis`. \n",
    "\n",
    "```\n",
    "pip install pyLDAvis\n",
    "\n",
    "```\n",
    "\n",
    "> Do not install it using `conda`. They have made some changes in the recent version and `conda` build is not available for this version yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kvarada/miniconda3/envs/575/lib/python3.10/site-packages/pyLDAvis/_prepare.py:244: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el5403080185861762383794278\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el5403080185861762383794278_data = {\"mdsDat\": {\"x\": [0.06596660956171739, -0.0019853982289354914, 0.11615792683029627, -0.18013913816307806], \"y\": [-0.15522983697705026, -1.5743509387739222e-05, 0.1289441050128747, 0.02630147547356335], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [30.635659336017795, 0.008558671951811498, 24.611010651484776, 44.74477134054562]}, \"tinfo\": {\"Term\": [\"google\", \"hockey\", \"ice\", \"player\", \"league\", \"team\", \"company\", \"play\", \"algorithm\", \"game\", \"\\\\displaystyle\", \"court\", \"learning\", \"intelligence\", \"search\", \"puck\", \"machine\", \"problem\", \"learn\", \"canada\", \"function\", \"penalty\", \"training\", \"human\", \"nhl\", \"artificial\", \"input\", \"datum\", \"billion\", \"sport\", \"\\\\displaystyle\", \"training\", \"input\", \"supervised\", \"neural\", \"logic\", \"solve\", \"variance\", \"symbolic\", \"classifier\", \"reasoning\", \"problem\", \"researcher\", \"vector\", \"mind\", \"optimization\", \"regression\", \"analysis\", \"learning\", \"intelligent\", \"minimization\", \"solution\", \"probability\", \"tree\", \"agent\", \"substance\", \"neuron\", \"pith\", \"theory\", \"linear\", \"algorithm\", \"machine\", \"function\", \"bias\", \"learn\", \"intelligence\", \"output\", \"human\", \"knowledge\", \"example\", \"artificial\", \"model\", \"risk\", \"network\", \"datum\", \"method\", \"research\", \"approach\", \"set\", \"law\", \"system\", \"include\", \"provincial\", \"use\", \"search\", \"feature\", \"federal\", \"court\", \"26.7\", \"junglee\", \"chef\", \"vestager\", \"educational\", \"certification\", \"port\", \"curie\", \"intern\", \"restructure\", \"universum\", \"billion-$12\", \"scott\", \"bernanke\", \"affair\", \"ascension\", \"commodity\", \"persuade\", \"atlanta\", \"targeted\", \"fudd\", \"levinson\", \"132,121\", \"2799\", \"megan\", \"respond\", \"manufacturer\", \"mentorship\", \"inktomi\", \"350\", \"covariate\", \"similarity\", \"mcm\", \"confuse\", \"i}l(y_{i},g(x_{i\", \"algorithms\", \"instability\", \"y_{i\", \"google\", \"\\\\beta\", \"ice\", \"l_{0\", \"-\\\\sum\", \"hockey\", \"player\", \"j}|\", \"court\", \"play\", \"team\", \"canada\", \"league\", \"conventions\", \"case\", \"penalty\", \"learning\", \"use\", \"game\", \"world\", \"search\", \"datum\", \"government\", \"algorithm\", \"company\", \"new\", \"machine\", \"training\", \"federal\", \"power\", \"\\\\displaystyle\", \"goal\", \"national\", \"year\", \"provincial\", \"law\", \"puck\", \"include\", \"employee\", \"alphabet\", \"brin\", \"larry\", \"corporate\", \"september\", \"page\", \"shriram\", \"worker\", \"revenue\", \"million\", \"quarter\", \"click\", \"nest\", \"stock\", \"profit\", \"fine\", \"sergey\", \"google\", \"2007\", \"investor\", \"wind\", \"pichai\", \"buy\", \"campus\", \"office\", \"billion\", \"android\", \"renewable\", \"bezos\", \"company\", \"advertising\", \"product\", \"engine\", \"service\", \"energy\", \"2018\", \"ceo\", \"cloud\", \"antitrust\", \"project\", \"announce\", \"california\", \"search\", \"2020\", \"lawsuit\", \"2013\", \"website\", \"user\", \"share\", \"market\", \"internet\", \"october\", \"january\", \"union\", \"2019\", \"inc.\", \"include\", \"new\", \"use\", \"information\", \"large\", \"time\", \"hockey\", \"ice\", \"league\", \"puck\", \"nhl\", \"stick\", \"player\", \"sport\", \"minute\", \"minor\", \"championship\", \"tournament\", \"iihf\", \"cup\", \"amateur\", \"season\", \"club\", \"rink\", \"goaltender\", \"opponent\", \"award\", \"peace\", \"skate\", \"branch\", \"ball\", \"junior\", \"women\", \"ontario\", \"overtime\", \"injury\", \"team\", \"district\", \"play\", \"game\", \"professional\", \"national\", \"penalty\", \"canada\", \"court\", \"supreme\", \"line\", \"case\", \"woman\", \"appeal\", \"justice\", \"rule\", \"body\", \"canadian\", \"government\", \"order\", \"states\", \"federal\", \"law\", \"power\", \"united\", \"goal\", \"world\", \"provincial\"], \"Freq\": [391.0, 247.0, 152.0, 118.0, 112.0, 121.0, 69.0, 120.0, 71.0, 121.0, 57.0, 186.0, 60.0, 74.0, 82.0, 68.0, 60.0, 49.0, 56.0, 104.0, 47.0, 77.0, 38.0, 49.0, 49.0, 56.0, 32.0, 53.0, 30.0, 40.0, 56.48269943310988, 37.68660180800789, 31.69289184488519, 18.967434730363575, 21.434737850596985, 20.58450721640431, 19.737903489485102, 15.553640851236668, 18.03660819956544, 14.676915408703442, 16.337977098675225, 47.759041436036185, 15.477507554824829, 13.00145974387493, 14.618274836496258, 12.963162444423048, 12.136274384876105, 12.115021878223677, 58.042476812544884, 12.086157044761569, 10.444444988663022, 11.253779318422136, 10.438352787275102, 10.423469096367166, 11.236116948554688, 9.571306340143096, 10.381314238395854, 9.571256198567886, 9.554462949337397, 8.730554970743723, 67.47605260251744, 56.27379625970155, 44.53274863463248, 20.626017755134185, 51.301647350489745, 67.2002806244101, 18.94999883327749, 44.37704901529281, 19.74039385438716, 32.56053163399051, 47.68490878847519, 16.368640343301504, 23.16709918913044, 30.80806686656983, 39.36366701740062, 19.7986968066546, 26.548265945268707, 23.130624535937415, 21.50452823774486, 35.75994186553871, 26.569953847932535, 29.10687018841475, 24.692947436007202, 23.997667640297742, 23.06719711468359, 20.685293453960686, 22.110129826192626, 24.1712427311995, 0.0003996851333379789, 0.000399454380195769, 0.0003994443411062382, 0.00039915116467185844, 0.00039913925784474053, 0.0003991106289295378, 0.0003990434487432007, 0.00039904797217016954, 0.00039903335128687033, 0.0003990049850222658, 0.00039897866159564716, 0.0003989783113948496, 0.0003989732334832846, 0.0003989374546351311, 0.0003988927164832394, 0.0003988808096561215, 0.00039888399064669954, 0.00039887132505118684, 0.0003988675895760126, 0.0003988631536992431, 0.00039884929158433865, 0.0003988516554397224, 0.0003988402155470012, 0.0003988384937264131, 0.00039882512772930516, 0.00039882308489131926, 0.000398819787167142, 0.0003988128998847894, 0.00039880898930921637, 0.0003988060709692365, 0.0004004647678636065, 0.0004002180805851064, 0.0004002138198087357, 0.00040016277804248745, 0.00040013861418745396, 0.0003999397001344247, 0.00039982641017640536, 0.00039982430897161984, 0.0012060349310932408, 0.00039970728353842626, 0.0009562787262703051, 0.00039970661232023087, 0.00039969260428832743, 0.0009724874783526111, 0.0008502309791118778, 0.0003996895692147483, 0.0008745991179439575, 0.0007773326557850185, 0.0007722353080757442, 0.0007546453056809564, 0.0007084796769714828, 0.0004007122722773009, 0.0006687885022409937, 0.0006519276762735814, 0.0006275267353001276, 0.0006333422281116532, 0.0006740181091181566, 0.0006110388731821451, 0.0006373698291178991, 0.0006035273579079068, 0.0006147706712480294, 0.0006104932019727053, 0.0005969802956302224, 0.0005855523681029721, 0.000583339916197418, 0.0005554320060701307, 0.0005876965892197963, 0.0005886798946926203, 0.0005764180806995162, 0.0005717389311093601, 0.000565525318357778, 0.0005655540931899797, 0.000571422291221542, 0.0005878916510640521, 0.0005712504009967265, 0.0005734302258613055, 25.066785332639437, 17.711137279198454, 11.87882515432857, 10.213594049953288, 10.177609694390197, 8.549556605730826, 23.53576647561565, 7.713059308321808, 8.505632194696751, 11.642189078206044, 25.02724819331441, 6.881895214655201, 9.244443379700568, 6.879145365394261, 6.877056124451866, 6.874370724782978, 7.661647331660959, 6.874665447396639, 364.1351250665624, 6.045088424934344, 6.045456996038899, 6.044668159886164, 6.042635983686733, 6.821994018291081, 6.040713908873727, 17.728447365464103, 28.551016211830166, 12.982924542791974, 5.21351010836881, 5.212603114630644, 62.97405086803439, 18.283664488605254, 14.284615638892115, 23.07802398965577, 25.770228461304175, 12.711739457229376, 14.903894313834414, 11.881442076305902, 11.880447135728579, 9.744362081510076, 24.34367702219944, 24.389871267303644, 17.126685552561955, 57.70248778962357, 19.044717496366633, 21.233408187398805, 14.525105263539496, 17.250977933536575, 21.165305109095986, 13.361859947368847, 15.989706130552694, 15.752361108917581, 14.445703366129832, 14.019229673914479, 14.590564565868297, 15.068514684336552, 19.339900656070242, 23.262908386059333, 17.97985985326468, 17.627422629920556, 15.418369895899042, 14.97664313356441, 14.759579592928569, 246.26967629215466, 151.4509289081952, 111.65263790541286, 67.46326273479926, 48.88183208888778, 40.04895649904405, 117.00115471082937, 40.02657617426388, 30.29495771214518, 24.994768054380614, 24.120561029004147, 24.09319102448459, 23.220175272940228, 22.345221260283793, 20.581242533396658, 19.69380699798706, 19.69116080775345, 19.680346580138252, 18.799809691961283, 18.79202003048948, 18.795477898077948, 18.77126916325629, 17.900439448016535, 17.883677395212974, 15.265529698791216, 15.25547344356304, 15.263820904359921, 14.373288789327324, 13.494053026053043, 13.491037003881805, 117.89555724524885, 22.05944003845897, 114.46463978609297, 114.43311008770067, 31.183594285926493, 46.30002922945224, 70.99305592201972, 92.77425784550059, 158.45884133903377, 45.58959062123627, 25.84440993923975, 67.40778550301978, 38.98690414297013, 28.069453138444242, 43.648556379917856, 40.44111993875666, 27.64286237321083, 34.9850784643183, 48.11775686460834, 31.638101869897472, 35.92201533326764, 43.33192653633834, 49.20617103648735, 43.70717291118111, 35.6672096722124, 35.63316072260138, 33.63683965802584, 34.10426572789742], \"Total\": [391.0, 247.0, 152.0, 118.0, 112.0, 121.0, 69.0, 120.0, 71.0, 121.0, 57.0, 186.0, 60.0, 74.0, 82.0, 68.0, 60.0, 49.0, 56.0, 104.0, 47.0, 77.0, 38.0, 49.0, 49.0, 56.0, 32.0, 53.0, 30.0, 40.0, 57.01758015245808, 38.205503046193385, 32.23280524591974, 19.441913241144825, 22.005066145909886, 21.144141909314556, 20.29329533018017, 16.017303028479613, 18.587477611133416, 15.163561491242136, 16.88090511327115, 49.36715856962064, 16.01987826855098, 13.459145974697428, 15.170409701084496, 13.45905311899945, 12.604873643028196, 12.606088038814946, 60.42636476835117, 12.604424958015569, 10.897003776575312, 11.743896130164753, 10.895244992261878, 10.891937798952414, 11.747526772060406, 10.045723346482765, 10.89741790646327, 10.047242191855274, 10.043075516374403, 9.182240923402745, 71.48527665700018, 60.418214915498666, 47.714068434431795, 21.964895679796317, 56.21385419837131, 74.85391296889232, 20.271130572074313, 49.31239078800027, 21.17947095532848, 36.55539145369944, 56.12154089451583, 17.70653337753699, 26.37084310364907, 38.948192765692156, 53.30429983967956, 22.816660660718927, 34.86794027171649, 28.928603090834645, 30.606835884606912, 85.33070315224838, 53.10222256470407, 78.88954289464432, 59.11744061128974, 66.95337946885287, 82.34958581555385, 33.313958257447354, 67.95711451449588, 186.65293613278723, 1.4811032276841944, 1.481218788585232, 1.4812329274868767, 1.4814007233628326, 1.4814257027117819, 1.4814292044411057, 1.4814519938651254, 1.481476895623278, 1.4814731050307697, 1.4814759565944788, 1.481494917277966, 1.4814938360065482, 1.4814949061288536, 1.4815274806640328, 1.4815281190935938, 1.481532186186342, 1.4815616311966597, 1.481558402482277, 1.4815699627768664, 1.4815598680477018, 1.48155817744687, 1.481571775212045, 1.4815697876702054, 1.4815718646105092, 1.4815696675900303, 1.4815696464285562, 1.4815799760155788, 1.481585734397936, 1.4816024429077206, 1.4816002725750548, 1.4979192415114808, 1.498063610076946, 1.4980642960423831, 1.498075670468187, 1.498105660291852, 1.4982070649400996, 1.498269665280934, 1.498268202230919, 391.19903855936053, 1.4983352571368176, 152.20512724412657, 1.4983475558462453, 1.498356710846149, 247.2281373061123, 118.48305870304527, 1.4983614949242738, 186.65293613278723, 120.92369554297524, 121.91053104789624, 104.79112694631361, 112.37409160134412, 1.5295804941744897, 84.17186970354729, 77.54260329200771, 60.42636476835117, 66.95337946885287, 121.87080031527705, 52.89269607293943, 82.34958581555385, 53.30429983967956, 66.37997563803953, 71.48527665700018, 69.47202075243248, 57.73924507490676, 60.418214915498666, 38.205503046193385, 67.95711451449588, 69.54815613982765, 57.01758015245808, 54.02143787718828, 49.28375027332476, 50.23065875027685, 59.11744061128974, 85.33070315224838, 68.04540174961069, 78.88954289464432, 25.76476300752256, 18.22168258168086, 12.363636344219099, 10.692583040850606, 10.693596665943998, 9.015596440377863, 24.93472113675275, 8.176538528245766, 9.016852531733235, 12.377330066322862, 26.62471317105118, 7.339358855329227, 9.859287029018807, 7.341488632536126, 7.3403465459849935, 7.338562124294608, 8.179634025480597, 7.3419041236713, 391.19903855936053, 6.501071708487861, 6.504048892411659, 6.50557650691927, 6.503825469942115, 7.343988637170565, 6.505957652175011, 19.111113832144483, 30.805459259821255, 14.073390940619616, 5.665952647733427, 5.666780984088601, 69.47202075243248, 19.933456298737504, 15.73558015017966, 25.824471890492507, 29.19779980348929, 14.048778416984284, 16.614092481779096, 13.216770021713446, 13.217837564583085, 10.714211037396527, 29.19416477823626, 29.345276194562636, 19.993970281135006, 82.34958581555385, 23.340026292516157, 26.891416495331598, 17.489340398300698, 21.754058801302687, 28.435336954796004, 15.789674724531144, 20.823782269238734, 20.839304731237426, 18.420558800113035, 17.54933998056756, 19.296335799786817, 20.97254860605951, 43.858289994813745, 78.88954289464432, 57.73924507490676, 66.95337946885287, 30.28737549188528, 38.226865103518605, 48.2311377206826, 247.2281373061123, 152.20512724412657, 112.37409160134412, 68.04540174961069, 49.42730208537486, 40.55449040942455, 118.48305870304527, 40.5415289013375, 30.784286635498212, 25.474506525730998, 24.585853951162, 24.577811379196717, 23.7031208248293, 22.81742414296092, 21.04679011382048, 20.158455063761593, 20.160270666719526, 20.15563351364677, 19.26155882561234, 19.259508315368823, 19.27053543879129, 19.25447969421563, 18.36340544880754, 18.366919487108795, 15.724464711466267, 15.71478759121125, 15.724558942080588, 14.829444864044069, 13.944130741588815, 13.94441174736479, 121.91053104789624, 22.804960381810485, 120.92369554297524, 121.87080031527705, 32.527807421258785, 49.28375027332476, 77.54260329200771, 104.79112694631361, 186.65293613278723, 50.14120062257021, 27.18470881884698, 84.17186970354729, 44.65983247462365, 30.67841413236434, 52.612240862570516, 49.021386511604234, 30.672487879629394, 42.08017803585598, 66.37997563803953, 38.48583297976564, 47.85202426603094, 67.95711451449588, 85.33070315224838, 69.54815613982765, 49.50901607035885, 54.02143787718828, 52.89269607293943, 59.11744061128974], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.8449, -5.2495, -5.4228, -5.9361, -5.8138, -5.8543, -5.8963, -6.1346, -5.9864, -6.1926, -6.0854, -5.0127, -6.1395, -6.3138, -6.1966, -6.3167, -6.3827, -6.3844, -4.8177, -6.3868, -6.5328, -6.4581, -6.5334, -6.5348, -6.4597, -6.6201, -6.5388, -6.6201, -6.6218, -6.712, -4.6671, -4.8486, -5.0826, -5.8523, -4.9411, -4.6712, -5.937, -5.0861, -5.8962, -5.3958, -5.0142, -6.0835, -5.7361, -5.4511, -5.206, -5.8932, -5.5999, -5.7377, -5.8106, -5.302, -5.5991, -5.5079, -5.6723, -5.7009, -5.7404, -5.8494, -5.7828, -5.6937, -8.5207, -8.5213, -8.5213, -8.522, -8.5221, -8.5221, -8.5223, -8.5223, -8.5223, -8.5224, -8.5225, -8.5225, -8.5225, -8.5226, -8.5227, -8.5227, -8.5227, -8.5227, -8.5228, -8.5228, -8.5228, -8.5228, -8.5228, -8.5228, -8.5229, -8.5229, -8.5229, -8.5229, -8.5229, -8.5229, -8.5188, -8.5194, -8.5194, -8.5195, -8.5196, -8.5201, -8.5204, -8.5204, -7.4163, -8.5207, -7.6483, -8.5207, -8.5207, -7.6315, -7.7659, -8.5207, -7.7376, -7.8555, -7.8621, -7.8851, -7.9483, -8.5181, -8.0059, -8.0315, -8.0696, -8.0604, -7.9981, -8.0962, -8.054, -8.1086, -8.0901, -8.0971, -8.1195, -8.1388, -8.1426, -8.1916, -8.1352, -8.1335, -8.1546, -8.1627, -8.1736, -8.1736, -8.1633, -8.1348, -8.1636, -8.1598, -5.4383, -5.7857, -6.1851, -6.3362, -6.3397, -6.514, -5.5014, -6.617, -6.5192, -6.2052, -5.4399, -6.731, -6.4359, -6.7314, -6.7317, -6.7321, -6.6237, -6.732, -2.7624, -6.8606, -6.8606, -6.8607, -6.861, -6.7397, -6.8614, -5.7847, -5.3082, -6.0962, -7.0086, -7.0088, -4.5172, -5.7539, -6.0007, -5.521, -5.4107, -6.1174, -5.9583, -6.1849, -6.185, -6.3832, -5.4676, -5.4657, -5.8192, -4.6046, -5.7131, -5.6043, -5.984, -5.812, -5.6075, -6.0675, -5.8879, -5.9029, -5.9895, -6.0195, -5.9795, -5.9473, -5.6977, -5.513, -5.7706, -5.7904, -5.9243, -5.9534, -5.968, -3.7512, -4.2374, -4.5423, -5.0461, -5.3683, -5.5676, -4.4955, -5.5681, -5.8467, -6.039, -6.0746, -6.0757, -6.1126, -6.151, -6.2333, -6.2774, -6.2775, -6.278, -6.3238, -6.3242, -6.324, -6.3253, -6.3728, -6.3738, -6.5321, -6.5327, -6.5322, -6.5923, -6.6554, -6.6556, -4.4879, -6.1639, -4.5174, -4.5177, -5.8178, -5.4225, -4.9951, -4.7275, -4.1922, -5.438, -6.0056, -5.0469, -5.5944, -5.923, -5.4815, -5.5578, -5.9383, -5.7027, -5.384, -5.8033, -5.6763, -5.4888, -5.3616, -5.4802, -5.6834, -5.6844, -5.742, -5.7282], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1736, 1.1693, 1.1661, 1.1583, 1.1567, 1.1562, 1.1553, 1.1536, 1.1529, 1.1504, 1.1503, 1.1499, 1.1486, 1.1484, 1.1459, 1.1455, 1.1451, 1.1433, 1.1428, 1.141, 1.1406, 1.1404, 1.1402, 1.139, 1.1385, 1.1346, 1.1345, 1.1345, 1.1331, 1.1326, 1.1253, 1.1119, 1.114, 1.1201, 1.0916, 1.0751, 1.1156, 1.0776, 1.1126, 1.0673, 1.0201, 1.1044, 1.0535, 0.9485, 0.8798, 1.0411, 0.9104, 0.9593, 0.83, 0.3133, 0.4906, 0.1859, 0.31, 0.157, -0.0896, 0.7065, 0.0602, -0.8611, 1.1484, 1.1477, 1.1477, 1.1468, 1.1468, 1.1467, 1.1465, 1.1465, 1.1465, 1.1464, 1.1463, 1.1463, 1.1463, 1.1462, 1.1461, 1.1461, 1.146, 1.146, 1.146, 1.146, 1.146, 1.146, 1.1459, 1.1459, 1.1459, 1.1459, 1.1459, 1.1458, 1.1458, 1.1458, 1.139, 1.1383, 1.1383, 1.1382, 1.1381, 1.1375, 1.1372, 1.1372, -3.3237, 1.1368, -2.6117, 1.1368, 1.1368, -3.08, -2.4788, 1.1368, -2.905, -2.5888, -2.6035, -2.4753, -2.6082, 1.1187, -2.3769, -2.3204, -2.1092, -2.2025, -2.7392, -2.0026, -2.4032, -2.0228, -2.2237, -2.3048, -2.2986, -2.1329, -2.1821, -1.7728, -2.2922, -2.3137, -2.1361, -2.0902, -2.0094, -2.0283, -2.1809, -2.5195, -2.3219, -2.4659, 1.3745, 1.3736, 1.362, 1.3561, 1.3525, 1.3489, 1.3442, 1.3436, 1.3436, 1.3407, 1.3401, 1.3376, 1.3376, 1.3369, 1.3368, 1.3366, 1.3366, 1.3362, 1.3303, 1.3293, 1.3289, 1.3285, 1.3284, 1.3282, 1.3278, 1.3269, 1.326, 1.3213, 1.3188, 1.3184, 1.3038, 1.3156, 1.3052, 1.2895, 1.2771, 1.302, 1.2933, 1.2955, 1.2953, 1.3071, 1.2203, 1.217, 1.2472, 1.0463, 1.1986, 1.1657, 1.2163, 1.17, 1.1067, 1.235, 1.1378, 1.1221, 1.1589, 1.1774, 1.1224, 1.0714, 0.5832, 0.1808, 0.2353, 0.0674, 0.7268, 0.4649, 0.2179, 0.8003, 0.7992, 0.7978, 0.7956, 0.7931, 0.7917, 0.7916, 0.7914, 0.7882, 0.7852, 0.7851, 0.7843, 0.7836, 0.7833, 0.7818, 0.7809, 0.7807, 0.7803, 0.7799, 0.7796, 0.7792, 0.7788, 0.7787, 0.7775, 0.7746, 0.7745, 0.7745, 0.773, 0.7714, 0.7711, 0.7707, 0.771, 0.7493, 0.7412, 0.762, 0.7417, 0.716, 0.6824, 0.6404, 0.709, 0.7536, 0.5821, 0.6683, 0.7153, 0.6174, 0.6118, 0.7002, 0.6195, 0.4825, 0.6083, 0.5174, 0.3542, 0.2537, 0.3397, 0.4763, 0.3881, 0.3516, 0.2541]}, \"token.table\": {\"Topic\": [1, 3, 3, 1, 3, 4, 3, 4, 1, 3, 4, 1, 3, 4, 3, 3, 3, 1, 1, 1, 3, 4, 3, 1, 1, 3, 4, 1, 3, 4, 1, 3, 4, 3, 4, 3, 4, 3, 4, 1, 3, 4, 1, 3, 4, 3, 3, 4, 4, 3, 3, 1, 3, 1, 3, 3, 1, 3, 4, 4, 3, 3, 1, 3, 4, 3, 1, 3, 4, 1, 4, 1, 3, 4, 1, 3, 3, 4, 3, 1, 3, 1, 3, 4, 3, 1, 3, 4, 1, 4, 3, 1, 3, 4, 1, 4, 3, 1, 3, 3, 4, 3, 3, 1, 3, 1, 3, 4, 1, 3, 4, 1, 3, 4, 1, 3, 4, 3, 3, 1, 4, 1, 3, 4, 1, 3, 4, 4, 1, 3, 4, 1, 3, 4, 3, 4, 1, 3, 4, 4, 4, 3, 4, 1, 3, 4, 1, 3, 4, 4, 3, 1, 1, 1, 3, 1, 3, 1, 3, 4, 3, 1, 3, 4, 3, 4, 1, 3, 4, 1, 1, 4, 1, 1, 3, 4, 3, 1, 4, 3, 4, 4, 1, 3, 4, 1, 3, 4, 3, 3, 4, 1, 1, 1, 3, 4, 3, 1, 3, 1, 3, 3, 1, 3, 1, 3, 4, 1, 1, 4, 4, 1, 3, 1, 3, 4, 3, 1, 3, 4, 1, 1, 1, 3, 4, 4, 3, 4, 3, 4, 4, 4, 1, 1, 3, 4, 1, 3, 4, 1, 3, 4, 1, 4, 3, 3, 1, 1, 3, 4, 1, 4, 3, 1, 3, 4, 1, 1, 4, 1, 3, 1, 4, 3, 1, 3, 1, 4, 4, 3, 1, 1, 3, 1, 3, 4, 1, 3, 3, 3, 4, 4, 1, 4, 1, 3, 4, 3, 1, 3, 4, 4, 3, 3, 1, 3, 4, 1, 3, 4, 1, 3, 4, 3, 1, 4, 1, 1, 4, 3, 4, 4, 3, 1, 1, 1, 4, 1, 1, 3, 4, 3, 3, 4, 1, 1, 3, 4, 4, 1, 1, 3, 4, 1, 3, 4, 3, 1, 3, 4, 1, 3, 4, 1, 1, 3, 1, 3, 4, 3, 3, 4, 4, 3, 1, 3, 4, 1, 1, 3, 4], \"Freq\": [0.6673978183975177, 0.674959767890865, 0.922924752878259, 0.05717768522003049, 0.8576652783004574, 0.11435537044006099, 0.9028479898285571, 0.06018986598857047, 0.09536275431123084, 0.7152206573342312, 0.19072550862246168, 0.1713794127679529, 0.8140522106477763, 0.042844853191988226, 0.6751723859001834, 0.6749588216990677, 0.6749458801475363, 0.667407374442292, 0.9821532209936445, 0.05016691460894996, 0.9030044629610993, 0.05016691460894996, 0.6749787514068952, 0.9363673063645808, 0.9372559376314457, 0.04196668377454234, 0.013988894591514114, 0.667464480311993, 0.9878341321836147, 0.99777685273776, 0.9519210053944759, 0.9237290468836818, 0.07105608052951398, 0.8178488367557755, 0.17038517432411987, 0.9333398385654652, 0.09333398385654652, 0.06519241807516023, 0.9126938530522433, 0.7950608582025523, 0.06913572680022194, 0.13827145360044388, 0.8552865661728567, 0.10691082077160709, 0.03563694025720236, 0.6749768984595138, 0.6749596881174124, 0.985961187240978, 0.9539275438140677, 0.6749790422731758, 0.8823351412449478, 0.9560710101307768, 0.04552719095860842, 0.06492355731922318, 0.9413915811287362, 0.6749943710164583, 0.06520501394763825, 0.032602506973819126, 0.9128701952669356, 0.9800228074519342, 0.9705882368184401, 0.9531605161492878, 0.050015078843221754, 0.8502563403347698, 0.10003015768644351, 0.9222316407168952, 0.10497071956899079, 0.009542792688090072, 0.8874797199923766, 0.16634910608114323, 0.8317455304057162, 0.1544458979678829, 0.04752181475934859, 0.7959903972190889, 0.07566145119852499, 0.9079374143822999, 0.6750238195670424, 0.9761710960975463, 0.6751132664169456, 0.9892135174618045, 0.9128449119607056, 0.07565534037727009, 0.907864084527241, 0.9920501728687552, 0.674963483761589, 0.05757713618629619, 0.906839894934165, 0.04318285213972214, 0.6675230228440159, 0.6537740274595337, 0.9351390661522795, 0.12858088652259989, 0.02143014775376665, 0.8464908362737826, 0.6675927328304738, 0.9641754416344541, 0.6750020894381117, 0.7316482932389728, 0.26264297706014406, 0.043850109066517484, 0.9647023994633847, 0.6750254151588422, 0.9703174833279362, 0.07118056604772477, 0.925347358620422, 0.07744592061672775, 0.8906280870923692, 0.03872296030836388, 0.9027396148061319, 0.054711491806432234, 0.054711491806432234, 0.6303664019061871, 0.12006979083927374, 0.2401395816785475, 0.32373358046724005, 0.04414548824553274, 0.6327519981859692, 0.9780388676411421, 0.6749650572097503, 0.9431180671972788, 0.06287453781315192, 0.04102705477493469, 0.016410821909973877, 0.9354168488685111, 0.31468988364670347, 0.018511169626276674, 0.6664021065459602, 0.9864206823559606, 0.015337461007306541, 0.9304726344432634, 0.05368111352557289, 0.2109069770730256, 0.06025913630657874, 0.7231096356789449, 0.0040448470424781086, 0.9950323724496146, 0.8922706706547882, 0.08111551551407165, 0.02027887887851791, 0.9920822165064542, 0.970336360767618, 0.43321342446882344, 0.547216957223777, 0.36760258629878256, 0.2915468787886896, 0.3422506837954183, 0.4292217397140605, 0.4952558535162237, 0.06603411380216316, 0.9322731023384142, 0.6749448914497258, 0.9927773817964786, 0.6674365924724869, 0.8950767881412394, 0.0935154853281892, 0.9520466058523999, 0.6750038165419346, 0.19194498336617405, 0.7677799334646962, 0.04798624584154351, 0.9225022903810366, 0.6675096600363861, 0.7977507994888836, 0.17094659989047506, 0.6751197106776763, 0.954514969606652, 0.13304888530189846, 0.03801396722911385, 0.8363072790405046, 0.6673956874809702, 0.9443106507326737, 0.047215532536633686, 0.6674018962410989, 0.39239419605504927, 0.39239419605504927, 0.23543651763302956, 0.9352277145564717, 0.42188800361539547, 0.5742364493653994, 0.7809183277364969, 0.18593293517535642, 0.9966710155693962, 0.9072496580652111, 0.03557841796334161, 0.05336762694501242, 0.9598459252405334, 0.016549067676560923, 0.016549067676560923, 0.6749588624262759, 0.03678538573518605, 0.9564200291148373, 0.9801528924232136, 0.993182891510435, 0.9268727995079297, 0.04965389997363909, 0.01655129999121303, 0.6749551264112691, 0.19208806297926348, 0.7683522519170539, 0.6675280911786099, 0.6749598225958775, 0.6749525031073309, 0.8765524586352779, 0.1314828687952917, 0.03755909006701681, 0.9389772516754202, 0.03755909006701681, 0.9887669677719835, 0.9176834481324535, 0.9813732789975063, 0.9745231505675421, 0.9036212599524452, 0.05647632874702783, 0.04058132729161475, 0.020290663645807373, 0.9333705277071392, 0.9534850968749429, 0.7959290996245303, 0.2054010579676207, 0.025675132245952588, 0.9543256930360694, 0.9176485738028813, 0.1731924272135307, 0.31174636898435526, 0.5195772816405921, 0.991354938114227, 0.7600203746215393, 0.2171486784632969, 0.9418603310145318, 0.052325573945251766, 0.9440677064011231, 0.986525704025282, 0.9658926140687097, 0.12991793636450083, 0.05196717454580033, 0.8314747927328053, 0.9372935531368224, 0.049331239638780125, 0.9322918897502219, 0.04010471962030653, 0.9625132708873568, 0.9867833512898259, 0.07737681926160478, 0.9156256945956565, 0.6749649546886238, 0.9225339806133206, 0.9952979941208573, 0.02480903338695786, 0.03307871118261048, 0.9427432687043986, 0.008440025189645935, 0.9874829471885742, 0.6750134355626256, 0.24443494901318785, 0.12940673771286415, 0.6326551621517803, 0.917831586816294, 0.9723063143751206, 0.02025638154948168, 0.06355024666749148, 0.8897034533448808, 0.03074292672264294, 0.953030728401931, 0.95386533239614, 0.13701368168552402, 0.8220820901131441, 0.42288704892318557, 0.5751263865355324, 0.9846367025143374, 0.953761784643789, 0.9478164762280066, 0.9520127166556125, 0.8824641346058848, 0.7743502997193483, 0.11471856292138494, 0.11471856292138494, 0.9363367029727606, 0.6749598322364265, 0.6750025172860283, 0.9695144215835748, 0.0807928684652979, 0.9922784112173207, 0.8721753760241884, 0.11376200556837239, 0.08159703926475292, 0.08159703926475292, 0.8159703926475292, 0.6749938834504671, 0.27929709387385715, 0.7043144106384224, 0.024286703815118015, 0.992139523427743, 0.9982700600585879, 0.9534311374934801, 0.06849831197763707, 0.8904780557092821, 0.03424915598881854, 0.718793673509533, 0.1306897588199151, 0.16336219852489386, 0.06333252694853686, 0.8233228503309792, 0.06333252694853686, 0.9784091363801546, 0.6675283968406631, 0.9802103455254734, 0.9366567856255114, 0.9855471807112577, 0.9866426127476501, 0.25077309025186895, 0.7523192707556069, 0.9863272746414367, 0.9536334498851208, 0.9954484764405966, 0.9772700744179021, 0.07977471521093708, 0.9174092249257765, 0.9683939034965383, 0.5084532943437725, 0.20714763843635178, 0.30130565590742076, 0.6749642870103735, 0.032810947221848116, 0.9679229430445195, 0.9957109237798549, 0.18660144515190646, 0.31100240858651074, 0.5183373476441846, 0.9764905275623609, 0.9946211139807551, 0.9181102742766122, 0.7773496562060097, 0.20729324165493593, 0.020198341218877546, 0.26257843584540813, 0.7271402838795917, 0.6749938783707449, 0.35845838089718757, 0.2688437856728907, 0.3733941467679037, 0.2110050606939623, 0.7385177124288681, 0.03516751011566038, 0.9989197289675517, 0.9658859503002195, 0.6750367974237006, 0.04596843325348176, 0.7814633653091899, 0.13790529976044527, 0.9222856719336798, 0.11195742847537708, 0.8732679421079412, 0.9539218273307755, 0.9981309961903085, 0.18906202070338643, 0.18906202070338643, 0.6428108703915139, 0.6674372442203615, 0.07963264069233322, 0.25880608225008295, 0.6370611255386658], \"Term\": [\"-\\\\sum\", \"132,121\", \"2007\", \"2013\", \"2013\", \"2013\", \"2018\", \"2018\", \"2019\", \"2019\", \"2019\", \"2020\", \"2020\", \"2020\", \"26.7\", \"2799\", \"350\", \"\\\\beta\", \"\\\\displaystyle\", \"advertising\", \"advertising\", \"advertising\", \"affair\", \"agent\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithms\", \"alphabet\", \"amateur\", \"analysis\", \"android\", \"android\", \"announce\", \"announce\", \"antitrust\", \"antitrust\", \"appeal\", \"appeal\", \"approach\", \"approach\", \"approach\", \"artificial\", \"artificial\", \"artificial\", \"ascension\", \"atlanta\", \"award\", \"ball\", \"bernanke\", \"bezos\", \"bias\", \"bias\", \"billion\", \"billion\", \"billion-$12\", \"body\", \"body\", \"body\", \"branch\", \"brin\", \"buy\", \"california\", \"california\", \"california\", \"campus\", \"canada\", \"canada\", \"canada\", \"canadian\", \"canadian\", \"case\", \"case\", \"case\", \"ceo\", \"ceo\", \"certification\", \"championship\", \"chef\", \"classifier\", \"click\", \"cloud\", \"cloud\", \"club\", \"commodity\", \"company\", \"company\", \"company\", \"confuse\", \"conventions\", \"corporate\", \"court\", \"court\", \"court\", \"covariate\", \"cup\", \"curie\", \"datum\", \"datum\", \"district\", \"district\", \"educational\", \"employee\", \"energy\", \"energy\", \"engine\", \"engine\", \"engine\", \"example\", \"example\", \"example\", \"feature\", \"feature\", \"feature\", \"federal\", \"federal\", \"federal\", \"fine\", \"fudd\", \"function\", \"function\", \"game\", \"game\", \"game\", \"goal\", \"goal\", \"goal\", \"goaltender\", \"google\", \"google\", \"google\", \"government\", \"government\", \"government\", \"hockey\", \"hockey\", \"human\", \"human\", \"human\", \"ice\", \"iihf\", \"inc.\", \"inc.\", \"include\", \"include\", \"include\", \"information\", \"information\", \"information\", \"injury\", \"inktomi\", \"input\", \"instability\", \"intelligence\", \"intelligence\", \"intelligent\", \"intern\", \"internet\", \"internet\", \"internet\", \"investor\", \"i}l(y_{i},g(x_{i\", \"january\", \"january\", \"junglee\", \"junior\", \"justice\", \"justice\", \"justice\", \"j}|\", \"knowledge\", \"knowledge\", \"l_{0\", \"large\", \"large\", \"large\", \"larry\", \"law\", \"law\", \"lawsuit\", \"lawsuit\", \"league\", \"learn\", \"learn\", \"learn\", \"learning\", \"learning\", \"learning\", \"levinson\", \"line\", \"line\", \"linear\", \"logic\", \"machine\", \"machine\", \"machine\", \"manufacturer\", \"market\", \"market\", \"mcm\", \"megan\", \"mentorship\", \"method\", \"method\", \"million\", \"million\", \"million\", \"mind\", \"minimization\", \"minor\", \"minute\", \"model\", \"model\", \"national\", \"national\", \"national\", \"nest\", \"network\", \"network\", \"network\", \"neural\", \"neuron\", \"new\", \"new\", \"new\", \"nhl\", \"october\", \"october\", \"office\", \"office\", \"ontario\", \"opponent\", \"optimization\", \"order\", \"order\", \"order\", \"output\", \"output\", \"overtime\", \"page\", \"page\", \"peace\", \"penalty\", \"penalty\", \"persuade\", \"pichai\", \"pith\", \"play\", \"play\", \"play\", \"player\", \"player\", \"port\", \"power\", \"power\", \"power\", \"probability\", \"problem\", \"problem\", \"product\", \"product\", \"professional\", \"professional\", \"profit\", \"project\", \"project\", \"provincial\", \"provincial\", \"puck\", \"quarter\", \"reasoning\", \"regression\", \"renewable\", \"research\", \"research\", \"research\", \"researcher\", \"respond\", \"restructure\", \"revenue\", \"revenue\", \"rink\", \"risk\", \"risk\", \"rule\", \"rule\", \"rule\", \"scott\", \"search\", \"search\", \"search\", \"season\", \"september\", \"sergey\", \"service\", \"service\", \"service\", \"set\", \"set\", \"set\", \"share\", \"share\", \"share\", \"shriram\", \"similarity\", \"skate\", \"solution\", \"solve\", \"sport\", \"states\", \"states\", \"stick\", \"stock\", \"substance\", \"supervised\", \"supreme\", \"supreme\", \"symbolic\", \"system\", \"system\", \"system\", \"targeted\", \"team\", \"team\", \"theory\", \"time\", \"time\", \"time\", \"tournament\", \"training\", \"tree\", \"union\", \"union\", \"united\", \"united\", \"united\", \"universum\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"variance\", \"vector\", \"vestager\", \"website\", \"website\", \"website\", \"wind\", \"woman\", \"woman\", \"women\", \"worker\", \"world\", \"world\", \"world\", \"y_{i\", \"year\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el5403080185861762383794278\", ldavis_el5403080185861762383794278_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el5403080185861762383794278\", ldavis_el5403080185861762383794278_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el5403080185861762383794278\", ldavis_el5403080185861762383794278_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0      0.065967 -0.155230       1        1  30.635659\n",
       "1     -0.001985 -0.000016       2        1   0.008559\n",
       "2      0.116158  0.128944       3        1  24.611011\n",
       "3     -0.180139  0.026301       4        1  44.744771, topic_info=            Term        Freq       Total Category  logprob  loglift\n",
       "681       google  391.000000  391.000000  Default  30.0000  30.0000\n",
       "3323      hockey  247.000000  247.000000  Default  29.0000  29.0000\n",
       "3340         ice  152.000000  152.000000  Default  28.0000  28.0000\n",
       "1119      player  118.000000  118.000000  Default  27.0000  27.0000\n",
       "3428      league  112.000000  112.000000  Default  26.0000  26.0000\n",
       "...          ...         ...         ...      ...      ...      ...\n",
       "1137       power   43.707173   69.548156   Topic4  -5.4802   0.3397\n",
       "1569      united   35.667210   49.509016   Topic4  -5.6834   0.4763\n",
       "677         goal   35.633161   54.021438   Topic4  -5.6844   0.3881\n",
       "1649       world   33.636840   52.892696   Topic4  -5.7420   0.3516\n",
       "2249  provincial   34.104266   59.117441   Topic4  -5.7282   0.2541\n",
       "\n",
       "[285 rows x 6 columns], token_table=      Topic      Freq     Term\n",
       "term                          \n",
       "1664      1  0.667398    -\\sum\n",
       "3914      3  0.674960  132,121\n",
       "3930      3  0.922925     2007\n",
       "36        1  0.057178     2013\n",
       "36        3  0.857665     2013\n",
       "...     ...       ...      ...\n",
       "1649      4  0.642811    world\n",
       "1894      1  0.667437     y_{i\n",
       "1654      1  0.079633     year\n",
       "1654      3  0.258806     year\n",
       "1654      4  0.637061     year\n",
       "\n",
       "[361 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2, 3, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis = gensimvis.prepare(lda, doc_term_matrix, dictionary, sort_topics=False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Topic modeling with `sklearn`\n",
    "\n",
    "- We are using `Gensim` LDA so that we'll be able to use `CoherenceModel` to evaluate topic model later. \n",
    "- Bit we can also train an LDA model with `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(wiki_df[\"text_pp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 3\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, learning_method=\"batch\", max_iter=10, random_state=0\n",
    ")\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda.components_.shape: (3, 4804)\n"
     ]
    }
   ],
   "source": [
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       \n",
      "--------      --------      --------      \n",
      "hockey        google        court         \n",
      "ice           court         intelligence  \n",
      "team          company       machine       \n",
      "player        search        artificial    \n",
      "game          power         problem       \n",
      "play          displaystyle  human         \n",
      "league        law           justice       \n",
      "penalty       government    case          \n",
      "puck          inc           law           \n",
      "canada        algorithm     supreme       \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_topics(\n",
    "    topics=range(3),\n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=5,\n",
    "    n_words=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A few comments on evaluation \n",
    "\n",
    "- In topic modeling we would like each topic to have some semantic theme which is interpretable by humans. \n",
    "- The idea of a topic model is to tell a story to humans and that's what we should care about and evaluate.\n",
    "\n",
    "![](img/TM_health_topics_social_media.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_health_topics_social_media.png\" height=\"700\" width=\"700\">  -->\n",
    "<!-- </center> -->\n",
    "\n",
    "(Credit: [Health topics in social media](https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0103408.g002))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A few ways to evaluate a topic model\n",
    "\n",
    "- Eye balling top $n$ words or documents with high probability for each topic. \n",
    "- Human judgments\n",
    "    - Ask humans to evaluate a topic model and whether they are able to tell a coherent story about the collection of documents using the topic model. \n",
    "    - Word intrusion \n",
    "- Extrinsic evaluation\n",
    "    - If you are using topic modeling as features in tasks such as sentiment analysis or machine translation, evaluate whether topic model with the current hyperparameters improves the results of that task or not. \n",
    "- Capturing topic interpretability\n",
    "    - Coherence scores    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Human judgments: Word intrusion\n",
    "\n",
    "- Word intrusion: Take high probability words from a topic. Now add a high probability word from another topic to this topic. \n",
    "- Likely words from old topic\n",
    "    - dentist, appointment, doctors, tooth\n",
    "- Likely words from old topic    \n",
    "    - dentist, **mom**, appointment, doctors, tooth\n",
    "\n",
    "- Are we able to distinguish this odd word out from the original topic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Coherence model](https://radimrehurek.com/gensim/models/coherencemodel.html)\n",
    "\n",
    "- Coherence models provide a framework to evaluate the **coherence** of the topic by measuring the degree of semantic similarity between most likely words in the topic. \n",
    "- Roughly, high coherence score means the topics are semantically interpretable and low coherence scores means they are not semantically similar. \n",
    "- Let's try it out on our toy data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "K = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "coherence_scores = []\n",
    "\n",
    "for num_topics in K:\n",
    "    lda = models.LdaModel(\n",
    "        corpus=doc_term_matrix,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        random_state=42,\n",
    "        passes=10,\n",
    "    )\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "        model=lda, texts=corpus, dictionary=dictionary, coherence=\"c_v\"\n",
    "    )\n",
    "    coherence_scores.append(coherence_model_lda.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coherence score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.291770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.376235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.498293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.438114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.566066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.471210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Coherence score\n",
       "1         0.291770\n",
       "2         0.376235\n",
       "3         0.498293\n",
       "4         0.438114\n",
       "5         0.566066\n",
       "6         0.471210"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(coherence_scores, index=K, columns=[\"Coherence score\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(title=\"Coherence scores\", xlabel=\"num_topics\", ylabel=\"Coherence score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting the best coherence score with `num_topics = 6`. That said, similar to other methods to pick the best hyperparameters in an unsupervised setting, take this with a grain of salt. The model with highest coherence score is not always going to be the best topic model. You need to balance the topic sizes and coherence scores. It's always a good idea to manually examine the topics resulted by the chosen number of topics and examine whether you are able to make sense of these topics or or not before finalizing the number of topics. In the end our goal is to get human interpretable topics, and that's what we should try to focus on.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coherence score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.291770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.376235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.498293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.438114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.566066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.471210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Coherence score\n",
       "1         0.291770\n",
       "2         0.376235\n",
       "3         0.498293\n",
       "4         0.438114\n",
       "5         0.566066\n",
       "6         0.471210"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final comments and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important ideas to know\n",
    "\n",
    "- Topic modeling is a tool to uncover important themes in a large collection of documents.\n",
    "- The overall goal is to tell a high-level story about a large collection of documents to humans. \n",
    "\n",
    "![](img/TM_health_topics_social_media.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_health_topics_social_media.png\" height=\"900\" width=\"900\">  -->\n",
    "<!-- </center> -->\n",
    "\n",
    "(Credit: [Health topics in social media](https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0103408.g002))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important ideas to know\n",
    "\n",
    "- Latent dirichlet allocation (LDA) is a commonly used model for topic modeling, which is a Bayesian, probabilistic, and generative model. \n",
    "- Topic is something that influences the choice of vocabulary of a document. For each document, we assume that several topics are active with varying importance. \n",
    "- The primary idea of the model is \n",
    "    - A document is a mixture of topics \n",
    "    - A topic is a mixture of words in the vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important ideas to know\n",
    "\n",
    "- You can carry out topic modeling with LDA in Python using the `Gensim` library. \n",
    "- Preprocessing is extremely important in topic modeling. \n",
    "- Some of the most common steps in preprocessing for topic modeling include\n",
    "    - Sentence segmentation, tokenization, lemmatization, stopword and punctuation removal  \n",
    "- You can visualize topic model using `pyLDAvis`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some useful resources and links \n",
    "- [LDA with `sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)\n",
    "- [Jordan Boyd-Graber's very approachable explanation of LDA](https://www.youtube.com/watch?v=fCmIceNqVog)\n",
    "- [lda2vec](https://github.com/cemoody/lda2vec)\n",
    "- [Original topic modeling paper: David Blei et al. 2003](http://www.cs.columbia.edu/~blei/papers/BleiNgJordan2003.pdf)\n",
    "- [Topic modeling for computational social scientists ](http://topicmodels.info/)\n",
    "- [spaCy's Python for data science cheat sheet](http://datacamp-community-prod.s3.amazonaws.com/29aa28bf-570a-4965-8f54-d6a541ae4e06)\n",
    "- If you want to learn more about practical aspects of LDA\n",
    "    - [Rethinking LDA: Why Priors Matter](https://papers.nips.cc/paper/2009/hash/0d0871f0806eae32d30983b62252da50-Abstract.html)\n",
    "    - [LDA Revisited: Entropy, Prior and Convergence](https://static.aminer.org/pdf/fa/cikm2016/lfp0616-zhangAemb.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
