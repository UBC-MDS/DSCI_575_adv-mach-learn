{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DSCI 575: Advanced Machine Learning (in the context of Natural Language Processing (NLP) applications)\n",
    "\n",
    "UBC Master of Data Science program, 2019-20\n",
    "\n",
    "Instructor: Varada Kolhatkar [ʋəɾəda kɔːlɦəʈkər]\n",
    "\n",
    "\n",
    "## Lecture 8: Data generators, using word embeddings with RNNs, image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys, re, os\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Thanks to Firas for the following code for making jupyter RISE slides pretty! \n",
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "from pathlib import Path\n",
    "path = Path.home() / \".jupyter\" / \"nbconfig\"\n",
    "cm = BaseJSONConfigManager(config_dir=str(path))\n",
    "tmp = cm.update(\n",
    "        \"rise\",\n",
    "        {\n",
    "            \"theme\": \"serif\",\n",
    "            \"transition\": \"fade\",\n",
    "            \"start_slideshow_at\": \"selected\",            \n",
    "            \"width\": \"100%\",\n",
    "            \"height\": \"100%\",\n",
    "            \"header\": \"\",\n",
    "            \"footer\":\"\",\n",
    "            \"scroll\": True,\n",
    "            \"enable_chalkboard\": True,\n",
    "            \"slideNumber\": True,\n",
    "            \"center\": False,\n",
    "            \"controlsLayout\": \"edges\",\n",
    "            \"slideNumber\": True,\n",
    "            \"hash\": True,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "     font-size: 130%;\n",
       "}\n",
       "\n",
       "body.rise-enabled div.inner_cell>div.input_area {\n",
       "    font-size: 100%;\n",
       "}\n",
       "\n",
       "body.rise-enabled div.output_subarea.output_text.output_result {\n",
       "    font-size: 100%;\n",
       "}\n",
       "body.rise-enabled div.output_subarea.output_text.output_stream.output_stdout {\n",
       "  font-size: 150%;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
    "     font-size: 130%;\n",
    "}\n",
    "\n",
    "body.rise-enabled div.inner_cell>div.input_area {\n",
    "    font-size: 100%;\n",
    "}\n",
    "\n",
    "body.rise-enabled div.output_subarea.output_text.output_result {\n",
    "    font-size: 100%;\n",
    "}\n",
    "body.rise-enabled div.output_subarea.output_text.output_stream.output_stdout {\n",
    "  font-size: 150%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning outcomes\n",
    "\n",
    "From this lecture you will be able to \n",
    "\n",
    "- explain why do we need data generators\n",
    "- implement a data generator for your application \n",
    "- explain how do we use word embeddings with RNNs/LSTMs\n",
    "- explain at a high-level how can we combine LSTMs and CNNs for image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data generators: Motivation \n",
    "\n",
    "- In the last lecture, we saw an application of LSTMs in text generation\n",
    "    - We trained a character-level LSTM model to generate text on a toy dataset.\n",
    "    - What's the size of `X` in our toy example? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4419, 25, 34)\n",
      "(4419, 34)\n",
      "Need to load 3756150 bool values\n"
     ]
    }
   ],
   "source": [
    "# The hyperparameters in our model \n",
    "n_examples = 4419\n",
    "seq_length = 25\n",
    "n_vocab = 34\n",
    "# Let's create X and y\n",
    "X = np.zeros((n_examples, seq_length, n_vocab),dtype=bool)\n",
    "y = np.zeros((n_examples, n_vocab))\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print('Need to load %d bool values'%(np.prod(X.shape)))   \n",
    "# This is how we trained the model \n",
    "#self.model.fit(X, y,  \n",
    "#               epochs=epochs, \n",
    "#               batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 100, 100)\n",
      "(1000000, 100)\n",
      "Need to load 10000000000 bool values\n"
     ]
    }
   ],
   "source": [
    "# The pre-trained model you are using in your lab 4\n",
    "# The hyperparameters in our model \n",
    "# approximately \n",
    "n_examples = 1000000\n",
    "seq_length = 100\n",
    "n_vocab = 100\n",
    "# Let's create X and y\n",
    "X = np.zeros((n_examples, seq_length, n_vocab),dtype=bool)\n",
    "y = np.zeros((n_examples, n_vocab))\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print('Need to load %d bool values'%(np.prod(X.shape)))   \n",
    "# This is how we trained the model \n",
    "#self.model.fit(X, y,  \n",
    "#               epochs=epochs, \n",
    "#               batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data generators: motivation \n",
    "\n",
    "- Do we need to load the whole dataset all at once?\n",
    "- If we are doing SGD or truncated backprop through time in case of RNNs, we don't.\n",
    "- So the idea is to load a minibatch from the disk into the memory at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data generators: How do we do it? \n",
    "1. Write a data generator function\n",
    "2. Create a data generator\n",
    "3. `fit` your model with the created data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Define a data generator function \n",
    "# Attribution: The following code is adapted from \n",
    "# https://developers.google.com/machine-learning/guides/text-classification/appendix\n",
    "\n",
    "def data_generator(X, y, num_features, batch_size = 128):\n",
    "    \"\"\"Generates batches of vectorized texts for training/validation.\n",
    "\n",
    "    # Arguments\n",
    "        x: np.matrix, feature matrix.\n",
    "        y: np.ndarray, labels.\n",
    "        num_features: int, number of features.\n",
    "        batch_size: int, number of samples per batch.\n",
    "\n",
    "    # Returns\n",
    "        Yields feature and label data in batches.\n",
    "    \"\"\"\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    if num_samples % batch_size:\n",
    "        num_batches += 1\n",
    "\n",
    "    while 1:\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            if end_idx > num_samples:\n",
    "                end_idx = num_samples\n",
    "            X_batch = X[start_idx:end_idx]\n",
    "            y_batch = y[start_idx:end_idx]\n",
    "            yield X_batch, y_batch            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we do it? \n",
    "- Step 2: create a generator by calling `data_generator` \n",
    "- Step 3: `tf.keras.model.fit_generator` instead of `tf.keras.model.fit`\n",
    "- Note that the latest verion `tf.keras.model.fit` supports generators. [See this](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit_generator).\n",
    "- (Optional) Check out a demo of using generators in [this notebook](code/LSTM-character-based-text-generation-2.0.ipynb). \n",
    "    - You will have to try it on Google Colab.\n",
    "    - To convince yourself that you need data generator in this case, try to run the model with `fit` first and examine what you observe. \n",
    "    - You might have to struggle a bit to get it working in your environment. Take it as part of the learning process.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data generators concluding remarks\n",
    "\n",
    "- A useful technique if you want to do large-scale ML \n",
    "- Very useful especially with text, images, and video data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using word embeddings with RNNs\n",
    "\n",
    "- You might be wondering about how do we actually use word embeddings with ML models? \n",
    "- In Lecture 2 we saw two (rather unsatisfactory) ways to create document representations by averaging or  concatenating word embeddings. We used these text representations with ML models. \n",
    "- We can conveniently use word embeddings with sequential models such as RNNs and LSTMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Embedding layers in RNNs/LSTMs \n",
    "\n",
    "<img src=\"images/RNN_generation.png\" height=\"1000\" width=\"1000\"> \n",
    "\n",
    "    \n",
    "[Credit](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Embedding layers in RNNs/LSTMs\n",
    "\n",
    "- Two common ways to incorporate embeddings in the network \n",
    "    - Use pre-trained embeddings (transfer learning)\n",
    "    - Initialize embeddings with random weights and learn as part of the training process. This way we get task-specific embeddings.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 10)           360       \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               273408    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 36)                9252      \n",
      "=================================================================\n",
      "Total params: 283,020\n",
      "Trainable params: 283,020\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# In Keras, an embedding layer requires three arguments: \n",
    "# input dimension, output dimension, sequence length\n",
    "vocab_size = 36\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length = 100))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:27, 14691.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# How can you get Glove embeddings for your vocab? \n",
    "# root_dir is where your glove.6B is located\n",
    "from tqdm import tqdm\n",
    "root_dir = '/Users/kvarada/MDS/2018-19/575/data'\n",
    "glove_dir = os.path.join(root_dir,'glove.6B')\n",
    "embeddings_index = {} \n",
    "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print(f'Found {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "words = ['data', 'science', 'image', 'caption']\n",
    "embedding_dim = 200\n",
    "idxtoword = {}\n",
    "wordtoidx = {}\n",
    "vocab_size = 5\n",
    "ix = 1\n",
    "for w in words:\n",
    "    wordtoidx[w] = ix\n",
    "    idxtoword[ix] = w\n",
    "    ix += 1\n",
    "    \n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in wordtoidx.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.74819982e-01,  3.56139988e-02,  4.85900015e-01,  9.40869972e-02,\n",
       "        6.17579997e-01,  2.00950000e-02, -5.32760024e-01,  5.62810004e-01,\n",
       "        5.61520010e-02, -1.15460001e-01, -3.29210013e-01, -4.50159982e-02,\n",
       "        5.10930002e-01,  7.94809982e-02,  4.99009997e-01,  3.65260005e-01,\n",
       "       -1.64450005e-01,  4.89789993e-01, -3.26680005e-01, -1.02959998e-01,\n",
       "       -6.43630028e-01,  2.41470003e+00, -2.09150001e-01, -2.29760006e-01,\n",
       "       -3.92089993e-01,  6.89310014e-01, -3.91079992e-01,  2.02930003e-01,\n",
       "        4.77270007e-01,  2.99600005e-01, -4.12849993e-01, -5.24999984e-02,\n",
       "        2.68130004e-01, -4.07340005e-02,  9.45689976e-01, -8.22300017e-01,\n",
       "       -5.88079989e-02, -1.04180001e-01, -6.38120025e-02,  3.66329998e-02,\n",
       "        8.74790028e-02, -2.24649996e-01,  2.12590005e-02,  9.59599972e-01,\n",
       "       -1.93100005e-01,  4.55760002e-01,  4.53520000e-01, -1.10679996e+00,\n",
       "        3.89319994e-02, -2.41340008e-02, -2.83039987e-01, -1.97080001e-01,\n",
       "        1.83649994e-02, -6.13059998e-02, -4.21530008e-01,  4.18870002e-01,\n",
       "       -1.45710006e-01, -2.46509999e-01,  4.10189986e-01, -3.74299996e-02,\n",
       "        2.67719999e-02,  5.08920014e-01,  1.50539994e-01, -5.52139997e-01,\n",
       "        2.28200004e-01, -2.74520010e-01,  3.60479988e-02,  5.07120013e-01,\n",
       "       -1.67270005e-01, -2.13719994e-01,  3.15789990e-02,  8.02940011e-01,\n",
       "        3.69139999e-01, -7.89010003e-02,  2.88630009e-01,  1.07299995e+00,\n",
       "        3.77739996e-01, -1.58649996e-01,  7.30499998e-02, -5.50369993e-02,\n",
       "       -4.31580007e-01,  3.51740003e-01,  1.66549996e-01,  2.08049998e-01,\n",
       "       -2.11510003e-01, -1.48560002e-01, -1.35179996e-01, -2.36249998e-01,\n",
       "        1.99899995e+00, -8.67330015e-01,  1.76660001e-01,  1.46899998e-01,\n",
       "        4.92249995e-01,  5.88299990e-01,  2.53829986e-01,  3.01849991e-01,\n",
       "        6.15849972e-01,  1.35030001e-01,  2.45790005e-01,  8.49049985e-02,\n",
       "        4.28730011e-01, -4.03409988e-01, -3.02060008e-01,  9.90930021e-01,\n",
       "        1.63210005e-01,  5.29420018e-01,  4.13969994e-01,  4.58009988e-02,\n",
       "       -1.35650003e+00,  4.81740013e-02,  4.23590004e-01,  2.23149993e-02,\n",
       "        7.70190001e-01, -8.41429979e-02,  2.44250000e-02,  9.63949978e-01,\n",
       "        3.85899991e-01,  6.73400015e-02, -2.11479999e-02, -4.34500009e-01,\n",
       "        2.06829995e-01,  4.36279997e-02,  2.12439999e-01, -1.18060000e-01,\n",
       "       -8.51189971e-01, -2.17950001e-01,  1.58120006e-01,  6.50569975e-01,\n",
       "       -3.49150002e-01,  2.34990001e-01, -9.02920008e-01,  4.84990001e-01,\n",
       "        4.02689993e-01,  7.26920009e-01,  4.99069989e-01, -6.83519989e-03,\n",
       "       -2.63940006e-01, -3.40290010e-01,  3.92560005e-01,  1.48980007e-01,\n",
       "       -9.62119997e-02, -2.61709988e-01, -2.91440010e-01, -2.10429996e-01,\n",
       "        1.02489996e+00,  3.09549987e-01, -5.19879997e-01, -4.26330000e-01,\n",
       "        9.25679982e-01, -6.38989985e-01, -1.71090007e-01, -5.72030008e-01,\n",
       "        3.10279995e-01, -1.75459997e-03, -1.64470002e-01,  4.01749998e-01,\n",
       "        2.54500002e-01,  3.12599987e-01,  8.22149992e-01, -1.35189998e+00,\n",
       "       -3.10110003e-01, -5.37540019e-01, -5.38760006e-01,  4.19530004e-01,\n",
       "        1.05080001e-01,  7.50029981e-01, -2.21159995e-01,  2.14120001e-01,\n",
       "       -1.78910002e-01,  6.73650026e-01,  3.53210002e-01, -1.94130000e-02,\n",
       "        3.42889994e-01, -2.08800003e-01, -5.13740003e-01,  2.03329995e-01,\n",
       "       -5.14630020e-01,  5.30250013e-01, -3.22010010e-01, -1.30319998e-01,\n",
       "        9.78619993e-01,  1.48479998e-01,  3.24699990e-02,  2.30389997e-01,\n",
       "        5.33819973e-01, -5.98680019e-01,  7.40119994e-01, -5.39390028e-01,\n",
       "        4.56209987e-01,  2.91390002e-01, -5.40340006e-01, -1.57790005e-01,\n",
       "       -8.96009989e-03,  5.95449984e-01, -2.31800005e-02, -4.60049987e-01,\n",
       "        1.03049995e-02, -1.17660001e-01,  1.16269998e-01, -3.62709999e-01])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[wordtoidx['data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Concluding remarks \n",
    "\n",
    "- You should generally be using embeddings in RNNs/LSTMs for text data.\n",
    "- Reduces the number of parameters dramatically\n",
    "- Feeds in word similarity and relatedness information in the network\n",
    "- Also, gives the model an ability generalize better. \n",
    "- Example: \n",
    "    <blockquote>\n",
    "    I have to make sure to feed the cat .\n",
    "    </blockquote>\n",
    "\n",
    "    - Would a Markov model of language able to generate the sequence \"feed the dog\" when you only have evidence for the following sequence in the corpus?     \n",
    "    - If we represent words with word embedding to an RNN, it would be able to generate \"feed the dog\" because it has the information that \"dogs\" are similar to \"cats\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image captioning with LSTMs and CNNs\n",
    "\n",
    "- You can access the code from the video [here](code/image-captioning-demo.ipynb). \n",
    "\n",
    "- LSTMs are expensive to train and, I have trained this model on [Google colab](https://colab.research.google.com/notebooks/welcome.ipynb). \n",
    "\n",
    "(Optional) If you feel adventurous, you can try to download the the data and run it on your own! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary and wrap-up \n",
    "\n",
    "This is what I promised you in the first lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 1 \n",
    "\n",
    "- Representation Learning\n",
    "- Word vectors and word embeddings\n",
    "\n",
    "<img src=\"images/tsne_example.png\" height=\"1000\" width=\"1000\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 2\n",
    "\n",
    "- Markov models\n",
    "- Hidden Markov models\n",
    "\n",
    "<img src=\"images/Markov_autocompletion.png\" height=\"800\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 3\n",
    "\n",
    "- Topic modeling (Latent Dirichlet Allocation (LDA))\n",
    "    - Suppose given a large collection of documents, you are asked to \n",
    "        - Infer different topics in the documents\n",
    "        - Pull all documents about a certain topic    \n",
    "- Introduction to Recurrent Neural Networks (RNNs)\n",
    "<img src=\"images/TM_dist_topics_words_blei.png\" height=\"1000\" width=\"1000\"> \n",
    "\n",
    "(Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 4 \n",
    "\n",
    "- LSTMs \n",
    "- RNN applications: Image captioning \n",
    "\n",
    "<blockquote>\n",
    "\n",
    "<img src=\"images/image_captioning.png\" width=\"1000\" height=\"1000\">\n",
    "\n",
    "<p style=\"font-size:30px\"></p>\n",
    "</blockquote>    \n",
    "\n",
    "[Source](https://cs.stanford.edu/people/karpathy/sfmltalk.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What we did not cover ...\n",
    "\n",
    "- If you are excited about NLP, here are some more things to explore: \n",
    "    - [Attention](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\n",
    "    - [Transformers](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)\n",
    "    - [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Final remarks \n",
    "\n",
    "That's all! I hope you learned something from the course that's useful for you. I certainly learned how to make videos :). \n",
    "\n",
    "I wish you every success in your job search!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### UBC teaching evaluations \n",
    "\n",
    "- Feel free to do them now if you like. \n",
    "- Evaluation link: https://eval.ctlt.ubc.ca/science"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
