{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/575_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Lecture 7: Introduction to self-attention and transformers\n",
    "\n",
    "UBC Master of Data Science program, 2022-23\n",
    "\n",
    "Instructor: Varada Kolhatkar\n",
    "\n",
    "> [Attention is all you need!](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture plan, imports, LO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "### Lecture plan \n",
    "\n",
    "- Recap: iClicker\n",
    "- Self-attention layers\n",
    "- Break\n",
    "- iClicker questions\n",
    "- Positional embeddings\n",
    "- Transformer blocks \n",
    "- Multihead attention\n",
    "- Final comments and summay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learning outcomes\n",
    "\n",
    "From this lecture you will be able to \n",
    "\n",
    "- Broadly explain the problem of vanishing gradients. \n",
    "- Broadly explain the limitations of RNNs. \n",
    "- Explain the idea of self-attention. \n",
    "- Describe the three core operations in self-attention. \n",
    "- Explain the query, key, and value roles in self-attention. \n",
    "- Explain the role of linear projections for query, key, and value in self-attention. \n",
    "- Explain transformer blocks. \n",
    "- Explain the advantages of using transformers over LSTMs. \n",
    "- Broadly explain the idea of multihead attention. \n",
    "- Broadly explain the idea of positional embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributions\n",
    "\n",
    "This material is heavily based on [Jurafsky and Martin, Chapter 9]((https://web.stanford.edu/~jurafsky/slp3/9.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you\n",
    "\n",
    "- Suppose you are training a vanilla RNN with one hidden layer. \n",
    "    - input representation is of size 200\n",
    "    - output layer is of size 200\n",
    "    - the hidden size is 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 7.1: Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) The shape of matrix $U$ between hidden layers in consecutive time steps is going to be $200 \\times 200$. \n",
    "- (B) The output of the hidden layer is going to be a $100$ dimensional vector.  \n",
    "- (C) In bidirectional RNNs, if we want to combine the outputs of two RNNs with element-wise addition, the hidden sizes of the two RNNs have to be the same.  \n",
    "- (D) Word2vec skipgram model is likely to suffer from the problem of vanishing gradients. \n",
    "- (E) In the forward pass, in each time step in RNNs, you calculate the output of the hidden layer by multiplying the input $x$ by the weight matrix $W$ or $W_{xh}$ and applying non-linearity. \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 7.1: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "- (A) False\n",
    "- (B) True\n",
    "- (C) True\n",
    "- (D) False\n",
    "- (E) False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention networks: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "What kind of neural network models are at the core of all state-of-the-art NLP models (e.g., BERT, GPT3, ChatGPT, GPT4)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/gpt3-transformer-blocks.gif)\n",
    "\n",
    "<!-- <center>\n",
    "<img src=\"img/gpt3-transformer-blocks.gif\" height=\"500\" width=\"500\"> \n",
    "</center>    \n",
    " -->\n",
    "[Source](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some reasonable predictions for the missing words in the following sentences?\n",
    "\n",
    "> ##### I am studying data science at the University of British Columbia Point Grey campus in Vancouver because I want to work as a ___\n",
    "\n",
    "> ##### The students in the exam where the fire alarm is ringing __ really stressed. \n",
    "\n",
    "<br><br>\n",
    "- What do we want when processing text data? \n",
    "    - Able to represent time\n",
    "    - Capture how words relate to each other over long distances  \n",
    "- We have seen several models in the course which represent time. \n",
    "    - Week 1 and 2: Markov models and hidden Markov models\n",
    "    - Week 3: RNNs which are supposed to be better at capturing long distance dependencies compared to Markov models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with RNNs \n",
    "\n",
    "- In practice, you'll hardly see people using vanilla RNNs because they are quite hard to train for tasks that require access to distant information.\n",
    "- There are two main problems: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    "\n",
    "- In RNNs, the hidden layer and the weights that determine the values in the hidden layer are asked to perform two tasks simultaneously:\n",
    "    - Providing information useful for current decision\n",
    "    - Updating and carrying forward information required for future decisions\n",
    "- Despite having access to the entire previous sequence, the information encoded in hidden states of RNNs is fairly local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the examples below in the context of language modeling. \n",
    "\n",
    "> The **students** in the exam where the fire _alarm_ _is_ ringing **are** really stressed. \n",
    "\n",
    "> The flies munching on the banana that is lying under the tree which is in full bloom **are** really happy. \n",
    "\n",
    "- Assigning high probability to **_is_** following *alarm* is straightforward since it provides a local context for singular agreement. \n",
    "- However, assigning a high probability to **_are_** following _ringing_ is quite difficult because not only the plural _students_ is distant, but also the intervening context involves singular constituents. \n",
    "- Ideally, we want the network to retain the distant information about the plural **_students_** until it's needed while still processing the intermediate parts of the sequence correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem 2: Vanishing gradients\n",
    "\n",
    "- Another difficulty with training RNNs arises from the need to backpropagate the error signal back through time. \n",
    "- Recall that we learn RNNs with \n",
    "    - Forward pass \n",
    "    - Backward pass (backprop through time)\n",
    "    \n",
    "- Computing new states and output in RNNs\n",
    "\n",
    "$$\n",
    "h_t = g(Uh_{t-1} + Wx_t + b_1)\\\\\n",
    "y_t = \\text{softmax}(Vh_t + b_2)\n",
    "$$ \n",
    "\n",
    "![](img/RNN-dynamic-model.png)\n",
    "\n",
    "<!-- <center>\n",
    "<img src=\"img/RNN-dynamic-model.png\" height=\"400\" width=\"400\"> \n",
    "</center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall: Backpropagation through time\n",
    "\n",
    "- When we backprop with feedforward neural networks\n",
    "    - Take the gradient (derivative) of the loss with respect to the parameters. \n",
    "    - Change parameters to minimize the loss. \n",
    "\n",
    "- In RNNs we use a generalized version of backprop called Backpropogation Through Time (BPTT)\n",
    "    - Calculating gradient at each output depends upon the current time step as well as the previous time steps. \n",
    "\n",
    "![](img/RNN_loss.png)\n",
    "\n",
    "<!-- <center>    \n",
    "<img src=\"img/RNN_loss.png\" height=\"600\" width=\"600\"> \n",
    "</center>\n",
    "     -->\n",
    "[Credit](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- So in the backward pass of RNNs, we have to multiply many derivatives together, which very often results in\n",
    "    - vanishing gradients (gradients becoming very small and eventually driven to zero) in case of long sequences\n",
    "- If we have a vanishing gradient, we might not be able to update our weights reliably. \n",
    "- So we are not able to capture long-term dependencies, which kind of defeats the whole purpose of using RNNs.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To address these issues more complex network architectures have been designed with the goal of maintaining relevant context over time by enabling the network to learn to forget the information that is no longer needed and to remember information required for decisions still to come. \n",
    "- Most commonly used models are \n",
    "    - The Long short-term memory network (LSTM)\n",
    "    - Gated Recurrent Units (GRU)\n",
    "- That said, even with these models, for long sequences, there is still a loss of relevant information and difficulties in training. \n",
    "- Also, inherently sequential nature of RNNs/LSTMs make them hard to parallelize. So they are slow to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "- This led to development of **transformers**. \n",
    "- **Transformers** provide an approach to sequence processing but they eliminate recurrent connections in RNNs and LSTMs.   \n",
    "- Similar to RNNs or LSTMs, they map sequences of input vectors $(x_1, \\dots, x_n)$ to sequences of output vectors $(y_1, \\dots, y_n)$ of the same length. \n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- They are much faster to train compared to LSTMs and much better at capturing long distance dependencies. \n",
    "- They are at the core of all state-of-the-art NLP models (e.g., BERT, GPT2, GPT3).\n",
    "- There are two main innovations which make these models work so well.  \n",
    "    - **Self-attention**\n",
    "    - **Positional embeddings/encodings**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "Inspired by the idea of human attention. When we process information, we often selectively focus on specific parts of the input, giving more attention to relevant information and less attention to irrelevant information. \n",
    "\n",
    "> #### I am studying science at UBC because I want to be a **scientist**. \n",
    "\n",
    "Suppose you are focusing on the word **scientist** in this sequence. Which words in the context are most relevant to **scientist**? Assign a qualitative weight (high or low) to each context word below.   \n",
    "\n",
    "- **scientist** attending to **I**: low weight \n",
    "- **scientist** attending to **am**: \n",
    "- **scientist** attending to **studying**: \n",
    "- **scientist** attending to **science**: \n",
    "- **scientist** attending to **at**: \n",
    "- **scientist** attending to **UBC**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Self-attention allows a network to directly extract and use information from arbitrarily large contexts without the need to pass it through intermediate recurrent connections as in RNNs. \n",
    "- Below is a single backward looking self-attention layer which maps sequences of input vectors $(x_1, \\dots, x_n)$ to sequences of output vectors $(y_1, \\dots, y_n)$ of the same length. \n",
    "- When processing an item at time $t$, the model has access to all of the inputs up to and including the one under consideration. \n",
    "- It does not have access to the input beyond the current one. \n",
    "- Note that unlike RNNs or LSTMs, each computation can be done independently; it does not depend upon the previous computation which allows us to easily parallelize forward pass and the training of such models. \n",
    "\n",
    "![](img/self_attention.png)\n",
    "<!-- <img src=\"img/self_attention.png\" width=\"500\" height=\"500\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core idea \n",
    "- We want to be able to compare a token of our interest to a collection of other tokens in a way that reveals their relevance in the current context.\n",
    "- For each token in the sequence, we assign a **weight** based on how relevant it is to the token under consideration. \n",
    "- Calculate the output for the current token based on these weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Calculating the output $y$ for the token _note_ in the given context\n",
    "\n",
    "![](img/self-attention-note1.png)\n",
    "\n",
    "<!-- <img src=\"img/self-attention-note1.png\" width=\"500\" height=\"500\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The key operations in self-attention\n",
    "\n",
    "In order to calculate the output $y_i$\n",
    "\n",
    "- We score token $x_i$ with all previous tokens $x_j$ by taking the dot product between them. \n",
    "$$\\text{score}(x_i, x_j) = x_i \\cdot x_j$$\n",
    "\n",
    "- We apply $\\text{softmax}$ on these scores to get probability distribution over these scores. \n",
    "$$\\alpha_{ij} = \\text{softmax}(\\text{score}(x_i \\cdot x_j)), \\forall j \\leq i$$\n",
    "\n",
    "- The output is the weighted sum of the inputs seen so far, where the weights correspond to the $\\alpha$ values calculated above. \n",
    " $$y_i = \\sum_{j \\leq i} \\alpha_{ij}x_j$$\n",
    " \n",
    "These three operations represent the core of an attention-based approach. These operations can be carried out independently for each input allowing easy parallelism. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query, Key, and Value roles\n",
    "\n",
    "Note that in the process of calculating outputs corresponding to each input, each input embedding plays three kinds of roles. \n",
    "\n",
    "- **Query**: _the current focus of attention_ when being compared to all previous inputs. \n",
    "- **Key**: _a preceding input_ being compared to the current focus of attention.    \n",
    "- **Value**: used to compute the output for the current focus of attention. \n",
    "\n",
    "For these three roles transformer introduces three weight matrices: $W^Q, W^K, W^V$. These weights will be used to project each input vector $x_i$ into its role as a key, query, or value.\n",
    "\n",
    "$$q_i = W^Qx_i$$\n",
    "$$k_i = W^Kx_i$$\n",
    "$$v_i = W^Vx_i$$\n",
    "\n",
    "For now let's assume that all these weight matrices have the same dimensionality and so the projected vectors in each case are going to be of the same size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these projections our equations become: \n",
    "\n",
    "- We score the $x_i$ with all previous tokens $x_j$ by taking the dot product between $x_i$'s query vector $q_i$ and $x_j$'s key vector $k_j$:  \n",
    "$$\\text{score}(x_i, x_j) = q_i \\cdot k_j$$\n",
    "\n",
    "- The softmax calculation remains the same but the output calculation for $y_i$ is now based on a weighted sum over the projected vectors $v$:\n",
    " $$y_i = \\sum_{j \\leq i} \\alpha_{ij}v_j$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/self_attention_ex.png)\n",
    "\n",
    "<!-- <img src=\"img/self_attention_ex.png\" width=\"400\" height=\"400\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's calculate the output of _**note**_ in the following sequence with $K, Q, V$ matrices.  \n",
    "> string melancholic note\n",
    "- Suppose input embedding is of size 300. \n",
    "- Suppose the projection matrices $W^k, W^q, W^v$ are of shape $300 \\times 100$. \n",
    "- So word$_k$, word$_q$, word$_v$ provide 100-dimensional projections of each word corresponding to the key, query and value roles. For example, note$_k$, note$_q$, bite$_v$ represent 100-dimensional projections of the word **note** corresponding to its key, query, and value roles, respectively.\n",
    "- The dot products will be calculated between the appropriate query and key projections. In this example, we will calculate the following dot products:\n",
    "    - $\\text{note}_q \\cdot \\text{string}_k$\n",
    "    - $\\text{note}_q \\cdot \\text{melancholic}_k$    \n",
    "    - $\\text{note}_q \\cdot \\text{note}_k$\n",
    "- We apply softmax on these dot products. Suppose the softmax output in this toy example is \n",
    "\\begin{bmatrix} 0.005 & 0.085 & 0.91 \\end{bmatrix}\n",
    "- So we have weights associated with three input words: _string_ (0.005), _melancholic_ (0.085) and _note_ (0.91)\n",
    "- We can calculate the output as the weighted sum of the inputs. Here we will use the value projections of the inputs: $0.005 \\times \\text{string}_v + 0.085 \\times \\text{melancholic}_v + 0.91 \\times \\text{note}_v$\n",
    "- Since we will be adding 100 dimensional vectors (size of our projections), the dimensionality of the output $y_3$ is going to be 100. \n",
    "\n",
    "![](img/self-attention-note2.png)\n",
    "\n",
    "<!-- <img src=\"img/self-attention-note2.png\" width=\"500\" height=\"500\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the dot products\n",
    "\n",
    "- The result of a dot product can be arbitrarily large and exponentiating such values can lead to numerical issues and problems during training. \n",
    "- So the dot products are usually scaled before applying the softmax. \n",
    "- The most common scaling is where we divide the dot product by the square root of the dimensionality of the query and the key vectors. \n",
    "$$\\text{score}(x_i, x_j) = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is how we calculate a single output of a single time step $i$. \n",
    "- Would the output calculation at different time steps be dependent upon each other? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient calculations with matrix multiplication \n",
    "\n",
    "- $X_{N \\times d} \\rightarrow$ matrix of all tokens in a sequence of length $N$ with each token represented with a $d$ dimensional embedding. Each row of $X$ is embedding representation of one token of the input. Then we can calculate $Q, K, V$ as follows.\n",
    "\n",
    "$$Q = XW^Q$$\n",
    "$$K = XW^K$$\n",
    "$$V = XW^V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With these, we can now calculate all the query-key scores simultaneously as $Q \\times K$. \n",
    "\n",
    "![](img/self_attention_calc_all.png)\n",
    "\n",
    "<!-- <img src=\"img/self_attention_calc_all.png\" width=\"300\" height=\"300\"> -->\n",
    "\n",
    "- We can then apply softmax on all rows and multiply the resulting matrix by $V$.\n",
    "\n",
    "$$SelfAttention(Q, K, V) = \\text{softmax}(\\frac{QK}{\\sqrt{d_k}})V$$\n",
    "\n",
    "- Finally, we get output sequence $y_1, \\dots, y_n$.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What's the problem with the approach above?\n",
    "    - This process goes a bit too far since the calculation of the comparisons in $QK$ results in a score for each value to each key value, _including those that follow the query_. \n",
    "    - Is this appropriate in the setting of language modeling? \n",
    "\n",
    "![](img/self_attention_calc_partial.png)\n",
    "\n",
    "<!-- <img src=\"img/self_attention_calc_partial.png\" width=\"300\" height=\"300\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break\n",
    "\n",
    "![](img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 7.2: Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) The main difference between the RNN layer and a self-attention layer is that \n",
    "in self-attention, we pass the information without intermediate recurrent connections. \n",
    "- (B) In self-attention, the output $y_i$ of input $x_i$ at time $i$ is a scalar. \n",
    "- (C) Calculating attention weights is quadratic in the length of the input \n",
    "since we need to compute dot products between each pair of tokens in\n",
    "the input.  \n",
    "- (D) Self-attention results in contextual embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 8.2: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "- (A) True\n",
    "- (B) False\n",
    "- (C) True\n",
    "- (D) True \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional embeddings\n",
    "\n",
    "- Also called **positional encodings**. \n",
    "- Are we capturing word order when we calculate $y_3$? In other words if you scramble the order of the inputs, will you get exactly the same answer for $y_3$? \n",
    "\n",
    "![](img/self-attention-note2.png)\n",
    "\n",
    "<!-- <img src=\"img/self-attention-note2.png\" width=\"500\" height=\"500\"> -->\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we capture word order and positional information?\n",
    "- A simple solution is positional embeddings!\n",
    "- To produce an input embedding that captures positional information, \n",
    "    - We create positional embedding for each position (e.g., 1, 2, 3, ...)\n",
    "    - We add it to the corresponding input embedding \n",
    "    - The resulting embedding has some information about the input along with its position in the text\n",
    "    \n",
    "- Where do we get these positional embeddings? The simplest method is to start with randomly initialized embeddings corresponding to each possible input position and learn them along with other parameters during training. \n",
    "\n",
    "![](img/positional-embeddings.png)\n",
    "<!-- <img src=\"img/positional-embeddings.png\" width=\"500\" height=\"500\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer blocks and multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer blocks\n",
    "\n",
    "- In many advanced architectures, you will see transformer blocks which consists of\n",
    "    - The self-attention layer\n",
    "    - Additional feedforward layers\n",
    "    - Residual connections\n",
    "    - Normalizing layers\n",
    "\n",
    "![](img/transformer_block.png)\n",
    "\n",
    "<!-- <img src=\"img/transformer_block.png\" width=\"400\" height=\"400\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The input and output dimensions of these layers are matched so that they can be stacked. \n",
    "- In deep networks, **residual connections** are connections that pass information from a lower layer to a higher layer without going through the intermediate layer. Why? It has been shown that allowing information from the activation going forward and the gradient going backwards to skip a layer improves learning and gives higher level layers direct access to information from lower layers. \n",
    "- We then have a summed vector (projected output of the attention or feedforward layer + input of the attention or feedforward layers). \n",
    "- **Layer normalization or layer norm** normalizes the resulting vector which improves training performance in deep neural networks keeping the values of a hidden layer in a range that facilitates gradient-based training. Layer norm applies something similar to `StandardScaler` so that the mean is 0 and standard deviation is 1 in the vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "\n",
    "- Different words in a sentence can relate to each other in many different ways simultaneously. \n",
    "- Consider the sentence below. \n",
    "> The cat was scared because it didn't recognize me in my mask. \n",
    "\n",
    "Let's look at all the dependencies in this sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"284f1de792f246bc900dcbbae7f0e96b-0\" class=\"displacy\" width=\"2325\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">cat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">scared</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">because</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">it</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">did</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">n't</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">recognize</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">me</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">my</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">mask .</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-284f1de792f246bc900dcbbae7f0e96b-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-284f1de792f246bc900dcbbae7f0e96b-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-284f1de792f246bc900dcbbae7f0e96b-0-1\" stroke-width=\"2px\" d=\"M245,439.5 C245,352.0 380.0,352.0 380.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-284f1de792f246bc900dcbbae7f0e96b-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,441.5 L237,429.5 253,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-284f1de792f246bc900dcbbae7f0e96b-0-2\" stroke-width=\"2px\" d=\"M420,439.5 C420,352.0 555.0,352.0 555.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-284f1de792f246bc900dcbbae7f0e96b-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M555.0,441.5 L563.0,429.5 547.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-284f1de792f246bc900dcbbae7f0e96b-0-3\" stroke-width=\"2px\" d=\"M770,439.5 C770,89.5 1445.0,89.5 1445.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-284f1de792f246bc900dcbbae7f0e96b-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,441.5 L762,429.5 778,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-284f1de792f246bc900dcbbae7f0e96b-0-4\" stroke-width=\"2px\" d=\"M945,439.5 C945,177.0 1440.0,177.0 1440.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-284f1de792f246bc900dcbbae7f0e96b-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,441.5 L937,429.5 953,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-284f1de792f246bc900dcbbae7f0e96b-0-5\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,264.5 1435.0,264.5 1435.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-284f1de792f246bc900dcbbae7f0e96b-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,441.5 L1112,429.5 1128,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-284f1de792f246bc900dcbbae7f0e96b-0-6\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,352.0 1430.0,352.0 1430.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-284f1de792f246bc900dcbbae7f0e96b-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,441.5 L1287,429.5 1303,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-284f1de792f246bc900dcbbae7f0e96b-0-7\" stroke-width=\"2px\" d=\"M420,439.5 C420,2.0 1450.0,2.0 1450.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-284f1de792f246bc900dcbbae7f0e96b-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1450.0,441.5 L1458.0,429.5 1442.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-284f1de792f246bc900dcbbae7f0e96b-0-8\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,352.0 1605.0,352.0 1605.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-284f1de792f246bc900dcbbae7f0e96b-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1605.0,441.5 L1613.0,429.5 1597.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-284f1de792f246bc900dcbbae7f0e96b-0-9\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,264.5 1785.0,264.5 1785.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-284f1de792f246bc900dcbbae7f0e96b-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1785.0,441.5 L1793.0,429.5 1777.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-284f1de792f246bc900dcbbae7f0e96b-0-10\" stroke-width=\"2px\" d=\"M1995,439.5 C1995,352.0 2130.0,352.0 2130.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-284f1de792f246bc900dcbbae7f0e96b-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,441.5 L1987,429.5 2003,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-284f1de792f246bc900dcbbae7f0e96b-0-11\" stroke-width=\"2px\" d=\"M1820,439.5 C1820,264.5 2135.0,264.5 2135.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-284f1de792f246bc900dcbbae7f0e96b-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2135.0,441.5 L2143.0,429.5 2127.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"The cat was scared because it did n't recognize me in my mask .\")\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So a single attention layer usually is not enough to capture all different kinds of parallel relations between inputs. \n",
    "- Transformers address this issue with **multihead self-attention layers**.\n",
    "- These self-attention layers are called **heads**.\n",
    "- They are at the same depth of the model, operate in parallel, each with a different set of parameters. \n",
    "- The idea is that with these different sets of parameters, each head can learn different aspects of the relationships that exist among inputs.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "MultiHeadAttn(X) &= (\\text{head}_1 \\oplus \\text{head}_2 \\dots \\oplus \\text{head}_h)W^O\\\\\n",
    "               Q &= XW_i^Q ; K = XW_i^K ; V = XW_i^V\\\\\n",
    "               \\text{head}_i &= SelfAttention(Q,K,V)\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![](img/multihead_attention.png) -->\n",
    "\n",
    "<img src=\"img/multihead_attention.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention visualization\n",
    "- Similar to RNNs you can stack self-attention layers or multihead self-attention layers on the top of each other.\n",
    "- Let's look at this visualization which shows where the attention of different attention heads is going in multihead attention. \n",
    "    - [Multi-head attention interactive visualization](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=OJKU36QAfqOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final comments and summary\n",
    "\n",
    "- Transformers are non-recurrent networks based on self-attention. \n",
    "- There are two main components of transformers: \n",
    "    - A self-attention layer maps input sequences to output sequences of the same length using attention heads which model how the surrounding words are relevant for the processing of the current word. \n",
    "    - Positional embeddings/encodings  \n",
    "- A transformer block consists of a single attention layer, followed by a feedforward layer with residual connections and layer normalizations. These blocks can be stacked together to create powerful networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Resources\n",
    "Attention-mechanisms and transformers are quite new. But there are many resources on transformers. I'm listing a few resources here. \n",
    "\n",
    "- [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Transformers](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Transformers documentation](https://huggingface.co/transformers/index.html)\n",
    "- [A funny video: I taught an AI to make pasta](https://www.youtube.com/watch?v=Y_NvR5dIaOY)\n",
    "- [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming up: Some applications of transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-excited.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
