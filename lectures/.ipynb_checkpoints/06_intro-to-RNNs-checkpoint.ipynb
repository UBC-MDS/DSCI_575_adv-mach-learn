{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/575_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 6: Introduction to Recurrent Neural Networks (RNNs)\n",
    "\n",
    "UBC Master of Data Science program, 2022-23\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture plan, imports, LO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Lecture plan \n",
    "\n",
    "- Left-over iClicker questions from lecture 5\n",
    "- RNNs motivation\n",
    "- RNN forward pass\n",
    "- Break\n",
    "- iClicker questions\n",
    "- RNN training \n",
    "- RNN architectures\n",
    "- Final comments and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learning outcomes\n",
    "\n",
    "From this lecture you will be able to \n",
    "\n",
    "- Explain the motivation to use RNNs. \n",
    "- Explain how an RNN differs from a feed-forward neural network. \n",
    "- Define vanilla or simple RNNs. \n",
    "- Explain three weight matrices in RNNs. \n",
    "- Explain parameter sharing in RNNs. \n",
    "- Explain how states and outputs are calculated in the forward pass of an RNN. \n",
    "- Explain the backward pass in RNNs at a high level. \n",
    "- Specify different architectures of RNNs and explain how these architectures are used in the context of NLP applications.\n",
    "- Broadly explain character-level text generation with RNNs.\n",
    "- Specify the shapes of weight matrices in RNNs.\n",
    "- Carry out forward pass with RNNs in `PyTorch`.\n",
    "- Explain stacked RNNs and bidirectional RNNs and the difference between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN-generated music! \n",
    "\n",
    "- [Magenta PerformanceRNN](https://www.youtube.com/watch?v=dMhQalLBXIU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/dMhQalLBXIU\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1059a2da0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### An example of a state-of-the-art language model\n",
    "url = \"https://www.youtube.com/embed/dMhQalLBXIU\"\n",
    "# IPython.display.IFrame(url, width=500, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Language is an inherently sequential phenomenon.\n",
    "- This temporal nature of language is reflected in the metaphors used to describe language \n",
    "    - *flow of conversation*, *news feeds*, or *twitter streams*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fixed-length input\n",
    "\n",
    "- ML algorithms we have seen in 571, 572, and 573 work with fixed length input.  \n",
    "    - SVM\n",
    "    - Logistic Regression\n",
    "    - Multi-layer Perceptron\n",
    "\n",
    "- Example of fixed length input\n",
    "$$X = \\begin{bmatrix}1 & 0.8 & \\ldots & 0.3\\\\ 0 & 0 &  \\ldots & 0.4\\\\ 1 & 0.2 &  \\ldots & 0.8 \\end{bmatrix}$$ \n",
    "\n",
    "$$y = \\begin{bmatrix}1 \\\\ 0 \\\\ 1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fixed-length input\n",
    "\n",
    "- When we used these models for sentiment analysis we created a **fixed size** input representation using `CountVectorizer`, where we had simultaneous access to all aspects of the input. \n",
    "\n",
    "$$X = \\begin{bmatrix}\\text{\"@united you're terrible. You don't understand safety\"}\\\\ \\text{\"@JetBlue safety first !! #lovejetblue\"}\\\\ \\text{\"@SouthwestAir truly the best in #customerservice!\"}\\\\ \\end{bmatrix} \\text{ and } y = \\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\end{bmatrix} $$ \n",
    "<br><br>\n",
    "$$X_{counts} = \\begin{bmatrix}1 & 3 & \\ldots & 2\\\\ 1 & 0 & \\ldots & 0\\\\ 0 & 2 & \\ldots & 1\\end{bmatrix} \\text{ and } y = \\begin{bmatrix}1 \\\\ 0 \\\\ 1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentiment analysis using feed-forward neural networks \n",
    "\n",
    "- Reminder: In feed-forward neural networks, \n",
    "    - all connections flow forward (no loops)\n",
    "    - each layer of hidden units is fully connected to the next\n",
    "- We pass fixed sized vector representation of text (e.g., representation created with `CountVectorizer`) as input. \n",
    "- We lose the temporal aspect of text in this representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How about using Markov models? \n",
    "\n",
    "- They have some temporal aspect. \n",
    "\n",
    "![](img/Markov_assumption.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/Markov_assumption.png\" height=\"550\" width=\"550\">  -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall language modeling task \n",
    "\n",
    "- Recall the task of predicting the next word given a sequence. \n",
    "- What's the probability of an upcoming word?  \n",
    "    - $P(w_t|w_1,w_2,\\dots,w_{t-1})$\n",
    "    \n",
    "<blockquote>\n",
    "    I am studying medicine at UBC because I want to work as a ___.\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Recall that when we used Markov models for this task, we made Markov assumption. \n",
    "    - Markov model: $P(w_t|w_1,w_2,\\dots,w_{t-1}) \\approx P(w_t|w_{t-1})$\n",
    "    - Markov model with more context: $P(w_t|w_1,w_2,\\dots,w_{t-1}) \\approx P(w_t|w_{t-2}, w_{t-1})$ \n",
    "- These models do not have memory beyond the previous 2, 3 or maximum $n$ steps and when $n$ becomes larger, there is sparsity problem.  \n",
    "- Also, they have huge RAM requirements because you have to store all ngrams. \n",
    "- Would a Markov model with $n=5$ predict the correct words in the following cases? \n",
    "<blockquote>\n",
    "    I am studying medicine at UBC because I want to work as a <b>__</b>.<br>\n",
    "    I am studying law at UBC because I want to work as a <b>__</b>.<br>\n",
    "    I am studying history at UBC because I want to work as a <b>__</b>.     \n",
    "</blockquote>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNNs motivation \n",
    "\n",
    "- RNNs can help us with this limited memory problem!\n",
    "- **RNNs are a kind of neural network model which use hidden units to retain information over time.**  \n",
    "- Unlike Markov models, this approach does not impose a fixed-length limit on this prior context; the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence.\n",
    "- Condition the neural network on all previous time steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can a feedforward network handle sequences effectively?\n",
    "\n",
    "![](img/feedforwardNN.png)\n",
    "\n",
    "<!-- <img src=\"img/feedforwardNN.png\" height=\"400\" width=\"400\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)\n",
    "\n",
    "They are not inherently designed to handle sequences because they lack ability to capture temporal dependencies. But it is possible to incorporate some context. For example, by create n-gram features with `CountVectorizer`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN intuition: Example\n",
    "\n",
    "- How can a temporal dimension be added to a feedforward neural network?\n",
    "- For word representation with a vector of size 4, a single feedforward neural network can be used for prediction.\n",
    "- For 2 words, two separate feedforward neural networks can be used together.\n",
    "\n",
    "![](img/RNN-intro.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN-intro.png\" height=\"400\" width=\"400\">  -->\n",
    "\n",
    "(Credit: [Stanford CS224d slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to connect multiple feedforward networks? \n",
    "- **Make connections between hidden layers**.\n",
    "- The network typically consists of input, hidden layer, and output. The hidden layer is connected to itself for recurrent connections.\n",
    "- Sequences can be processed by presenting one element at a time to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN presentations\n",
    "\n",
    "- Unrolled presentation \n",
    "\n",
    "![](img/RNN-intro.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN-intro.png\" height=\"400\" width=\"400\">  -->\n",
    "\n",
    "\n",
    "- Recursive presentation\n",
    "\n",
    "![](img/RNN_recursive_2.png)\n",
    "<!-- <img src=\"img/RNN_recursive_2.png\" height=\"200\" width=\"200\">  -->\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_recursive_2.png\" height=\"300\" width=\"300\">  -->\n",
    "<!-- </center>      -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The key distinction between non-recurrent and recurrent architectures is **the inclusion of a new set of weights connecting the previous hidden layer to the current hidden layer**.\n",
    "- The hidden layer from the previous time step acts as a form of \"memory\" that influences decisions made at later time steps.\n",
    "- These weights determine how the network incorporates the previous context when computing output for the current input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN as a graphical model\n",
    "\n",
    "- RNNs can be visualized as a graphical model. The states below are the hidden layers in each time step.  \n",
    "    - Somewhat similar to hidden Markov models (HMMs) \n",
    "    - But a hidden state in an RNN is continuous valued, high dimensional, and much richer. \n",
    "- Each state is a function of the previous state and the input.\n",
    "- A state contains information about the whole past sequence. \n",
    "    - $h_t = g(x_t, x_{t-1}, \\dots, x_2, x_1)$ \n",
    "\n",
    "![](img/RNN-dynamic-model.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN-dynamic-model.png\" height=\"400\" width=\"400\">  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN as a feedforward neural network\n",
    "- Adding a temporal dimension and the recursion make RNNs appear to be complex. But they are not all that different from standard feedforrward neural networks. \n",
    "- Given an input vector and the values for the hidden layer from the previous time step we are still performing standard feedforward calculations. \n",
    "- The most significant change lies in the new set of weights $U$ that connect the hidden layer from the previous time step to the current hidden layer. \n",
    "- As with the other weights in the network, these connections are trained via a variant of backpropagation.\n",
    "\n",
    "![](img/RNN-as-FFNN.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN-as-FFNN.png\" height=\"500\" width=\"500\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter sharing\n",
    "\n",
    "- What are the parameters of this model? There are three weight matrices. \n",
    "    - Input to hidden weight matrix: $W$\n",
    "    - Hidden to output weight matrix: $V$    \n",
    "    - Hidden to hidden weight matrix: $U$\n",
    "    \n",
    "- The key point in RNNs: **All weights between time steps are shared.**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionality of different weight matrices\n",
    "Lets consider an example: \n",
    "- Suppose input vector $x_t$ is of size 300 (i.e., $x_t \\in \\mathbb{R}^{300}$)\n",
    "- Suppose the hidden state vector is of size 100 (memory of the network) (i.e., $h_t \\in \\mathbb{R}^{100}$)\n",
    "- Suppose the output vector $y_t$ is of size 60 (i.e., $y_t \\in \\mathbb{R}^{60}$)\n",
    "- $W_{100 \\times 300}$, $V_{60\\times 100}$, $U_{100\\times 100}$ \n",
    "\n",
    "![](img/RNN-as-FFNN.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN-as-FFNN.png\" height=\"500\" width=\"500\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)\n",
    "\n",
    "- Input size: Suppose $x \\in \\mathbb{R}^{d_{in}}$\n",
    "- Output size: Suppose $y \\in \\mathbb{R}^{d_{out}}$\n",
    "- Hidden size: Suppose $h \\in \\mathbb{R}^{d_h}$\n",
    "- Three kinds of weights: $W_{d_{h}\\times d_{in}}$, $V_{d_{out}\\times d_{h}}$, $U_{d_h\\times d_h}$    \n",
    "> You may see transpose of these matrices in some notations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass in RNNs\n",
    "- The forward inference in RNNs is very similar to what you have seen with feedforward networks. \n",
    "- Given an input $x_t$ at timestep $t$, how do we compute the new hidden state $h_{t}$ and output $y_t$? \n",
    "\n",
    "![](img/RNN-dynamic-model.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN-dynamic-model.png\" height=\"400\" width=\"400\">  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the new state $h_t$\n",
    "\n",
    "- Multiply the input $x_t$ with the weight matrix between input and hidden layer ($W$) and the state or the hidden layer from the previous time step $h_{t-1}$ with the weight matrix between hidden layers ($U$). \n",
    "- Add these values together. \n",
    "- Add the bias vector and pass the result through a suitable activation function $g$. \n",
    "\n",
    "$$\n",
    "h_t = g(U_{d_h \\times d_h}(h_{t-1})_{d_h \\times 1} + W_{d_{h} \\times d_{in}} (x_t)_{d_{in} \\times 1}  + b_1)\\\\\n",
    "$$ \n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_dynamic_model.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the output $y_t$\n",
    "\n",
    "- Once we have the value for the new state $h_t$, we can calculate the output vector $y_t$ by multiplying $h_t$ with the weight matrix $V$ between the hidden layer and the output layer, adding the bias vector, and applying an appropriate activation function $f$ the multiplication.  \n",
    "\n",
    "$$\n",
    "y_t = f(V_{d_{out} \\times d_{h}} (h_t){_{d_h \\times 1}} + b_2)\n",
    "$$ \n",
    "\n",
    "![](img/RNN-dynamic-model.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN-dynamic-model.png\" height=\"400\" width=\"400\">  -->\n",
    "\n",
    "- Typically, we are interested in soft classification. So computing $y_t$ involves a softmax computation which provides a probability distribution over the possible output classes. \n",
    "\n",
    "$$\n",
    "y_t = \\text{softmax}(Vh_t + b_2)\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary \n",
    "\n",
    "So in the forward pass we compute the new state $h_t$ and the output $y_t$ for all time steps in a sequence, as shown below.  \n",
    "\n",
    "$$\n",
    "h_t = g(Uh_{t-1} + Wx_t + b_1)\\\\\n",
    "y_t = \\text{softmax}(Vh_t + b_2)\n",
    "$$ \n",
    "\n",
    "![](img/RNN-dynamic-model.png)\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN-dynamic-model.png\" height=\"400\" width=\"400\">  -->\n",
    "<!-- </center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward pass pseudo code\n",
    "\n",
    "We compute this for the full sequence. \n",
    "\n",
    "- Given: $x$, network\n",
    "- $h_0 = 0$\n",
    "- for $t$ in 1 to length(input sequence $x$)\n",
    "    - $h_t = g(Uh_{t-1} + Wx_t + b_1$)\n",
    "    - $y_t = \\text{softmax}(Vh_t + b_2)$\n",
    "\n",
    "Note that the matrices $U$, $V$ and $W$ are **shared across time** and new values for $h_t$ and $y_t$ are calculated at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Forward pass with PyTorch\n",
    "\n",
    "- See the documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an RNN object \n",
    "\n",
    "We are creating an RNN with \n",
    "- only one hidden layer \n",
    "- input of size 20 (e.g., imagine word vectors of size 20)\n",
    "- hidden layer of size 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(20, 10, 1)  # input size, hidden_size, number of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input\n",
    "\n",
    "- The input is going to be sequences (e.g., sequences of words)\n",
    "- We need to provide the sequence length of the sequence and the size of each input vector. \n",
    "- For example, suppose you have the following sequence and you are representing each word with a 20-dimensional word vector, then your sequence length is going to be 5 and input size is going to be 20.  \n",
    "\n",
    "> Cherry blossoms are beautiful ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5098, -0.1926, -0.4484, -1.4965,  2.1181, -0.1226, -0.3528, -1.5821,\n",
       "          0.3901,  0.1464,  0.1158, -1.2263, -0.9533, -0.0492, -0.9606,  0.0234,\n",
       "         -0.3946, -1.2163,  0.3433,  0.3573],\n",
       "        [ 0.2797,  0.7981,  1.0023,  0.6701,  0.6317,  0.4525, -1.5256,  0.7832,\n",
       "         -0.7206, -0.1639, -0.3042,  0.8411, -0.8911, -1.6665, -1.1807, -0.4542,\n",
       "          1.0172,  1.8657,  0.4950, -1.1755],\n",
       "        [ 0.5938,  0.7645,  0.1083,  0.2653,  1.3708, -0.6390, -1.8478, -0.2760,\n",
       "          0.5242, -0.5039,  0.0874, -1.1764, -1.0408,  0.5149, -1.0565,  0.8938,\n",
       "          0.1098, -1.0632,  0.3263,  0.5102],\n",
       "        [ 0.6355, -1.3617, -0.3484,  1.9830,  0.6681,  0.0059,  0.3680,  0.1266,\n",
       "          0.3448, -0.9010,  0.2430,  1.5386,  0.0574,  0.7943,  1.5282,  0.4070,\n",
       "          0.7298, -0.6993,  0.1714,  0.3751],\n",
       "        [-0.4997,  1.0177, -0.3549,  0.1806,  0.4644,  0.5211, -1.3615,  0.0337,\n",
       "         -0.7859, -0.1764,  0.3620, -0.4455, -0.5399,  1.3742,  0.2995,  1.7460,\n",
       "          0.7271,  0.4901,  0.4149,  0.4269]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(5, 20)  # sequence length, input size\n",
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the 0th time step, we do not have anything to remember. So we initialize the hidden state randomly. \n",
    "- Let's initialize h0. \n",
    "- The shape of h0 is the number of hidden layers and hidden size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.randn(1, 10)  # number of hidden layers, hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7295, -0.1498,  0.8130, -1.0384,  0.8369, -0.3162,  0.5140, -1.1429,\n",
       "          0.9118, -0.0173]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating new hidden states and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch calculates the output and new hidden states for us for all time steps.\n",
    "output, hn = rnn(inp, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7839,  0.4937,  0.1763, -0.2922,  0.3116,  0.4545, -0.3453, -0.2193,\n",
       "         -0.8915, -0.2134]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn  # hidden state for the last time step in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6763, -0.3148,  0.7597,  0.9100, -0.6214,  0.8229, -0.0993,  0.5824,\n",
       "         -0.5741,  0.4514],\n",
       "        [-0.3105,  0.8508,  0.2145,  0.5286, -0.6977, -0.8278,  0.0504, -0.5231,\n",
       "          0.9345,  0.6215],\n",
       "        [ 0.2092,  0.4821,  0.4495,  0.7968, -0.3406,  0.3009, -0.6349, -0.0692,\n",
       "         -0.5152, -0.0124],\n",
       "        [-0.2362, -0.4294,  0.2354,  0.2154,  0.6586, -0.5720,  0.0145, -0.2873,\n",
       "         -0.5269, -0.4186],\n",
       "        [-0.7839,  0.4937,  0.1763, -0.2922,  0.3116,  0.4545, -0.3453, -0.2193,\n",
       "         -0.8915, -0.2134]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `tanh` activation function is used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapes of the weight matrices \n",
    "\n",
    "What would be the shapes of weight matrices? \n",
    "- Input to hidden ($W$)\n",
    "- Hidden to hidden ($U$)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight matrix $W$ between input to hidden layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 20])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.state_dict()[\"weight_ih_l0\"].shape # (hidden, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight matrix $U$ between hidden layer in time step $t-1$ to hidden layer in time step $t$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.state_dict()[\"weight_hh_l0\"].shape # (hidden, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `rnn` above is calculating the output of the hidden layer at each time step but we are not calculating $y_t$ in each time step $t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 6.1: Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) RNNs pass along information between time steps through hidden layers.\n",
    "- (B) RNNs are appropriate only for text data.\n",
    "- (C) At each time step in an RNN, we use a unique hidden state (`h`), a unique input (`X`), but we reuse the same `U` matrix of weights.\n",
    "- (D) The number of parameters in an RNN language model would grow with the number of time steps.\n",
    "- (E) The hidden state at the current time step in an RNN depends only on the input data at the current time step and the hidden state from the previous time step.\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 6.1: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "- (A) True\n",
    "- (B) False\n",
    "- (C) True\n",
    "- (D) False\n",
    "- (E) True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN is a **supervised machine learning model**. Similar to feedforward networks, we'll use a \n",
    "    - training set\n",
    "    - a loss function  \n",
    "    - backpropagation to obtain the gradients needed to adjust the weights in these networks \n",
    "\n",
    "- We have 3 sets of weights (and the corresponding bias terms) to update\n",
    "    - $W \\rightarrow $ the weight matrix between input layer and hidden layer\n",
    "    - $U \\rightarrow $ the weight matrix between previous hidden layer to current hidden layer\n",
    "    - $V \\rightarrow $ the weight matrix between hidden layer and output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to assess the error occurring at time step $t$.\n",
    "\n",
    "- To compute the loss function for the output at time $t$ we need the hidden layer from time $t-1$.\n",
    "- The hidden layer at time $t$ influences both the output at time $t$ and the hidden layer at time $t+1$. \n",
    "\n",
    "![](img/RNN_loss.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN_loss.png\" height=\"1500\" width=\"1500\">  -->\n",
    "\n",
    "[Credit](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To assess the error occurring to $h_t$, we need to know its influence on both the current output and the ones that follow.  \n",
    "- This is different than the usual backpropagation. We need to tailor backpropogation algorithm to this situation. In RNNs we use a generalized version of **Backpropogation called Backpropogation Through Time (BPTT)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "- The overall loss is the summation of losses at each time step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN code in 112 lines of Python\n",
    "\n",
    "- See [the code](https://gist.github.com/karpathy/d4dee566867f8291f086) for the above in ~112 lines of Python written by Andrej Karpathy. The code has only `numpy` dependency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN applications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### What can we do with RNNs?\n",
    "\n",
    "- We have seen the basic RNN architecture below. \n",
    "\n",
    "![](img/RNN_introduction.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN-intro.png\" height=\"500\" width=\"500\">  -->\n",
    "\n",
    "- But a number of architectures are possible, which makes them a very rich family of models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN architectures\n",
    "\n",
    "- A number of possible RNN architectures\n",
    "\n",
    "![](img/RNN_architectures.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/RNN_architectures.png\" height=\"1500\" width=\"1500\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "[source](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how can we apply it to three different types of NLP tasks:\n",
    "- Sequence labeling (e.g., POS tagging)\n",
    "- Sequence classification (e.g. sentiment analysis or text classification)\n",
    "- Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sequence labeling \n",
    "\n",
    "- The task is to assign a label from a fixed set of labels to each element in the sequence.  \n",
    "    - Part-of-speech tagging \n",
    "    - Named entity recognition\n",
    "- Many-to-many architecture\n",
    "- Inputs are usually pre-trained word embeddings and outputs are tag probabilities generated by a softmax layer over the given tagset. \n",
    "- The RNN block is an abstraction representing an unrolled simple RNN consisting of an input layer, hidden layer and output layer at each time step and shared weight matrices $U$, $W$, and $V$. \n",
    "\n",
    "![](img/RNN_seq_labeling.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN_seq_labeling.png\" height=\"600\" width=\"600\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequence classification\n",
    "\n",
    "- We have done text classification such as sentiment analysis or spam identification before with traditional ML models, where we ignored the temporal nature of language.  \n",
    "- These are actually sequence classification tasks where we want to map a sequence of text to a label from a small set of labels (e.g., positive, negative, neutral). \n",
    "- To apply RNNs in this setting, we take the text to be classified and pass one word at a time generating a new hidden layer at each time step. We can then take the hidden layer from the last time step, $h_n$, which has the compressed representation of the entire sequence. We pass this representation through a feedforward neural network which chooses a class via a softmax.     \n",
    "- This is a many-to-one RNN architecture. \n",
    "\n",
    "![](img/RNN_classification.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN_classification.png\" height=\"600\" width=\"600\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similar to the sequence labeling example, we can also pass word embeddings as input. \n",
    "- Note that in this approach we do not have immediate outputs at each time step and we do not need to compute $y$ at each time step. We only have an output at the last time step. \n",
    "- So there won't be loss terms associated with each time step. \n",
    "- The loss function used to train the weights in the network is entirely based on the final text classification task. \n",
    "- We will compare the output of the softmax layer of the feed-forward classifier and the actual $y$ to calculate the loss (e.g., cross-entropy loss) and this loss will drive the training. \n",
    "- The error signal is backpropagated all the way through the weights in the feed-forward classifier, through its input, which is the hidden layer output of the last time step, through the three sets of RNN weights: $U$, $V$, and $W$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text generation\n",
    "\n",
    "- The idea is similar to text generation with Markov models. \n",
    "- We start with a seed. We then continue to sample words conditioned on our previous choices until we reach a pre-determined desired length of a sequence or end-of-sequence token is generated.\n",
    "- In the context of RNNs\n",
    "    - We start with a seed. In the example below, we are starting with a special beginning of sequence token \\<s\\>. \n",
    "    - We use embedding representation of this token and pass it to the RNN. \n",
    "    - We sample a word in the output from the softmax distribution.  \n",
    "    - We use this sampled word as the input in the next time step and then sample the next word in the same fashion. \n",
    "    - We continue this until the fixed length limit or the end of the sentence marker is reached. \n",
    "\n",
    "![](img/RNN_generation.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN_generation.png\" height=\"600\" width=\"600\">  -->\n",
    "    \n",
    "- The same idea can be used for music generation. \n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can find a toy example of of RNN text generation with PyTorch in [AppendixC](AppendixC-toy-RNN.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image captioning \n",
    "\n",
    "- The same idea can be used for more complicated applications such as machine translation, summarization, or image captioning. \n",
    "- The idea is to prime the generation component with an appropriate context. \n",
    "- For example, in image captioning we can prime the generation component with a meaningful  representation of an image given by the last layer in CNNs.  \n",
    "- We'll talk more about this application next week. \n",
    "\n",
    "![](img/image_captioning.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/image_captioning.png\" width=\"1000\" height=\"1000\"> -->\n",
    "<!-- </center> -->\n",
    "    \n",
    "[Source](https://cs.stanford.edu/people/karpathy/sfmltalk.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked and Bidirectional RNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have seen a simple RNN with one hidden layer. \n",
    "- But RNNs are quite flexible. \n",
    "- Two common ways to create complex networks by combining RNNs are:\n",
    "    - Stacked RNNs\n",
    "    - Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked RNNs \n",
    "\n",
    "- In the examples thus far, the input of RNNs was a sequence of word or character embeddings. We were passing the output of the RNN layer to the output layer and the outputs have been vectors useful for predicting next words, tags, or sequence labels.  \n",
    "\n",
    "![](img/RNN_seq_labeling.png)\n",
    "<!-- <img src=\"img/RNN_seq_labeling.png\" height=\"600\" width=\"600\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But nothing prevents us from using **the sequence of outputs from one RNN as an input sequence to another one**.\n",
    "- These are called **stacked RNNs** which consist of multiple networks where the output of one layer serves as the input to a subsequent layer. \n",
    "\n",
    "![](img/RNN_stacked.png)\n",
    "\n",
    "\n",
    "<!-- <img src=\"img/RNN_stacked.png\" height=\"600\" width=\"600\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stacked RNNs generally outperform single-layer networks. \n",
    "- The network learns a different level of abstraction at each layer. \n",
    "- You can optimize your network for number of layers for your specific application and dataset.  \n",
    "- But remember that more layers means higher training cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs \n",
    "\n",
    "- The RNN uses information from the prior context to make predictions at time $t$. \n",
    "- But in many applications (e.g., POS tagging) we do have access to the entire input sequence and knowing the context on the right of time $t$ can be useful. \n",
    "- For example, suppose you are doing POS tagging and you are at the token **Teddy** in the sequence. It will be useful to know the right context in order to make the decision on whether it should be tagged as a _noun_ or a _proper noun_.  \n",
    "\n",
    "> He said , \" Teddy Roosevelt was a great president ! \"<br>\n",
    "\n",
    "> He said , \" Teddy bears are on sale ! \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can we use the words on the right of time step $t$ as context?  \n",
    "- In the left-to-right RNN, the hidden state at time $t$ represents everything the network knows about the sequence up to that point. \n",
    "- Suppose $h_t^f$ denotes a hidden state at time $t$ representing everything the network has gleaned from the sequence so far. \n",
    "$$h_t^f = RNN_{forward}(x_1, x_2, \\dots, x_t) $$\n",
    "- We can also train the network in the reverse direction, from right to left, to take advantage of the right context. \n",
    "- With this approach the hidden state at time $t$, $h_t^b$ represents all the information we have learned about the sequence from time $t$ to the end of the sequence. \n",
    "$$h_t^b = RNN_{backward}(x_t, x_{t+1}, \\dots, x_n) $$\n",
    "- (Somewhat similar to the $\\alpha$ and $\\beta$ values in the forward and backward algorithms in HMMs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **bidirectional RNN** combines two independent RNNs:\n",
    "- One where the input is processed from the start to the end\n",
    "- The other from the end to the start. \n",
    "- Each RNN will result in some representation of the input. \n",
    "- We then combine the two representations computed by two independent RNNs into a single vector which captures both the left and right contexts of an input at each point in time. \n",
    "- We can combine vectors by\n",
    "    - Concatenating them, as shown in the picture below or\n",
    "    - Element-wise addition \n",
    "    - Element-wise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/bidirectional_seq_labeling.png)\n",
    "<!-- <img src=\"img/bidirectional_seq_labeling.png\" height=\"600\" width=\"600\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also use bidirectional RNNs for sequence classification. \n",
    "- Recall that in sequence classification we pass the final hidden state of the RNN as input to a subsequent feedforward classifier. \n",
    "- The problem with this approach is that the final hidden state reflects more information about the end of the sequence than its beginning. \n",
    "- Bidirectional RNNs provide a simple solution to this problem. We can create a final hidden state by combining hidden states of forward and backward passes so that the hidden state reflects information about both the beginning and end of the sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/bidirectional_classification.png)\n",
    "<!-- <img src=\"img/bidirectional_classification.png\" height=\"600\" width=\"600\">  -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final comments and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important ideas to know \n",
    "\n",
    "- RNNs are supervised neural network models to process sequential data.\n",
    "- The intuition is to put multiple feed-forward networks together and making connections between hidden layers.  \n",
    "- They have feedback connections in their structure to \"remember\" previous inputs, when reading in a sequence. \n",
    "- In simple RNNs sequences are processed one element at a time. The output of each neural unit at time $t$ is based on the current input at $t$ and the hidden layer at time $t-1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In RNNs, the parameters are shared across different time steps.\n",
    "- A generalized version of backpropagation called backpropagation through time is used for training the network. \n",
    "- In practice truncated backpropagation through time is used where we work through chunks. \n",
    "- A number of RNNs architectures are possible. \n",
    "- RNNs fail to capture long-distance dependencies because of the problems like vanishing gradients.  \n",
    "- In practice, some other complicated variants such as LSTMs and GRUs are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coming up ...\n",
    "- Intuition of transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resources\n",
    "\n",
    "- [Sequence processing with Recurrent Neural Networks](https://web.stanford.edu/~jurafsky/slp3/9.pdf) (The notes above are heavily based on this resource.)\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [Coursera: NLP sequence models](https://www.coursera.org/lecture/nlp-sequence-models/recurrent-neural-network-model-ftkzt)\n",
    "- [RNN code in 112 lines of Python](https://gist.github.com/karpathy/d4dee566867f8291f086#file-min-char-rnn-py-L112)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
