{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DSCI 575: Advanced Machine Learning (in the context of Natural Language Processing (NLP) applications)\n",
    "\n",
    "UBC Master of Data Science program, 2019-20\n",
    "\n",
    "Instructor: Varada Kolhatkar [ʋəɾəda kɔːlɦəʈkər]\n",
    "\n",
    "### Lecture plan\n",
    "- Course information\n",
    "- What is NLP?\n",
    "- Today's lecture: Word embeddings: \n",
    "    - Meaning representation\n",
    "    - Word2Vec Skip-gram\n",
    "    - Pre-trained embeddings\n",
    "    - Summary and preview for the next lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Slide settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# And import the libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import IPython\n",
    "import altair as alt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThemeRegistry.enable('theme_fm')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thanks to Firas for the following code for making jupyter RISE slides pretty! \n",
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "from pathlib import Path\n",
    "path = Path.home() / \".jupyter\" / \"nbconfig\"\n",
    "cm = BaseJSONConfigManager(config_dir=str(path))\n",
    "tmp = cm.update(\n",
    "        \"rise\",\n",
    "        {\n",
    "            \"theme\": \"serif\",\n",
    "            \"transition\": \"fade\",\n",
    "            \"start_slideshow_at\": \"selected\",            \n",
    "            \"width\": \"100%\",\n",
    "            \"height\": \"100%\",\n",
    "            \"header\": \"\",\n",
    "            \"footer\":\"\",\n",
    "            \"scroll\": True,\n",
    "            \"enable_chalkboard\": True,\n",
    "            \"slideNumber\": True,\n",
    "            \"center\": False,\n",
    "            \"controlsLayout\": \"edges\",\n",
    "            \"slideNumber\": True,\n",
    "            \"hash\": True,\n",
    "        }\n",
    "    )\n",
    "\n",
    "## Set Altair default size\n",
    "\n",
    "def theme_fm(*args, **kwargs):\n",
    "    return {'height': 300,\n",
    "            'config': {'style': {'circle': {'size': 400},\n",
    "                                'point': {'size': 30},\n",
    "                                'square': {'size': 400},\n",
    "                                },\n",
    "                       'legend': {'symbolSize': 20, 'titleFontSize': 20, 'labelFontSize': 20}, \n",
    "                       'axis': {'titleFontSize': 20, 'labelFontSize': 20}},\n",
    "            }\n",
    "\n",
    "alt.themes.register('theme_fm', theme_fm)\n",
    "alt.themes.enable('theme_fm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "     font-size: 130%;\n",
       "}\n",
       "\n",
       "body.rise-enabled div.inner_cell>div.input_area {\n",
       "    font-size: 100%;\n",
       "}\n",
       "\n",
       "body.rise-enabled div.output_subarea.output_text.output_result {\n",
       "    font-size: 100%;\n",
       "}\n",
       "body.rise-enabled div.output_subarea.output_text.output_stream.output_stdout {\n",
       "  font-size: 150%;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
    "     font-size: 130%;\n",
    "}\n",
    "\n",
    "body.rise-enabled div.inner_cell>div.input_area {\n",
    "    font-size: 100%;\n",
    "}\n",
    "\n",
    "body.rise-enabled div.output_subarea.output_text.output_result {\n",
    "    font-size: 100%;\n",
    "}\n",
    "body.rise-enabled div.output_subarea.output_text.output_stream.output_stdout {\n",
    "  font-size: 150%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "plt.rcParams['font.size'] = 16\n",
    "sys.path.append('code/.')\n",
    "from preprocessing import MyPreprocessor\n",
    "from comat import CooccurrenceMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Course information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### High-level goals of this course\n",
    "\n",
    "- Apply machine learning algorithms you have learned so far in interesting applications. \n",
    "- Learn new ML algorithms and methods with the theme of NLP applications.\n",
    "- Prepare you a bit for employment in the NLP area.\n",
    "- Have fun! \n",
    "\n",
    "\n",
    "<img src=\"images/NLP_in_industry.png\" width=\"900\" height=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topics we'll be covering in this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 1 \n",
    "\n",
    "- Representation Learning\n",
    "- Word vectors and word embeddings\n",
    "\n",
    "<img src=\"images/tsne_example.png\" height=\"1000\" width=\"1000\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 2\n",
    "\n",
    "- Markov models\n",
    "- Hidden Markov models\n",
    "\n",
    "<img src=\"images/Markov_autocompletion.png\" height=\"800\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Markov models application\n",
    "\n",
    "<img src=\"images/Markov_chain_applications.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 3\n",
    "\n",
    "- Topic modeling (Latent Dirichlet Allocation (LDA))\n",
    "    - Suppose given a large collection of documents, you are asked to \n",
    "        - Infer different topics in the documents\n",
    "        - Pull all documents about a certain topic    \n",
    "- Introduction to Recurrent Neural Networks (RNNs)\n",
    "<img src=\"images/TM_food_magazines.png\" height=\"1000\" width=\"1000\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 4 \n",
    "\n",
    "- LSTMs, GRUs \n",
    "- RNN applications: Image captioning \n",
    "\n",
    "<blockquote>\n",
    "\n",
    "<img src=\"images/image_captioning.png\" width=\"1000\" height=\"1000\">\n",
    "\n",
    "<p style=\"font-size:30px\"></p>\n",
    "</blockquote>    \n",
    "Source: https://cs.stanford.edu/people/karpathy/sfmltalk.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ASIDE: [Neural Storyteller](https://github.com/ryankiros/neural-storyteller)\n",
    "\n",
    "<img src=\"images/RNN_example.jpg\" width=\"600\" height=\"600\">\n",
    "\n",
    "<blockquote> \n",
    "<p style=\"font-size:30px\">We were barely able to catch the breeze at the beach , and it felt as if someone stepped out of my mind . She was in love with him for the first time in months , so she had no intention of escaping . The sun had risen from the ocean , making her feel more alive than normal . She 's beautiful , but the truth is that I do n't know what to do ...</p>\n",
    "</blockquote>    \n",
    "\n",
    "Source: https://github.com/ryankiros/neural-storyteller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### That's all about course information. In the next video we will talk about what Natural Language Processing (NLP) is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# DSCI 575 Lecture 1: Word Embeddings\n",
    "\n",
    "UBC Master of Data Science program, 2019-20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Today's promise\n",
    "\n",
    "- We will learn a state-of-the art method for word \"meaning\" representation.  \n",
    "\n",
    "#### Specific learning outcomes\n",
    "\n",
    "From this class, you will be able to \n",
    "\n",
    "- Explain what natural language processing is.\n",
    "- Explain the general idea of vector space model.\n",
    "- Explain the skip-gram model at a high level.\n",
    "- Explain the difference between sparse and dense word representations.\n",
    "- Train your own word vectors with `Gensim`. \n",
    "- Use word2vec models for word similarity and analogies. \n",
    "- Load pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Natural Language Processing (NLP)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What should a search engine return when asked the following question? \n",
    "\n",
    "\n",
    "<img src=\"images/lexical_ambiguity.png\" width=\"1000\" height=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Natural Language Processing (NLP)?\n",
    "#### How often do you search everyday? \n",
    "\n",
    "<img src=\"files/images/Google_search.png\" width=\"900\" height=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Natural Language Processing (NLP)?\n",
    "\n",
    "<img src=\"images/WhatisNLP.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "- Language is complex and subtle. \n",
    "- Language is ambiguous at different levels. \n",
    "- Language understanding involves common-sense knowledge and real-world reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Lexical ambiguity\n",
    "\n",
    "<img src=\"files/images/lexical_ambiguity.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Referential ambiguity\n",
    "\n",
    "<img src=\"files/images/referential_ambiguity.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Ambiguous news headlines](http://www.fun-with-words.com/ambiguous_headlines.html)\n",
    "\n",
    "<blockquote>\n",
    "PROSTITUTES APPEAL TO POPE\n",
    "</blockquote>    \n",
    "\n",
    "- **appeal to** means make a serious or urgent request or be attractive or interesting?\n",
    "\n",
    "<blockquote>\n",
    "KICKING BABY CONSIDERED TO BE HEALTHY    \n",
    "</blockquote> \n",
    "\n",
    "- **kicking** is used as an adjective or a verb?\n",
    "\n",
    "<blockquote>\n",
    "MILK DRINKERS ARE TURNING TO POWDER\n",
    "</blockquote>\n",
    "\n",
    "- **turning** means becoming or take up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "- All the problems related to representation and reasoning in artificial intelligence arise in this domain. \n",
    "- For language understanding, we need a representation that captures its \"meaning\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word meaning \n",
    "\n",
    "- A favourite topic of philosophers for centuries. \n",
    "- An example from legal domain: [Are hockey gloves gloves or \"articles of plastics\"?](https://www.scc-csc.ca/case-dossier/info/sum-som-eng.aspx?cas=36258)\n",
    "\n",
    "<blockquote>\n",
    "Canada (A.G.) v. Igloo Vikski Inc. was a tariff code case that made its way to the SCC (Supreme Court of Canada). The case disputed the definition of hockey gloves as either gloves or as \"articles of plastics.\"\n",
    "</blockquote>\n",
    "\n",
    "<img src=\"images/hockey_gloves_case.png\" width=\"900\" height=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word meaning: NLP view\n",
    "- Modeling word meaning that allows us to \n",
    "    * draw useful inferences to solve meaning-related problems \n",
    "    * find relationship between words, \n",
    "        * E.g., which words are similar, which ones have positive or negative connotations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reminder: One-hot representation\n",
    "\n",
    "- Build the **vocabulary** containing all unique words from the corpus. \n",
    "- Represent each word as **one-hot** encoding.\n",
    "- A vector of length $V$ such that the value at word index is 1 and all other indices is 0.\n",
    "- Example: \n",
    "    * Vocabulary size = 10\n",
    "    * Index of the word *pineapple* = 4\n",
    "    * One-hot vector for *pineapple*:\n",
    "    \\begin{bmatrix} 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activity 1:  Brainstorm ways to represent words (~4 mins) \n",
    "\n",
    "- Suppose you are building a Question Answering system and you are given the following question and three candidate answers. \n",
    "- Think about the following questions.  \n",
    "    - What kind of relationship between words do we need to capture in order to arrive at the correct answer?  \n",
    "    - Would one-hot representation help in this context?\n",
    "    \n",
    "<blockquote>       \n",
    "<p style=\"font-size:30px\"><b>Question:</b> How <b>tall</b> is Machu Picchu?</p>\n",
    "    <p style=\"font-size:30px\"><b>Candidate 1:</b> Machu Picchu is 13.164 degrees south of the equator.</p>    \n",
    "<p style=\"font-size:30px\"><b>Candidate 2:</b> The official height of Machu Picchu is 2,430 m.</p>\n",
    "<p style=\"font-size:30px\"><b>Candidate 3:</b> Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.</p>    \n",
    "</blockquote> \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_onehot_encoding(word, vocab):\n",
    "    onehot = np.zeros(len(vocab), dtype='float64')    \n",
    "    onehot[vocab[word]] = 1\n",
    "    print('one-hot encoding of the word \"%s\" is: %s' % (word, str(onehot)))\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocabulary and one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 12\n",
      "{'.': 0, 'add': 1, 'freshly': 2, 'in': 3, 'it': 4, 'juice': 5, 'make': 6, 'pineapple': 7, 'smoothie': 8, 'special': 9, 'squeezed': 10, 'your': 11}\n",
      "one-hot encoding of the word \"pineapple\" is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "one-hot encoding of the word \"juice\" is: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "The dot product between pineapple and juice is 0\n"
     ]
    }
   ],
   "source": [
    "# Note: In the NLP community a text data set is referred \n",
    "# to as a **corpus** (plural: corpora).\n",
    "corpus = \"\"\"make your smoothie special .\n",
    "          add freshly squeezed pineapple juice in it .\n",
    "          \"\"\"\n",
    "unique_words = list(set(corpus.split()))\n",
    "unique_words.sort()\n",
    "vocab = {word: index for index, word in enumerate(unique_words)}\n",
    "print('Size of the vocabulary: %d' %(len(vocab)))\n",
    "print(vocab)\n",
    "\n",
    "word1 = 'pineapple'\n",
    "onehot_word1 = get_onehot_encoding(word1, vocab)\n",
    "\n",
    "word2 = 'juice'\n",
    "onehot_word2 = get_onehot_encoding(word2, vocab)\n",
    "\n",
    "print(\"The dot product between %s and %s is %d\" % \n",
    "      (word1, word2, onehot_word1.dot(onehot_word2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem with one-hot encoding\n",
    "\n",
    "-  The problem with this representation is that there is no inherent notion of relationship between words.\n",
    "\n",
    "<center>\n",
    "$\\vec{pineapple}\\cdot\\vec{juice} = 0$ \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Need a representation that captures relationships between words.\n",
    "\n",
    "- We will be looking at two such representations.  \n",
    "    1. Sparse representation with **term-term co-occurrence matrix**\n",
    "    2. Dense representation with **Word2Vec skip-gram model**\n",
    "- Both are based on two ideas: **distributional hypothesis** and **vector space model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distributional hypothesis\n",
    "\n",
    "<blockquote> \n",
    "    <p>You shall know a word by the company it keeps.</p>\n",
    "    <footer>Firth, 1957</footer>        \n",
    "</blockquote>\n",
    "\n",
    "<blockquote> \n",
    "If A and B have almost identical environments we say that they are synonyms.\n",
    "<footer>Harris, 1954</footer>    \n",
    "</blockquote>    \n",
    "\n",
    "Example: \n",
    "\n",
    "- Her **child** loves to play in the playground. \n",
    "- Her **kid** loves to play in the playground. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector space model\n",
    "\n",
    "- Model the meaning of a word by placing it into a vector space.  \n",
    "- A standard way to represent meaning in NLP\n",
    "- Distances among words in the vector space indicate the relationship between them. \n",
    "- Called an \"embedding\" because it's embedded into a high-dimensional space\n",
    "\n",
    "<img src=\"images/t-SNE_word_embeddings.png\" width=\"700\" height=\"700\">\n",
    "(Attribution: Jurafsky and Martin 3rd edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representation 1: Term-term co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term-term co-occurrence matrix\n",
    "\n",
    "- The idea is to go through a corpus of text, keeping a count of all of the words that appear in context of each word (within a window).\n",
    "\n",
    "- An example: \n",
    "<img src=\"images/term-term_comat.png\" width=\"600\" height=\"600\">\n",
    "(Credit: Jurafsky and Martin 3rd edition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing word vectors and similarity \n",
    "\n",
    "<img src=\"images/word_vectors_and_angles.png\" width=\"700\" height=\"700\">\n",
    "(Credit: Jurafsky and Martin 3rd edition)\n",
    "\n",
    "- The similarity is calculated using dot products between word vectors.\n",
    "    - Example: $\\vec{\\text{digital}}.\\vec{\\text{information}} = 0 \\times 1 + 1\\times 6 = 6$\n",
    "    - Higher the dot product more similar the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing word vectors and similarity\n",
    "\n",
    "<img src=\"images/word_vectors_and_angles.png\" width=\"600\" height=\"600\">\n",
    "(Credit: Jurafsky and Martin 3rd edition)\n",
    "\n",
    "- The similarity is calculated using dot products between word vectors.\n",
    "    - Example: $\\vec{\\text{digital}}.\\vec{\\text{information}} = 0 \\times 1 + 1\\times 6 = 6$\n",
    "    - Higher the dot product more similar the words.\n",
    "\n",
    "- We can also calculate a normalized version of dot products. \n",
    "    $$similarity_{cosine}(w_1,w_2) = \\frac{w_1.w_2}{\\left\\lVert w_1\\right\\rVert_2 \\left\\lVert w_2\\right\\rVert_2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tall</th>\n",
       "      <th>machu</th>\n",
       "      <th>picchu</th>\n",
       "      <th>13.164</th>\n",
       "      <th>degrees</th>\n",
       "      <th>south</th>\n",
       "      <th>equator</th>\n",
       "      <th>official</th>\n",
       "      <th>height</th>\n",
       "      <th>2,430</th>\n",
       "      <th>...</th>\n",
       "      <th>mean</th>\n",
       "      <th>sea</th>\n",
       "      <th>level</th>\n",
       "      <th>1,000</th>\n",
       "      <th>3,300</th>\n",
       "      <th>ft</th>\n",
       "      <th>lower</th>\n",
       "      <th>elevation</th>\n",
       "      <th>3,400</th>\n",
       "      <th>11,200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tall</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machu</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>picchu</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.164</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degrees</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tall  machu  picchu  13.164  degrees  south  equator  official  \\\n",
       "tall        0      1       1       0        0      0        0         0   \n",
       "machu       1      0       5       1        1      0        0         1   \n",
       "picchu      1      5       0       1        1      1        0         1   \n",
       "13.164      0      1       1       0        1      1        1         0   \n",
       "degrees     0      1       1       1        0      1        1         0   \n",
       "\n",
       "         height  2,430  ...  mean  sea  level  1,000  3,300  ft  lower  \\\n",
       "tall          0      0  ...     0    0      0      0      0   0      0   \n",
       "machu         1      2  ...     0    0      0      0      0   0      0   \n",
       "picchu        1      2  ...     0    0      0      0      0   0      0   \n",
       "13.164        0      0  ...     0    0      0      0      0   0      0   \n",
       "degrees       0      0  ...     0    0      0      0      0   0      0   \n",
       "\n",
       "         elevation  3,400  11,200  \n",
       "tall             0      0       0  \n",
       "machu            0      0       0  \n",
       "picchu           0      0       0  \n",
       "13.164           0      0       0  \n",
       "degrees          0      0       0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's build term-term co-occurrence matrix for our text. \n",
    "corpus = [\"How tall is Machu Picchu?\",\n",
    "          \"Machu Picchu is 13.164 degrees south of the equator.\", \n",
    "          \"The official height of Machu Picchu is 2,430 m.\",\n",
    "          \"Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.\",\n",
    "          \"It is 80 kilometres (50 miles) northwest of Cusco, on the crest of the mountain Machu Picchu, located about 2,430 metres (7,970 feet) above mean sea level, over 1,000 metres (3,300 ft) lower than Cusco, which has an elevation of 3,400 metres (11,200 ft).\"\n",
    "         ]\n",
    "pp = MyPreprocessor()\n",
    "pp_corpus = pp.preprocess_corpus(corpus)\n",
    "cm = CooccurrenceMatrix(pp_corpus)\n",
    "vocab, comat = cm.fit_transform()\n",
    "words = [key for key, value in sorted(vocab.items(), \n",
    "                                      key = lambda item: (item[1],item[0]))]\n",
    "df = pd.DataFrame(comat.todense(), columns = words, \n",
    "                  index = words, dtype = np.int8)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product between tall and height is 2.00 and cosine similarity is 0.71\n",
      "The dot product between tall and official is 2.00 and cosine similarity is 0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def similarity(word1, word2): \n",
    "    \"\"\"\n",
    "    Returns similarity score between word1 and word2\n",
    "    Arguments\n",
    "    ---------\n",
    "    word1 -- (str)\n",
    "        The first word\n",
    "    word2 -- (str)\n",
    "        The second word\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    None. Prints the similarity score between word1 and word2. \n",
    "    \"\"\"\n",
    "    vec1 = cm.get_word_vector(word1).todense().flatten()\n",
    "    vec2 = cm.get_word_vector(word2).todense().flatten()\n",
    "    v1 = np.squeeze(np.asarray(vec1))\n",
    "    v2 = np.squeeze(np.asarray(vec2))\n",
    "    print('The dot product between %s and %s is %0.2f and cosine similarity is %0.2f' \n",
    "          %(word1,word2,v1.dot(v2),cosine_similarity(vec1, vec2)))\n",
    "    \n",
    "similarity('tall', 'height')\n",
    "similarity('tall', 'official')\n",
    "### Not very reliable similarity scores because we used only 4 sentences.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Break (~5 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representation 2: Dense word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sparse vs. dense word vectors\n",
    "\n",
    "- Term-term co-occurrence matrices are long and sparse. \n",
    "    - length |V| is usually large (e.g., > 50,000) \n",
    "    - most elements are zero\n",
    "- OK because there are efficient ways to deal with sparse matrices.\n",
    "\n",
    "\n",
    "### Alternative \n",
    "- Learn short (~100 to 1000 dimensions) and dense vectors. \n",
    "- Short vectors may be easier to train with ML models (less weights to train).\n",
    "- They may generalize better.\n",
    "- In practice they work much better! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec \n",
    "\n",
    "- A family of algorithms to create dense word embeddings\n",
    "<img src=\"images/word2vec.png\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "# You can download them from here: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "# (Under the pre-trained embeddings section on this page: https://code.google.com/archive/p/word2vec/)\n",
    "# You'll need to install gensim (https://radimrehurek.com/gensim/)\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('/Users/kvarada/MDS/2019-20/575/data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  3000000\n",
      "The similarity between height and tall is 0.473\n",
      "The similarity between pineapple and mango is 0.668\n",
      "The similarity between pineapple and juice is 0.418\n",
      "The similarity between sun and robot is 0.029\n",
      "The similarity between GPU and lion is 0.002\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary: ', len(model.vocab))\n",
    "word_pairs = [('height','tall'),\n",
    "              ('pineapple','mango'), \n",
    "              ('pineapple','juice'), \n",
    "              ('sun','robot'), \n",
    "              ('GPU','lion')]\n",
    "for pair in word_pairs: \n",
    "    print('The similarity between %s and %s is %0.3f' %(pair[0], pair[1], model.similarity(pair[0], pair[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('UVic', 0.7886475324630737),\n",
       " ('SFU', 0.7588527202606201),\n",
       " ('Simon_Fraser', 0.7356574535369873),\n",
       " ('UFV', 0.688043475151062),\n",
       " ('VIU', 0.6778583526611328),\n",
       " ('Kwantlen', 0.677142858505249),\n",
       " ('UBCO', 0.6734488010406494),\n",
       " ('UPEI', 0.6731126308441162),\n",
       " ('UBC_Okanagan', 0.6709134578704834),\n",
       " ('Lakehead_University', 0.6622507572174072)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('UBC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Activity 2: Try out Word similarity with the code above (~4 mins)\n",
    "\n",
    "- Take a moment here and try out some words to find most similar words. To get you started here are some words: *Vancouver, bread, Computer_Science*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec \n",
    "\n",
    "- A family of models to obtain dense word vectors.\n",
    "\n",
    "- Two primary algorithms \n",
    "    - **Skip-gram**\n",
    "    - Continuous bag of words (CBOW)\n",
    "- Two moderately efficient training methods \n",
    "    - Hierarchical softmax\n",
    "    - Negative sampling \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-gram\n",
    "\n",
    "- A neural network model to obtain robust and dense representations of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fake word-prediction task \n",
    "\n",
    "- Given a target word (i.e., center word) word, predict context words (i.e., surrounding words). \n",
    "- Note that we are using \"target\" in a different sense here compared to how we use it in supervised machine learning.  \n",
    "<blockquote>\n",
    "    Add freshly squeezed$_{context}$ pineapple$_{target}$ juice$_{context}$ to your smoothie. \n",
    "</blockquote> \n",
    "\n",
    "<img src=\"images/target_context.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "- So in the example above given the target word **pineapple**, predict whether: \n",
    "    - **juice** is likely to occur in the context of **pineapple**\n",
    "    - **squeezed** is likely to occur in the context of **pineapple** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-gram objective\n",
    "- Consider the conditional probabilities $p(w_c|w_t)$ and set the parameters $\\theta$ of $p(w_c|w_t; \\theta)$ so as to maximize the corpus probability. \n",
    "\n",
    "$$\\arg \\max\\limits_\\theta \\prod\\limits_{(w_c,w_t) \\in D} p(w_c|w_t;\\theta)$$\n",
    "\n",
    "\n",
    "- $w_t$ &rarr; target word\n",
    "- $w_c$ &rarr; context word\n",
    "- $D$ &rarr; the set of all word and context pairs from the text. \n",
    "- $V$ &rarr; vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-gram objective\n",
    "\n",
    "- Model the conditional probability using softmax of the dot product.\n",
    "    * Higher the dot product higher the probability and vice-versa.     \n",
    "    \n",
    "\n",
    "$$P(w_c|w_t;\\theta) = \\frac{\\exp(w_c.w_t)}{\\sum\\limits_{\\substack{c' \\in V}} \\exp(w_{c'}.w_t)}$$\n",
    "\n",
    "- Substituting the conditional probability with the softmax of dot product: \n",
    "$$ \\arg \\max\\limits_\\theta \\prod\\limits_{(w_c,w_t) \\in D} P(w_c|w_t;\\theta) \\approx \\prod\\limits_{(w_c,w_t) \\in D}\\frac{\\exp(w_c.w_t)}{\\sum\\limits_{\\substack{c' \\in V}} \\exp(w_{c'}.w_t)}$$\n",
    "- Assumption: Maximizing this objective on a large corpus will result in meaningful embeddings for all words in the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we do it?\n",
    "\n",
    "- We use a neural network architecture with \n",
    "    - an input layer\n",
    "    - a hidden layer\n",
    "    - an output layer \n",
    "- We use the softmax activation function for the output layer. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example \n",
    "\n",
    "<img src=\"images/skipgram_0.png\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Input layer and \"gold\" \n",
    "\n",
    "<img src=\"images/skipgram_1.png\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden layer\n",
    "\n",
    "<img src=\"images/skipgram_2.png\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What will be the dimensions of the weight matrix between input and hidden layers?\n",
    "\n",
    "1. $10000 \\times 1$\n",
    "2. $10000 \\times 300$\n",
    "3. $300 \\times 300$\n",
    "\n",
    "\n",
    "<img src=\"images/skipgram_2.png\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden layer and output layer \n",
    "\n",
    "<img src=\"images/skipgram_3.png\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What will be the dimensions of the weight matrix between hidden and output layers?\n",
    "\n",
    "1. $10000 \\times 1$\n",
    "2. $300 \\times 10000$\n",
    "3. $300 \\times 300$\n",
    "\n",
    "\n",
    "<img src=\"images/skipgram_3.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Softmax activation function \n",
    "\n",
    "- Apply softmax to get probability distribution \n",
    "\n",
    "<img src=\"images/skipgram_4.png\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compare prediction ($\\hat{y}$) with \"gold\" ($y$)\n",
    "\n",
    "- Learn weights using backpropagation and gradient descent. \n",
    "- We want a number closer to 1 in the prediction at index 5,428\n",
    "    - Loss is high!\n",
    "\n",
    "<img src=\"images/skipgram_5.png\" width=\"800\" height=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fake word-prediction task \n",
    "\n",
    "- Given a target word (i.e., center word) word, predict context words (i.e., surrounding words). \n",
    "- Note that we are using \"target\" in a different sense here compared to how we use it in supervised machine learning.  \n",
    "<blockquote>\n",
    "    Add freshly squeezed$_{context}$ pineapple$_{target}$ juice$_{context}$ to your smoothie. \n",
    "</blockquote> \n",
    "\n",
    "<center>\n",
    "<img src=\"images/target_context.png\" width=\"300\" height=\"300\">\n",
    "</center>    \n",
    "\n",
    "- So in the example above given the target word **pineapple**, predict whether: \n",
    "    - **juice** is likely to occur in the context of **pineapple**\n",
    "    - **squeezed** is likely to occur in the context of **pineapple** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-gram model for two target-context pairs \n",
    "<center>\n",
    "<img src=\"images/skip-gram.png\" width=\"1000\" height=\"1000\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters to learn\n",
    "\n",
    "- Given a corpus with vocabulary of size $V$, where a word $w_i$ is identified by its index $i \\in {1, ..., V}$, learn a vector representation for each $w_i$ by predicting the words that appear in its context. \n",
    "- Learn the following parameters of the model\n",
    "    - Suppose $V = 10,000$, $d = 300$, the number of parameters to learn are 6,000,000! \n",
    "\n",
    "$$\n",
    "\\theta = \n",
    "\\begin{bmatrix} aardvark_t\\\\\n",
    "                aback_t\\\\\n",
    "                \\dots\\\\\n",
    "                zymurgi_t\\\\\n",
    "                aardvark_c\\\\\n",
    "                aback_c\\\\                \n",
    "                \\dots\\\\\n",
    "                zymurgi_c\\\\                \n",
    "\\end{bmatrix} \\in R^{2dV}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main hyperparameters of the model\n",
    "\n",
    "- Dimensionality of the word vectors \n",
    "- Window size\n",
    "    * shorter window: more syntactic representation\n",
    "    * longer window: more semantic representation \n",
    "    * Mikolov et al. (2015) suggest setting this parameter in the range 5 to 20 for small training datasets and in the range 2 to 5 for large training datasets.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Video lecture1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training word2vec embeddings \n",
    "\n",
    "- [Original C code](https://code.google.com/archive/p/word2vec/) \n",
    "- [GitHub version of the code](https://github.com/tmikolov/word2vec)\n",
    "- [Gensim](https://radimrehurek.com/gensim/), an open source Python library has provides a Python interface for word2vec family of algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "corpus:\n",
      " ['How tall is Machu Picchu?', 'Machu Picchu is 13.164 degrees south of the equator.', 'The official height of Machu Picchu is 2,430 m.', 'Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.', 'It is 80 kilometres (50 miles) northwest of Cusco, on the crest of the mountain Machu Picchu, located about 2,430 metres (7,970 feet) above mean sea level, over 1,000 metres (3,300 ft) lower than Cusco, which has an elevation of 3,400 metres (11,200 ft).']\n",
      "\n",
      "Preprocessed corpus: \n",
      " [['tall', 'machu', 'picchu'], ['machu', 'picchu', '13.164', 'degrees', 'south', 'equator'], ['official', 'height', 'machu', 'picchu', '2,430'], ['machu', 'picchu', '80', 'kilometres', '50', 'miles', 'northwest', 'cusco'], ['80', 'kilometres', '50', 'miles', 'northwest', 'cusco', 'crest', 'mountain', 'machu', 'picchu', 'located', '2,430', 'metres', '7,970', 'feet', 'mean', 'sea', 'level', '1,000', 'metres', '3,300', 'ft', 'lower', 'cusco', 'elevation', '3,400', 'metres', '11,200', 'ft']]\n"
     ]
    }
   ],
   "source": [
    "### First let's preprocess the corpus. \n",
    "### We already have done this for sparse represenation.  \n",
    "### Let's reuse it. \n",
    "print(\"\\ncorpus:\\n\", corpus)\n",
    "print(\"\\nPreprocessed corpus: \\n\", pp_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.8686343e-03, -4.3829111e-03,  8.2461245e-04, -2.5939611e-03,\n",
       "        3.8368141e-03, -6.4964296e-04, -1.1072899e-03, -1.6571992e-03,\n",
       "       -2.0547302e-03, -1.0690627e-03, -3.4108125e-03,  2.7660755e-03,\n",
       "        8.8897522e-04,  1.9116396e-03,  1.3635573e-03,  8.5409568e-04,\n",
       "        2.1677304e-03,  2.7105369e-04,  2.0211251e-03,  3.6618642e-03,\n",
       "       -9.9170825e-04, -1.3098032e-03,  1.0668045e-03, -1.0649072e-03,\n",
       "       -2.9500048e-03, -2.9360277e-03,  3.1440256e-03, -2.2108371e-03,\n",
       "       -5.1903533e-04, -3.2085821e-03,  3.9160475e-03,  1.1417432e-04,\n",
       "       -4.1589267e-03, -3.9233039e-03, -3.6281554e-03,  3.3286007e-03,\n",
       "        2.6871550e-03, -1.6688406e-04, -8.8384928e-05,  2.6939563e-03,\n",
       "       -3.0400269e-03,  1.6563045e-03, -1.4687789e-03, -6.1041053e-04,\n",
       "        3.8005984e-03,  2.4170585e-03, -1.7380904e-03,  4.3574700e-04,\n",
       "       -1.6147987e-03, -4.7198241e-03,  4.2445287e-03, -2.4527828e-03,\n",
       "        4.8504239e-03,  1.7698731e-03, -4.0522716e-03,  4.8034373e-03,\n",
       "       -3.9200257e-03, -2.0824620e-04, -2.6402634e-04, -2.3443410e-03,\n",
       "       -4.8656301e-03, -2.9785498e-03, -3.0992390e-03, -1.6119560e-03,\n",
       "       -2.1151955e-05,  4.1955733e-03, -1.1223253e-03,  4.9861828e-03,\n",
       "        1.2203163e-03,  2.1972251e-03, -2.5027657e-03, -7.0906687e-04,\n",
       "        2.8494294e-03, -2.3159382e-03,  4.3795104e-03,  2.0162291e-03,\n",
       "        2.4245610e-03,  4.4089727e-04,  2.7725918e-03, -5.7753472e-04,\n",
       "        1.0784416e-04,  2.4087285e-03,  1.2057422e-03, -1.2140234e-03,\n",
       "        2.6131405e-03, -1.7263648e-03,  2.5240812e-03,  4.8677940e-03,\n",
       "       -4.9263369e-03, -4.2265942e-04,  1.7522400e-03, -3.2665185e-03,\n",
       "       -2.0714758e-03,  5.3762423e-04,  4.6278920e-04,  4.4581643e-03,\n",
       "       -1.0119214e-03, -2.9026500e-03, -1.0403853e-03, -1.0925770e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build a word2vec model on our tiny machu picchu corpus\n",
    "# Just for demonstration. Won't give any meaningful relationships because \n",
    "# of the size of our corpus. \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(pp_corpus, \n",
    "                 size=100, \n",
    "                 window=4, \n",
    "                 min_count=1)\n",
    "\n",
    "# How does a learned dense word vector look like? \n",
    "model.wv['tall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other popular methods to get embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [fastText](https://fasttext.cc/)\n",
    "\n",
    "- NLP library by Facebook research  \n",
    "- Includes an algorithm which is an extension to Word2Vec\n",
    "- Helps deal with unknown words elegantly\n",
    "- Breaks words into several n-gram subwords \n",
    "- Example: trigram sub-words for *berry* are *ber*, *err*, *rry*)\n",
    "- Embedding(*berry*) = embedding(*ber*) + embedding(*err*) + embedding(rry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n",
    "- Starts with the co-occurrence matrix\n",
    "    - Co-occurrence can be interpreted as an indicator of semantic proximity of words\n",
    "- Takes advantage of global count statistics    \n",
    "- Predicts co-occurrence ratios\n",
    "- Loss based on word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pre-trained embeddings\n",
    "\n",
    "- Training embeddings is computationally expensive\n",
    "- For typical corpora, the vocabulary size is greater than 100,000.  \n",
    "- If the size of embeddings is 300, the number of parameters of the model is $2 \\times 30,000,000$. \n",
    "- So people have trained embeddings on huge corpora and made them available.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pre-trained embeddings\n",
    "\n",
    "A number of pre-trained word embeddings are available. The most popular ones are:  \n",
    "\n",
    "- [Word2Vec](https://code.google.com/archive/p/word2vec/)\n",
    "    * trained on several corpora using the word2vec algorithm \n",
    "- [wikipedia2vec](https://wikipedia2vec.github.io/wikipedia2vec/pretrained/)\n",
    "    * pretrained embeddings for 12 languages \n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using [the fastText algorithm](http://aclweb.org/anthology/Q17-1010)\n",
    "    * published by Facebook    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "# You can download them from here: https://code.google.com/archive/p/word2vec/\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('/Users/kvarada/MDS/2019-20/575/data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  3000000\n",
      "The similarity between height and tall is 0.473\n",
      "The similarity between pineapple and mango is 0.668\n",
      "The similarity between pineapple and juice is 0.418\n",
      "The similarity between sun and robot is 0.029\n",
      "The similarity between GPU and lion is 0.002\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary: ', len(model.vocab))\n",
    "word_pairs = [('height','tall'),\n",
    "              ('pineapple','mango'), \n",
    "              ('pineapple','juice'), \n",
    "              ('sun','robot'), \n",
    "              ('GPU','lion')]\n",
    "for pair in word_pairs: \n",
    "    print('The similarity between %s and %s is %0.3f' %(pair[0], pair[1], model.similarity(pair[0], pair[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding similar words \n",
    "\n",
    "- Given word $w$, search in the vector space for the word closest to $w$ as measured by cosine distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('UVic', 0.7886475324630737),\n",
       " ('SFU', 0.7588527202606201),\n",
       " ('Simon_Fraser', 0.7356574535369873),\n",
       " ('UFV', 0.688043475151062),\n",
       " ('VIU', 0.6778583526611328),\n",
       " ('Kwantlen', 0.677142858505249),\n",
       " ('UBCO', 0.6734488010406494),\n",
       " ('UPEI', 0.6731126308441162),\n",
       " ('UBC_Okanagan', 0.6709134578704834),\n",
       " ('Lakehead_University', 0.6622507572174072)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('UBC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('info', 0.7363681793212891),\n",
       " ('infomation', 0.6800296306610107),\n",
       " ('infor_mation', 0.6733849048614502),\n",
       " ('informaiton', 0.6639008522033691),\n",
       " ('informa_tion', 0.660125732421875),\n",
       " ('informationon', 0.6339334845542908),\n",
       " ('informationabout', 0.6320979595184326),\n",
       " ('Information', 0.6186580657958984),\n",
       " ('informaion', 0.6093292236328125),\n",
       " ('details', 0.6063088774681091)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Captures different contracted forms and mispelled occurrences \n",
    "model.most_similar('information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Success of Word2Vec\n",
    "\n",
    "- Able to capture complex relationships between words.\n",
    "- Example: What is the word that is similar to **WOMAN** in the same sense as **KING** is similar to **MAN**?\n",
    "- Perform a simple algebraic operations with the vector representation of words.\n",
    "    $\\vec{X} = \\vec{\\text{KING}} − \\vec{\\text{MAN}} + \\vec{\\text{WOMAN}}$\n",
    "- Search in the vector space for the word closest to $\\vec{X}$ measured by cosine distance.\n",
    "\n",
    "<img src=\"images/word_analogies1.png\" width=\"500\" height=\"500\">\n",
    "(Credit: Mikolov et al. 2013)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model = model):\n",
    "    '''    \n",
    "    Returns analogy word using the given model. \n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str) \n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation    \n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation         \n",
    "    model : \n",
    "        word embedding model\n",
    "    \n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    '''\n",
    "    print('%s : %s :: %s : ?' %(word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=['Analogy word', 'Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : king :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.711819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>monarch</td>\n",
       "      <td>0.618967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>princess</td>\n",
       "      <td>0.590243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>crown_prince</td>\n",
       "      <td>0.549946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prince</td>\n",
       "      <td>0.537732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kings</td>\n",
       "      <td>0.523684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Queen_Consort</td>\n",
       "      <td>0.523595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>queens</td>\n",
       "      <td>0.518113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sultan</td>\n",
       "      <td>0.509859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>monarchy</td>\n",
       "      <td>0.508741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Analogy word     Score\n",
       "0          queen  0.711819\n",
       "1        monarch  0.618967\n",
       "2       princess  0.590243\n",
       "3   crown_prince  0.549946\n",
       "4         prince  0.537732\n",
       "5          kings  0.523684\n",
       "6  Queen_Consort  0.523595\n",
       "7         queens  0.518113\n",
       "8         sultan  0.509859\n",
       "9       monarchy  0.508741"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man','king','woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montreal : Canadiens :: Vancouver : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canucks</td>\n",
       "      <td>0.821327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vancouver_Canucks</td>\n",
       "      <td>0.750401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Calgary_Flames</td>\n",
       "      <td>0.705470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leafs</td>\n",
       "      <td>0.695783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maple_Leafs</td>\n",
       "      <td>0.691617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thrashers</td>\n",
       "      <td>0.687504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avs</td>\n",
       "      <td>0.681716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sabres</td>\n",
       "      <td>0.665307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Blackhawks</td>\n",
       "      <td>0.664625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Habs</td>\n",
       "      <td>0.661023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Analogy word     Score\n",
       "0            Canucks  0.821327\n",
       "1  Vancouver_Canucks  0.750401\n",
       "2     Calgary_Flames  0.705470\n",
       "3              Leafs  0.695783\n",
       "4        Maple_Leafs  0.691617\n",
       "5          Thrashers  0.687504\n",
       "6                Avs  0.681716\n",
       "7             Sabres  0.665307\n",
       "8         Blackhawks  0.664625\n",
       "9               Habs  0.661023"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('Montreal', 'Canadiens', 'Vancouver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toronto : UofT :: Vancouver : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SFU</td>\n",
       "      <td>0.579245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UVic</td>\n",
       "      <td>0.576921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UBC</td>\n",
       "      <td>0.571431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Simon_Fraser</td>\n",
       "      <td>0.543464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Langara_College</td>\n",
       "      <td>0.541347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UVIC</td>\n",
       "      <td>0.520495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Grant_MacEwan</td>\n",
       "      <td>0.517273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UFV</td>\n",
       "      <td>0.514150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ubyssey</td>\n",
       "      <td>0.510421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kwantlen</td>\n",
       "      <td>0.503807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Analogy word     Score\n",
       "0              SFU  0.579245\n",
       "1             UVic  0.576921\n",
       "2              UBC  0.571431\n",
       "3     Simon_Fraser  0.543464\n",
       "4  Langara_College  0.541347\n",
       "5             UVIC  0.520495\n",
       "6    Grant_MacEwan  0.517273\n",
       "7              UFV  0.514150\n",
       "8          Ubyssey  0.510421\n",
       "9         Kwantlen  0.503807"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Recall the title of today's lesson \n",
    "analogy('Toronto', 'UofT', 'Vancouver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples of semantic and syntactic relationships\n",
    "\n",
    "<center>\n",
    "<img src=\"files/images/word_analogies2.png\" width=\"800\" height=\"800\">\n",
    "(Credit: Mikolov 2013)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implicit biases and stereotypes in word embeddings\n",
    "\n",
    "- Reflect gender stereotypes present in broader society.\n",
    "- They may also amplify these stereotypes because of their widespread usage. \n",
    "- See the paper [Man is to Computer Programmer as Woman is to ...](http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : computer_programmer :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>homemaker</td>\n",
       "      <td>0.562712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>housewife</td>\n",
       "      <td>0.510505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>graphic_designer</td>\n",
       "      <td>0.505180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>schoolteacher</td>\n",
       "      <td>0.497949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>businesswoman</td>\n",
       "      <td>0.493489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paralegal</td>\n",
       "      <td>0.492551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>registered_nurse</td>\n",
       "      <td>0.490797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saleswoman</td>\n",
       "      <td>0.488163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>electrical_engineer</td>\n",
       "      <td>0.479773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mechanical_engineer</td>\n",
       "      <td>0.475540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Analogy word     Score\n",
       "0            homemaker  0.562712\n",
       "1            housewife  0.510505\n",
       "2     graphic_designer  0.505180\n",
       "3        schoolteacher  0.497949\n",
       "4        businesswoman  0.493489\n",
       "5            paralegal  0.492551\n",
       "6     registered_nurse  0.490797\n",
       "7           saleswoman  0.488163\n",
       "8  electrical_engineer  0.479773\n",
       "9  mechanical_engineer  0.475540"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man', 'computer_programmer', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- Vector space model \n",
    "    * Modeling word meaning by placing it in a vector space.\n",
    "    * Distance between words in this vector space indicate the relationship between them. \n",
    "- Word embeddings\n",
    "    * Sparse embeddings using co-occurrence matrix\n",
    "    * Dense embeddings using word2vec models \n",
    "        * Freely available code and pre-trained models \n",
    "        * Available for many different languages. \n",
    "        * Stereotypes in the society reflected in word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Post-assessment\n",
    "\n",
    "1. Word representation created by term-term co-occurrence matrix are long and sparse whereas the ones created by word2vec models are short and dense. True or False? \n",
    "2. The skip-gram model predicts context word given a target word. True or False? \n",
    "3. Given the following table, which word pair is more similar in terms of dot product: (word 1, word 2) or (word 1, word 3)?\n",
    "\n",
    "<img src=\"images/similarity_question.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "4. True or False? Suppose you learn word embeddings for a vocabulary of 20,000 words using Word2Vec. Then each dense word embedding associated with a word is of size 20,000 to make sure that we capture the full range of meaning of that word.\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relevant papers\n",
    "\n",
    "- [Distributed representations of words and phrases and their compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- [Efficient estimation of word representations in vector space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "- [Linguistic regularities in continuous space word representations](https://www.aclweb.org/anthology/N13-1090)\n",
    "- [Enriching Word Vectors with Subword Information](http://aclweb.org/anthology/Q17-1010)\n",
    "\n",
    "## Fun tools\n",
    "[wevi: word embedding visual inspector](https://ronxin.github.io/wevi/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
