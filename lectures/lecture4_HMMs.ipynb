{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DSCI 575: Advanced Machine Learning (in the context of Natural Language Processing (NLP) applications)\n",
    "\n",
    "UBC Master of Data Science program, 2019-20\n",
    "\n",
    "Instructor: Varada Kolhatkar [Ê‹É™É¾É™da kÉ”ËlÉ¦É™ÊˆkÉ™r]\n",
    "\n",
    "## Lecture 4: Hidden Markov Models (HMMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning outcomes\n",
    "\n",
    "From this lesson you will be able to\n",
    "\n",
    "- explain the motivation for using HMMs\n",
    "- define an HMM\n",
    "- state the Markov assumption in HMMs\n",
    "- explain three fundamental questions for an HMM\n",
    "- apply the forward algorithm given an HMM\n",
    "- explain supervised training in HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Observable Markov models \n",
    "\n",
    "- Example\n",
    "    - States: {uniformly, are, charming}   \n",
    "    \n",
    "<img src=\"images/observable_Markov.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/A.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden phenomenon \n",
    "\n",
    "Very often the things you observe in the real world are only a function of some other **hidden** variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden phenomenon example \n",
    "\n",
    "- Speech sounds are the outputs of hidden phonemes\n",
    "- Phonemes\n",
    "    - distinct units of sound\n",
    "    - Example: seven $\\rightarrow$ seh v ax n\n",
    "    \n",
    "<img src=\"images/hmm_eks.gif\" height=\"600\" width=\"600\"> \n",
    "\n",
    "\n",
    "[Source](https://www.uea.ac.uk/computing/research-at-the-uea-speech-group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden phenomenon example \n",
    "\n",
    "- Words are the outputs of hidden parts-of-speech\n",
    "\n",
    "\n",
    "<img src=\"images/hmm_pos_tagging.png\" height=\"1000\" width=\"1000\"> \n",
    "\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/8.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden phenomenon \n",
    "\n",
    "More examples\n",
    "\n",
    "- Encrypted symbols are outputs of hidden messages\n",
    "- Genes are outputs of functional relationships\n",
    "- Stock prices or trader's mood are the output of market conditions\n",
    "\n",
    "\n",
    "<img src=\"images/stock_market_hmm.png\" height=\"1000\" width=\"1000\"> \n",
    "\n",
    "\n",
    "[Source](https://letianquant.com/hidden-markov-chain.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Markov process with hidden variables: Example\n",
    "\n",
    "- Suppose you have a little robot that is trying to estimate the posterior probability that you are **Happy (H or ğŸ™‚)** or **Sad (S or ğŸ˜”)**, given that the robot has observed whether you are doing one of the following activities: \n",
    "    - **Learning data science (L or ğŸ“š)**, \n",
    "    - **Eat (E or ğŸ)**, \n",
    "    - **Cry (C or ğŸ˜¿)**, \n",
    "    - **Social media (F)**\n",
    "\n",
    "- The robot is trying to estimate the unknown (hidden) state $Q$, where $Q =H$ when you are happy (ğŸ™‚) and $Q = S$ when you are sad (ğŸ˜”). \n",
    "- The robot is able to observe the activity you are doing: $O = {L, E, C, F}$ \n",
    "\n",
    "(Attribution: Example adapted from [here](https://www.cs.ubc.ca/~nando/340-2012/lectures/l6.pdf).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Markov process with hidden variables: Example\n",
    "\n",
    "- Example questions we are interested in answering are:\n",
    "    - What is $P(Q = ğŸ˜”|O = F)$?\n",
    "    - What is the best possible sequence of state of mind (e.g.,ğŸ™‚,ğŸ™‚,ğŸ˜”,ğŸ™‚,ğŸ™‚ ) given an observation sequence (e.g., L,L,C,L,L). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HMM ingredients\n",
    "\n",
    "- State space (e.g., ğŸ™‚ (H), ğŸ˜” (S))\n",
    "- An initial probability distribution over the states (categorical)\n",
    "- Transition probabilities (categorical) \n",
    "- **Emission probabilities (categorical)** \n",
    "    - Conditional probabilities for all observations given a hidden state\n",
    "    - Example: Below $P(L|ğŸ™‚) = 0.7$ and $P(L|ğŸ˜”) = 0.1$\n",
    "    \n",
    "\n",
    "<img src=\"files/images/HMM_example.png\" height=\"600\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of an HMM\n",
    "\n",
    "- A hidden Markov model (HMM) is specified by the 5-tuple:  $\\{S, O, \\pi, T, B\\}$ \n",
    "    - $S = \\{s_1, s_2, \\dots, s_n\\}$ is a set of states (e.g., moods)\n",
    "    - **$Y = \\{y_1, y_2, \\dots, y_k\\}$ is output alphabet (e.g., set of activities)**\n",
    "    - $\\pi = {\\pi_1, \\pi_2, \\dots, \\pi_n}$ is discrete initial state probability distribution \n",
    "    - Transition probability matrix $T$, where each $a_{ij}$ represents the probability of moving from state $s_i$ to state $s_j$\n",
    "    - **Emission probabilities B = $b_i(o), i \\in S, o \\in Y\\$**\n",
    "    \n",
    "<img src=\"files/images/HMM_example.png\" height=\"600\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of an HMM continued\n",
    "\n",
    "- Yielding the state sequence and the observation sequences in an unrolled HMM \n",
    "    - State sequence: $Q = {q_0,q_1, q_2, \\dots q_T}, q_i \\in S$ \n",
    "    - Observation sequence: $O = {o_0,o_1, o_2, \\dots o_T}, o_i \\in Y$\n",
    "\n",
    "<img src=\"files/images/HMM_example.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "\n",
    "<img src=\"files/images/HMM_unrolling_timesteps.png\" height=\"700\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unrolling the timesteps \n",
    "\n",
    "- Each state produces only a single observation and the sequence of hidden states and the sequence of observations have the same length. \n",
    "\n",
    "\n",
    "<img src=\"files/images/HMM_unrolling_timesteps.png\" height=\"700\" width=\"700\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HMM assumptions\n",
    "\n",
    "- **The probability of a particular state only depends on the previous state.**\n",
    "    * $P(q_i|q_0,q_1,\\dots,q_{i-1})$ = $P(q_i|q_{i-1})$\n",
    "    \n",
    "- **The probability of an output observation $o_i$ depends only on the state that produces the observation and not on any other state or any other observation.** \n",
    "    * $P(o_i|q_0,q_1,\\dots,q_{i-1}, o_0,o_1,\\dots,o_{i-1})$ = $P(o_i|q_i)$\n",
    "\n",
    "<img src=\"files/images/HMM_unrolling_timesteps.png\" height=\"800\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Questions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Three fundamental questions for an HMM\n",
    "\n",
    "#### Likelihood\n",
    "Given a model with parameters $\\theta = <\\pi, T, B>$, how do we efficiently compute the likelihood of a particular observation sequence $O$?\n",
    "#### Decoding\n",
    "Given an observation sequence $O$ and a model $\\theta$ how do we choose a state sequence $Q={q_0, q_1, \\dots q_T}$ that best explains the observation sequence?\n",
    "#### Learning\n",
    "Training: Given a large observation sequence $O$ how do we choose the best parameters $\\theta$ that explain the data $O$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Likelihood\n",
    "\n",
    "Given a model with parameters $\\theta = <\\pi, T, B>$, how do we efficiently compute the likelihood of a particular observation sequence $O$?\n",
    "\n",
    "- Example: What's the probability of the sequence below? \n",
    "\n",
    "<img src=\"files/images/HMM_example_activity_seq.png\" height=\"400\" width=\"400\"> \n",
    "\n",
    "- Recall that in HMMs, the observations are dependent upon the hidden states in the same time step. \n",
    "<br><br>\n",
    "\n",
    "<img src=\"files/images/HMM_likelihood_known_hidden.png\" height=\"500\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Probability of an observation sequence given state sequence \n",
    "\n",
    "- Suppose we know both the sequence of hidden states (moods) and the sequence of activities emitted by them. \n",
    "- $P(O|Q) = \\prod\\limits_{i=1}^{T} P(o_i|q_i)$\n",
    "- $P(E L F C|ğŸ™‚ ğŸ™‚ ğŸ˜” ğŸ˜”) = P(E|ğŸ™‚) \\times P(L|ğŸ™‚) \\times P(F|ğŸ˜”) \\times P(C|ğŸ˜”)$\n",
    "\n",
    "<img src=\"files/images/HMM_likelihood_known_hidden.png\" height=\"400\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Joint probability of observations and a possible hidden sequence \n",
    "\n",
    "- But we do not know what the hidden state sequence was. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Joint probability of observations and a possible hidden sequence \n",
    "\n",
    "- We need to look at hidden states. \n",
    "- Let's consider the joint probability of being in a particular state sequence $Q$ and generating a particular sequence $O$ of activities. \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"files/images/HMM_likelihood_unknown_hidden.png\" height=\"800\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Joint probability of observations and a possible hidden sequence \n",
    "\n",
    "- $P(O,Q) = P(O|Q)\\times P(Q) = \\prod\\limits_{i=1}^T P(o_i|q_i) \\times \\prod\\limits_{i=1}^T P(q_i|q_{i-1})$ \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(E L F C, ğŸ™‚ ğŸ™‚ ğŸ˜” ğŸ˜”) = & P(ğŸ™‚|start)\\\\ \n",
    "                          & \\times P(ğŸ™‚|ğŸ™‚) \\times P(ğŸ˜”|ğŸ™‚) \\times P(ğŸ˜”|ğŸ˜”)\\\\\n",
    "                          & \\times P(E|ğŸ™‚) \\times P(L|ğŸ™‚) \\times P(F|ğŸ˜”) \\times P(C|ğŸ˜”)\\\\\n",
    "                      = & 0.8 \\times 0.7 \\times 0.3 \\times 0.6 \\times 0.2 \\times 0.7 \\times 0.2 \\times 0.6 \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "<br>\n",
    "<img src=\"files/images/HMM_likelihood_unknown_hidden.png\" height=\"700\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Total probability of an observation sequence \n",
    "\n",
    "- But we do not know the state sequence $Q$\n",
    "- We need to compute the probability of activity sequence (ELFC) by summing over all possible state (mood) sequences.  \n",
    "\n",
    "- $P(O) = \\sum\\limits_Q P(O,Q) = \\sum\\limits_QP(O|Q)P(Q)$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(E L F C) = & P(E L F C,ğŸ™‚ğŸ™‚ğŸ™‚ğŸ™‚)\\\\ \n",
    "             & + P(E L F C,ğŸ™‚ğŸ™‚ğŸ™‚ğŸ˜”)\\\\\n",
    "             & + P(E L F C,ğŸ™‚ğŸ™‚ğŸ˜”ğŸ˜”) + \\dots\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "- Computationally inefficient \n",
    "    - For HMMs with $n$ hidden states and an observation sequence of $T$ observations, there are $n^T$ possible hidden sequences!!\n",
    "    - In real-world problems both $n$ and $T$ are large numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to compute $P(O)$ cleverly? \n",
    "\n",
    "- To avoid this complexity we use **dynamic programming**; we remember the results rather than recomputing them. \n",
    "- We make a **trellis** which is an array of states vs. time.\n",
    "- The element at $(i,t)$ is $\\alpha_i(t)$, which is the probability of being in state $i$ at time $t$ after seeing all previous observations: $P(o_{1:t-1}, q_t = s_i;\\theta)$\n",
    "\n",
    "<img src=\"files/images/HMM_trellis.png\" height=\"600\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trellis \n",
    "\n",
    "- Note the alternative paths in the trellis\n",
    "\n",
    "<img src=\"files/images/HMM_trellis.png\" height=\"600\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: intuition \n",
    "\n",
    "- To compute $\\alpha_j(t)$, we can compute $\\alpha_{i}(t-1)$ for all possible states $i$ and then use our knowledge of $a_{ij}$ and $b_j(o_t)$.\n",
    "- We compute the trellis left-to-right because of the convention of time.\n",
    "- Remember that $o_t$ is fixed and known.\n",
    "<center>\n",
    "<img src=\"files/images/HMM_trellis.png\" height=\"600\" width=\"600\"> \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure\n",
    "\n",
    "Three steps of the forward procedure. \n",
    "\n",
    "- Initialization: Compute the $\\alpha$ values for nodes in the first column of the trellis $(t = 0)$.\n",
    "- Induction: Iteratively compute the $\\alpha$ values for nodes in the rest of the trellis $(1 \\leq t < T)$.\n",
    "- Conclusion: Sum over the $\\alpha$ values for nodes in the last column of the trellis $(t = T)$.\n",
    "\n",
    "<img src=\"files/images/HMM_example_trellis.png\" height=\"800\" width=\"800\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Initialization $\\alpha_ğŸ™‚(0)$ and $\\alpha_ğŸ˜”(0)$\n",
    "\n",
    "- Compute the nodes in the first column of the trellis $(T = 0)$.\n",
    "    * Probability of starting at state ğŸ™‚ and observing the activity E: $\\alpha_ğŸ™‚(0) = \\pi_ğŸ™‚ \\times b_ğŸ™‚(E) = 0.8 \\times 0.2 = 0.16$ \n",
    "    * Probability of starting at state ğŸ˜” and observing the activity E: $\\alpha_ğŸ˜”(0) = \\pi_ğŸ˜” \\times b_ğŸ˜”(E) = 0.2 \\times 0.1 = 0.02$  \n",
    "\n",
    "\n",
    "<img src=\"files/images/HMM_example_trellis.png\" height=\"1000\" width=\"1000\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction\n",
    "\n",
    "- Iteratively compute the nodes in the rest of the trellis $(1 \\leq t < T)$.\n",
    "-  To compute $\\alpha_j(t+1)$ we can compute $\\alpha_{i}(t)$ for all possible states $i$ and then use our knowledge of $a_{ij}$ and $b_j(o_{t+1})$ \n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "\n",
    "<img src=\"files/images/HMM_example_trellis.png\" height=\"1000\" width=\"1000\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction $\\alpha_ğŸ™‚(1)$\n",
    "\n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "\n",
    "- Probability of being at state ğŸ™‚ at $t=1$ and observing the activity L\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\alpha_ğŸ™‚(1) = & \\alpha_ğŸ™‚(0)a_{ğŸ™‚ğŸ™‚}b_ğŸ™‚(L) + \\alpha_ğŸ˜”(0)a_{ğŸ˜”ğŸ™‚}b_ğŸ™‚(L)\\\\\n",
    "             = & 0.16 \\times 0.7 \\times 0.7 + 0.02 \\times 0.4 \\times 0.7\\\\ \n",
    "             = & 0.084\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "<img src=\"files/images/HMM_example_trellis.png\" height=\"700\" width=\"700\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction $\\alpha_ğŸ˜”(1)$\n",
    "\n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "- Probability of being at state ğŸ˜” at $t=1$ and observing the activity L:\n",
    "\\begin{equation}\n",
    "\\begin{split}             \n",
    "\\alpha_ğŸ˜”(1) = & \\alpha_ğŸ™‚(0)a_{ğŸ™‚ğŸ˜”}b_ğŸ˜”(L) + \\alpha_ğŸ˜”(0)a_{ğŸ˜”ğŸ˜”}b_ğŸ˜”(L)\\\\\n",
    "             = & 0.16 \\times 0.3 \\times 0.1 + 0.02 \\times 0.6 \\times 0.1\\\\\n",
    "             = & 0.006\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"files/images/HMM_example_trellis.png\" height=\"700\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction $\\alpha_ğŸ™‚(2)$\n",
    "\n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "\n",
    "- Probability of being at state ğŸ™‚ at $t=2$ and observing the activity F\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\alpha_ğŸ™‚(2) = & \\alpha_ğŸ™‚(1)a_{ğŸ™‚ğŸ™‚}b_ğŸ™‚(F) + \\alpha_ğŸ˜”(1)a_{ğŸ˜”ğŸ™‚}b_ğŸ™‚(F)\\\\\n",
    "             = & 0.084 \\times 0.7 \\times 0.0 + 0.006 \\times 0.4 \\times 0.0\\\\ \n",
    "             = & 0.0\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"files/images/HMM_example_trellis.png\" height=\"700\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction $\\alpha_ğŸ˜”(2)$\n",
    "\n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "- Probability of being at state ğŸ˜” at $t=2$ and observing the activity F:\n",
    "\\begin{equation}\n",
    "\\begin{split}             \n",
    "\\alpha_ğŸ˜”(2) = & \\alpha_ğŸ™‚(1)a_{ğŸ™‚ğŸ˜”}b_ğŸ˜”(F) + \\alpha_ğŸ˜”(1)a_{ğŸ˜”ğŸ˜”}b_ğŸ˜”(F)\\\\\n",
    "             = & 0.084 \\times 0.3 \\times 0.2 + 0.006 \\times 0.6 \\times 0.2\\\\\n",
    "             = & 0.00576\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "<img src=\"files/images/HMM_example_trellis.png\" height=\"700\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction $\\alpha_ğŸ™‚(3)$ (Activity)\n",
    "\n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "\n",
    "- Probability of being at state ğŸ™‚ at $t=3$ and observing the activity C:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\alpha_ğŸ™‚(3) = & \\alpha_ğŸ™‚(2)a_{ğŸ™‚ğŸ™‚}b_ğŸ™‚(C) + \\alpha_ğŸ˜”(2)a_{ğŸ˜”ğŸ™‚}b_ğŸ™‚(C)\\\\\n",
    "             = & 0 \\times 0.7 \\times 0.1 + 0.00576 \\times 0.4 \\times 0.1\\\\ \n",
    "             = & 2.3 \\times 10^{-4}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "<img src=\"files/images/HMM_example_trellis.png\" height=\"700\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Induction $\\alpha_ğŸ˜”(3)$ (Activity)\n",
    "\n",
    "- $\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})$\n",
    "- Probability of being at state ğŸ˜” at $t=3$ and observing the activity C:\n",
    "\\begin{equation}\n",
    "\\begin{split}             \n",
    "\\alpha_ğŸ˜”(3) = & \\alpha_ğŸ™‚(2)a_{ğŸ™‚ğŸ˜”}b_ğŸ˜”(C) + \\alpha_ğŸ˜”(2)a_{ğŸ˜”ğŸ˜”}b_ğŸ˜”(C)\\\\\n",
    "             = & 0.0 \\times 0.3 \\times 0.6 + 0.00576 \\times 0.6 \\times 0.6\\\\\n",
    "             = & 2.07 \\times 10^{-3}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "<img src=\"files/images/HMM_example_trellis.png\" height=\"700\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The forward procedure: Conclusion\n",
    "\n",
    "- Sum over all possible final states:\n",
    "  * $P(O;\\theta) = \\sum\\limits_{i=1}^{n}\\alpha_i(T-1)$\n",
    "  * $P(E,L,F,C) = \\alpha_ğŸ™‚(3) + \\alpha_ğŸ˜”(3) = 2.3 \\times 10^{-4} + 2.07 \\times 10^{-3}$ \n",
    "\n",
    "- The forward procedure using dynamic programming needs only $2N^2T$ multiplications compared to the $(2T)N^T$ multiplications with the naive approach!! \n",
    "\n",
    "<img src=\"files/images/HMM_example_trellis.png\" height=\"700\" width=\"700\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Generation with an HMM\n",
    "\n",
    "- An HMM is a generative model and we can generate new sequences using an HMM\n",
    "- $t = 0$\n",
    "- Start in state $q_0$ = $s_i$ with probability $\\pi_i$\n",
    "- Emit observation symbol $o_0 = y_k$ with probability $b_i(o_0)$\n",
    "- While (not forever): \n",
    "    * Go from state $q_t = s_i$ to state $q_{t+1} = s_j$ with probability $a_{ij}$\n",
    "    * Emit observation symbol $o_{t+1} = y_k$ with probability $b_j(o_{t+1})$\n",
    "    * $t = t + 1$  \n",
    "    \n",
    "<img src=\"files/images/HMM_example.png\" height=\"500\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised training of HMMs\n",
    "\n",
    "- Suppose we have training data where we have $O$ and corresponding $Q$, then we can use MLE to learn parameters $\\theta = <\\pi, T, B>$\n",
    "- Get transition matrix and the emission probabilities. \n",
    "    - Suppose $i$, $j$ are unique states from the state space and $k$ is a unique observation.    \n",
    "    - $\\pi_0(i) = P(q_0 = i) = \\frac{Count(q_0 = i)}{\\#samples}$\n",
    "    - $a_{ij} = P(q_{t+1} = j|q_t = i) = \\frac{Count(i,j)}{Count(i)}$\n",
    "    - $b_{ik} = P(o_{t} = k|q_t = i) = \\frac{Count(i,k)}{Count(i)}$\n",
    "\n",
    "<img src=\"files/images/HMM_unrolling_timesteps.png\" height=\"700\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Supervised training of HMMs\n",
    "\n",
    "- Suppose we have training data where we have $O$ and corresponding $Q$, then we can use MLE to learn parameters $\\theta = <\\pi, T, B>$\n",
    "    - Count how often $q_{i-1}$ and $q_i$ occur together normalized by how often $q_{i-1}$ occurs: \n",
    "      $p(q_i|q_{i-1}) = \\frac{Count(q_{i-1} q_i)}{Count(q_{i-1})}$\n",
    "    - Count how often $q_i$ is associated with the observation $o_i$.   \n",
    "      $p(o_i|q_{i}) = \\frac{Count(o_i \\wedge q_i)}{Count(q_{i})}$    \n",
    "\n",
    "<center>\n",
    "<img src=\"files/images/HMM_unrolling_timesteps.png\" height=\"700\" width=\"700\"> \n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unsupervised Learning of HMMs \n",
    "\n",
    "- So far we were assuming a supervised setting where we knew the hidden states $S$. \n",
    "- We used MLE to get transition matrix and the emission probabilities. \n",
    "- In many cases, the number of states is unknown and we cannot count them. \n",
    "- How to deal with the incomplete data?\n",
    "    - Use expectation-maximization\n",
    "    - Baum-Welch re-estimation\n",
    "    - We do not have time to talk about it in this class but if curious here are some resources:\n",
    "        * [Frank Rudzicz's slides](http://www.cs.toronto.edu/~frank/csc401/lectures2018/5-HMMs.pdf) (from page 77 to 95). \n",
    "        * [Andrew McCallum's slides](https://people.cs.umass.edu/~mccallum/courses/inlp2004a/lect10-hmm2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Optional) HMMs with [ `hmmlearn`](https://hmmlearn.readthedocs.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 3 2]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultinomialHMM' object has no attribute 'transmat_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-94e4de90aa3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mobservation_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#print('loglikelihood of X: ', model.score(observation_sequence))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Assume the following observation sequence:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/hmmlearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \"\"\"\n\u001b[1;32m    280\u001b[0m         \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"startprob_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/hmmlearn/hmm.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memissionprob_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memissionprob_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/hmmlearn/base.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    569\u001b[0m                              .format(self.startprob_.sum()))\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmat_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmat_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmat_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultinomialHMM' object has no attribute 'transmat_'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Initializing an HMM \n",
    "states = ['Happy', 'Sad']\n",
    "n_states = len(states)\n",
    "\n",
    "observations = ['Learn', 'Eat', 'Cry', 'Facebook']\n",
    "n_observations = len(observations)\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=n_states)\n",
    "model.startprob_ = np.array([0.8,0.2])\n",
    "model.transprob_ = np.array([\n",
    " [0.7, 0.3],\n",
    " [0.4, 0.6]\n",
    "])\n",
    "model.emissionprob_ = np.array([\n",
    "    [0.6, 0.3, 0.1, 0.0],\n",
    "    [0.1, 0.1, 0.6, 0.2]\n",
    "])\n",
    "observation_sequence = np.array([[1, 0, 3, 2]])\n",
    "print(observation_sequence)\n",
    "model.\n",
    "#print('loglikelihood of X: ', model.score(observation_sequence))\n",
    "# Assume the following observation sequence: \n",
    "# Learn, Learn, Cry, Facebook, Cry, Learn, Eat, Learn, Eat, Cry, Cry\n",
    "#observation_sequence = np.array([[1, 0, 3, 2]]).T\n",
    "#print(observation_sequence)\n",
    "#print('loglikelihood of X: ', model.score(observation_sequence))\n",
    "\n",
    "# Fit the model\n",
    "#model = model.fit(observation_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]]\n",
      "[0 0 1 1 0]\n",
      "loglikelihood of X:  -5.524303857896946\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n",
      "[0 0 0 0 0 0 1 1 1]\n",
      "loglikelihood of X:  -8.276619324554098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kvarada/anaconda3/lib/python3.6/site-packages/hmmlearn/hmm.py:412: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.emissionprob_)[:, np.concatenate(X)].T\n",
      "/Users/kvarada/anaconda3/lib/python3.6/site-packages/hmmlearn/hmm.py:412: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.emissionprob_)[:, np.concatenate(X)].T\n"
     ]
    }
   ],
   "source": [
    "# Likelihood computation\n",
    "X, Z = model.sample(5)\n",
    "print(X)\n",
    "print(Z)\n",
    "print('loglikelihood of X: ', model.score(X))\n",
    "X, Z = model.sample(9)\n",
    "print(X)\n",
    "print(Z)\n",
    "print('loglikelihood of X: ', model.score(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- Hidden Markov models (HMMs) model time-series with latent factors\n",
    "- There are tons of applications associated with them and they are more realistic than Markov chains\n",
    "\n",
    "Important ideas we learned \n",
    "- The definition of an HMM\n",
    "- Three fundamental questions for HMMs\n",
    "- The purpose of the forward algorithm and how to calculate $\\alpha_i(t)$\n",
    "- Supervised training in HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other useful/interesting material \n",
    "\n",
    "- [Hidden Markov Models chapter from Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/A.pdf)\n",
    "- Attribution: Many presentation ideas in this notebook are taken from [Frank Rudzicz's slides](http://www.cs.toronto.edu/~frank/csc401/lectures2018/5-HMMs.pdf).\n",
    "- [Jason Eisner's lecture on hidden Markov Models](https://vimeo.com/31374528)\n",
    "- [Jason Eisner's interactive spreadsheet for HMMs](https://cs.jhu.edu/~jason/papers/eisner.hmm.xls)\n",
    "- [Who each player is guarding?](https://www.youtube.com/watch?v=JvNkZdZJBt4)\n",
    "- [The Viterbi Algorithm: A Personal History](https://arxiv.org/pdf/cs/0504020v2.pdf)\n",
    "- [A nice demo of independent vs. Markov vs. HMMs for DNA](https://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/src/chapter10.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
