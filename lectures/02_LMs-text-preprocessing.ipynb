{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/575_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 2: Applications of Markov Models  and Text Preprocessing\n",
    "\n",
    "UBC Master of Data Science program, 2022-23\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture plan, imports, LO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Lecture plan \n",
    "\n",
    "- Stationary distribution recap and leftover T/F (~15 mins)\n",
    "- Learning Markov models (~5 mins)\n",
    "- Language models (~20 mins)\n",
    "- Break (~5 mins)\n",
    "- Q&A and T/F (~5 mins)\n",
    "- PageRank (~5)\n",
    "- Preprocessing (~15)\n",
    "- Final comments, summary, reflection (~5 mins)\n",
    "- Questions for class discussion (~5 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import IPython\n",
    "import nltk\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "from ipywidgets import interactive\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "###  Learning outcomes <a name=\"lo\"></a>\n",
    "\n",
    "By the end of this class you will be able to \n",
    "- Explain learning of Markov models. \n",
    "- Given sequence data, estimate the transition matrix from the data. \n",
    "- Build a Markov model of language. \n",
    "- Generalize Markov model of language to consider more history. \n",
    "- Explain states, state space, and transition matrix in Markov models of language. \n",
    "- Explain and calculate stationary distribution over states in Markov models of language. \n",
    "- Generate text using Markov model of language. \n",
    "- Explain the intuition of the PageRank algorithm. \n",
    "- Carry out basic text preprocessing using `nltk` and `spaCy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "- Stationary distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Markov models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Similar to naive Bayes, learning Markov models is just counting.\n",
    "- Given $n$ samples ($n$ sequences), MLE for homogeneous Markov model is:\n",
    "\n",
    "    * Initial: $P(s_i) = \\frac{\\text{number of times we start in } s_i}{n} $\n",
    "\n",
    "    * Transition: $P(s_j \\mid s_{i}) = \\frac{\\text{number of times we moved from } s_{i} \\text{ to } s_j}{\\text{number of times we moved from } s_{i} \\text{ to } anything}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Suppose you want to learn a Markov chain for words in a corpus of $n$ documents.\n",
    "- Set of states is the set of all unique words in the corpus.\n",
    "- Calculate the initial probability distribution $\\pi_0$\n",
    "    - For all states (unique words) $w_i$, compute $\\frac{\\text{number of times a document starts with } w_i}{n} $ \n",
    "    \n",
    "- Calculate transition probabilities for all state combinations $w_i$ and $w_j$\n",
    "    - $\\frac{\\text{number of times } w_i \\text{ precedes } w_j}{\\text{number of times } w_i \\text{ precedes anything}}$ \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with daily activities as states\n",
    "\n",
    "![](img/activity-seqs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What's $\\pi_0$(ðŸ˜´)?\n",
    "- What's $\\pi_0$(ðŸŽ)?\n",
    "- What is P(ðŸŽ|ðŸ“š)? \n",
    "    - $\\frac{\\text{number of times ðŸ“š precedes ðŸŽ}}{\\text{number of times ðŸ“š precedes anything}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Markov model of language toy example \n",
    "- Work through this example on your own.\n",
    "- Consider this tiny corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'rose', 'is', 'a', 'rose', 'is', 'a', 'rose', 'a', 'rose', '.', 'a']\n"
     ]
    }
   ],
   "source": [
    "toy_corpus = \"a rose is a rose is a rose a rose.\"\n",
    "toy_corpus_tokens = nltk.word_tokenize(toy_corpus.lower())\n",
    "circ_corpus = toy_corpus_tokens + toy_corpus_tokens[:1]\n",
    "print(circ_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- What are the states in this model?\n",
    "    - Unique words in the corpus: {., a, is, rose}\n",
    "\n",
    "> We make it circular so that we have some non-zero transition from the last word in the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'rose', 'is', 'a', 'rose', 'is', 'a', 'rose', 'a', 'rose', '.', 'a']\n"
     ]
    }
   ],
   "source": [
    "toy_corpus = \"a rose is a rose is a rose a rose.\"\n",
    "toy_corpus_tokens = nltk.word_tokenize(toy_corpus.lower())\n",
    "circ_corpus = toy_corpus_tokens + toy_corpus_tokens[:1]\n",
    "print(circ_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- What are the states in this model?\n",
    "    - Unique words in the corpus: {., a, is, rose}\n",
    "\n",
    "> We make it circular so that we have some non-zero transition from the last word in the sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\pi_0$ of the toy corpus\n",
    "- Calculate the initial probability distribution $\\pi_0$.\n",
    "    - For all states (unique words) $w_i$, compute $\\frac{\\text{number of times a document starts with } w_i}{n} $   \n",
    "- Here we only have one sentence which starts with word \"a\". So $\\pi_0 = \\begin{bmatrix} 0.0 & 1.0 & 0.0 & 0.0\\end{bmatrix}$\n",
    "\n",
    "> In NLP, _document_ is a general terms used for sentences or small snippets of texts. For example a text message could be referred to as a document.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transition matrix of the toy corpus\n",
    "- Calculate transition probabilities for all state combinations $w_i$ and $w_j$\n",
    "    - $\\frac{\\text{number of times } w_i \\text{ precedes } w_j}{\\text{number of times } w_i \\text{ precedes anything}}$ \n",
    "    \n",
    "- Let's calculate the transition matrix.  \n",
    "    - Calculate transition probabilities for all state combinations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How often a word $w_i$ occurs before $w_j$?\n",
    "\n",
    "- Code that you used in [512 lab1](https://github.ubc.ca/MDS-2021-22/DSCI_512_alg-data-struct_students/blob/master/solutions/lab1/lab1.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rose</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rose</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rose   is    a    .\n",
       "a      4.0  0.0  0.0  0.0\n",
       "rose   0.0  2.0  1.0  1.0\n",
       "is     0.0  0.0  2.0  0.0\n",
       ".      0.0  0.0  1.0  0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1\n",
    "frequencies = defaultdict(Counter)\n",
    "for i in range(len(circ_corpus) - n):\n",
    "    frequencies[circ_corpus[i : i + n][0]][circ_corpus[i + n]] += 1\n",
    "frequencies\n",
    "\n",
    "freq_df = pd.DataFrame(frequencies).transpose()\n",
    "freq_df = freq_df.fillna(0)\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'a': Counter({'rose': 4}),\n",
       "             'rose': Counter({'is': 2, 'a': 1, '.': 1}),\n",
       "             'is': Counter({'a': 2}),\n",
       "             '.': Counter({'a': 1})})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rose</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rose</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rose   is    a    .\n",
       "a      4.0  0.0  0.0  0.0\n",
       "rose   0.0  2.0  1.0  1.0\n",
       "is     0.0  0.0  2.0  0.0\n",
       ".      0.0  0.0  1.0  0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1\n",
    "frequencies = defaultdict(Counter)\n",
    "for i in range(len(circ_corpus) - n):\n",
    "    frequencies[circ_corpus[i : i + n][0]][circ_corpus[i + n]] += 1\n",
    "frequencies\n",
    "\n",
    "freq_df = pd.DataFrame(frequencies).transpose()\n",
    "freq_df = freq_df.fillna(0)\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'a': Counter({'rose': 4}),\n",
       "             'rose': Counter({'is': 2, 'a': 1, '.': 1}),\n",
       "             'is': Counter({'a': 2}),\n",
       "             '.': Counter({'a': 1})})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How often a word $w_i$ occurs before anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Let's normalize the probabilities. \n",
    "    - Divide element in each row by the summation of the elements in the row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rose</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rose</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rose   is     a     .\n",
       "a      1.0  0.0  0.00  0.00\n",
       "rose   0.0  0.5  0.25  0.25\n",
       "is     0.0  0.0  1.00  0.00\n",
       ".      0.0  0.0  1.00  0.00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_df = freq_df.div(freq_df.sum(axis=1), axis=0)\n",
    "trans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This is our transition matrix for our tiny corpus! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov models of language "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Let's normalize the probabilities. \n",
    "    - Divide element in each row by the summation of the elements in the row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rose</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rose</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rose   is     a     .\n",
       "a      1.0  0.0  0.00  0.00\n",
       "rose   0.0  0.5  0.25  0.25\n",
       "is     0.0  0.0  1.00  0.00\n",
       ".      0.0  0.0  1.00  0.00"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_df = freq_df.div(freq_df.sum(axis=1), axis=0)\n",
    "trans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This is our transition matrix for our tiny corpus! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a language model? \n",
    "\n",
    "A model that computes the probability of a sequence of words (or characters) or the probability of an upcoming word (or character) is called a **language model**.\n",
    "\n",
    "![](img/voice-assistant-ex.png)\n",
    "<!-- <img src=\"img/voice-assistant-ex.png\" height=\"1400\" width=\"1400\"> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "### What is a language model? \n",
    "A model that computes the probability of a sequence of words (or characters) or the probability of an upcoming word (or character) is called a **language model**.\n",
    "\n",
    "- Compute the probability of a sentence or a sequence of words.\n",
    "    - $P(w_1, w_2,\\dots,w_t)$\n",
    "    - P(I have read this book) > P(eye have red this book)\n",
    "\n",
    "- A related task: What's the probability of an upcoming word? \n",
    "    - $P(w_t|w_1,w_2,\\dots,w_{t-1})$ \n",
    "    - P(book | read this) > P(book | red this)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Language modeling: Why should we care?\n",
    "\n",
    "Powerful idea in NLP and helps in many tasks.\n",
    "- Machine translation \n",
    "    * P(In the age of data algorithms have the answer) > P(the age data of in algorithms answer the have)\n",
    "- Spelling correction\n",
    "    * My office is a 20  <span style=\"color:red\">minuet</span> bike ride from my home.  \n",
    "        * P(20 <span style=\"color:blue\">minute</span> bike ride from my home) > P(20 <span style=\"color:red\">minuet</span> bike ride from my home)\n",
    "- Speech recognition \n",
    "    * P(<span style=\"color:blue\">I read</span> a book) > P(<span style=\"color:red\">Eye red</span> a book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A simplest model of language is a Markov model of language! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A naive way to calculate probability of a sentence\n",
    "\n",
    "- Calculate probability of a sequence by applying the chain rule.  \n",
    "- Example: Suppose we want to calculate the probability of the following sequence of words: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(\\text{there is a crack in everything that 's how the light gets in}) \\\\\n",
    "=& P(\\text{there}) \\times P(\\text{is}\\mid \\text{there})\\\\ \n",
    "                                              & \\times P(\\text{a} \\mid \\text{there is}) \\times P(\\text{crack}\\mid \\text{there is a})\\\\\n",
    "                                              & \\times P(\\text{in}\\mid \\text{there is a crack})\\\\\n",
    "                                              & \\times P(\\text{everything} \\mid \\text{there is a crack in}) \\\\\n",
    "                                              & \\times P(\\text{that} \\mid \\text{there is a crack in everything}) \\\\                                              \n",
    "                                              & \\dots \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$P(\\text{that} \\mid \\text{there is a crack in everything}) = \\frac{Count(\\text{there is a crack in everything that})}{Count(\\text{there is a crack in everything})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem with the naive approach \n",
    "\n",
    "$$P(\\text{that} \\mid \\text{there is a crack in everything}) = \\frac{Count(\\text{there is a crack in everything that})}{Count(\\text{there is a crack in everything})} $$\n",
    "\n",
    "- How often the exact same long sequences of words would occur in text? For example, how often the sequence \"there is a crack in everything\" is likely to occur in your data? \n",
    "- The counts will be tiny and the model will be very sparse and specific. \n",
    "- <span style=\"color:red\">BAD IDEA!!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Markov model of language\n",
    "\n",
    "**Markov assumption: The future is conditionally independent of the past given present**\n",
    "\n",
    "![](img/bigram-ex.png)\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/bigram-ex.png\" height=\"500\" width=\"500\"> -->\n",
    "<!-- </center> -->\n",
    "\n",
    "$$\n",
    "P(\\text{everything} \\mid \\text{a crack in}) \\approx P(\\text{everything}\\mid\\text{in})\n",
    "$$\n",
    "\n",
    "- How do we estimate the probabilities? \n",
    "$$P(\\text{everything} \\mid\\text{in}) = \\frac{Count(\\text{in everything})}{Count(\\text{in \\{any word\\}})}$$\n",
    "\n",
    "- The model would be more generalizable now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### n-gram language model\n",
    "\n",
    "- In NLP, a Markov model of language is also referred to as **an n-gram language model**. \n",
    "- So far we have been talking about approximating the conditional probability $P(s_{t+1} \\mid s_{1}s_{2}\\dots s_{t})$ using only the current state $P(s_{t+1} \\mid s_{t})$. \n",
    "- So we have been talking about n-gram models where $n=1$, i.e., we only consider the current state in predicting the future.    \n",
    "- Such a model is referred to as a **2-gram (bigram) language model**, because in such a model, we only consider 2 state sequences at a time, the current state to predict the next state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try this out on a toy corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The birds they sang\n",
      "At the break of day\n",
      "Start again\n",
      "I heard them say\n",
      "Don't dwell on what\n",
      "Has passed away\n",
      "Or what is yet to be\n",
      "Yeah the wars they will\n",
      "Be fought again\n",
      "The holy dove\n",
      "She will be caught again\n",
      "Bought and sold\n",
      "And bought again\n",
      "The dove is never free\n",
      "Ring the bells (ring the bells) that still can ring\n",
      "Forget your perfect offering\n",
      "There is a crack in everything (there is a crack in everything)\n",
      "That's how the light gets in\n",
      "We asked for signs\n",
      "The signs were sent\n",
      "The birth betrayed\n",
      "The marriage spent\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toy_data = open(\"data/cohen_poem.txt\")\n",
    "toy_corpus = toy_data.read()\n",
    "print(toy_corpus[0:512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's calculate word co-occurrence frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birds</th>\n",
       "      <th>break</th>\n",
       "      <th>wars</th>\n",
       "      <th>holy</th>\n",
       "      <th>dove</th>\n",
       "      <th>bells</th>\n",
       "      <th>light</th>\n",
       "      <th>signs</th>\n",
       "      <th>birth</th>\n",
       "      <th>marriage</th>\n",
       "      <th>...</th>\n",
       "      <th>out</th>\n",
       "      <th>loud</th>\n",
       "      <th>but</th>\n",
       "      <th>like</th>\n",
       "      <th>summoned</th>\n",
       "      <th>up</th>\n",
       "      <th>going</th>\n",
       "      <th>from</th>\n",
       "      <th>me</th>\n",
       "      <th>wo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birds</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sang</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refugee</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows Ã— 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         birds  break  wars  holy  dove  bells  light  signs  birth  marriage  \\\n",
       "the        1.0    1.0   1.0   1.0   1.0    6.0    6.0    1.0    1.0       1.0   \n",
       "birds      0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "they       0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "sang       0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "at         0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "...        ...    ...   ...   ...   ...    ...    ...    ...    ...       ...   \n",
       "heart      0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "love       0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "come       0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "like       0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "refugee    0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "\n",
       "         ...  out  loud  but  like  summoned   up  going  from   me   wo  \n",
       "the      ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "birds    ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "they     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "sang     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "at       ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "...      ...  ...   ...  ...   ...       ...  ...    ...   ...  ...  ...  \n",
       "heart    ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "love     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "come     ...  0.0   0.0  1.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "like     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "refugee  ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "\n",
       "[115 rows x 115 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_corpus_tokens = nltk.word_tokenize(toy_corpus.lower())\n",
    "\n",
    "frequencies = defaultdict(Counter)\n",
    "for i in range(len(toy_corpus_tokens) - 1):\n",
    "    frequencies[toy_corpus_tokens[i : i + 1][0]][toy_corpus_tokens[i + 1]] += 1\n",
    "\n",
    "freq_df = pd.DataFrame(frequencies).transpose()\n",
    "freq_df = freq_df.fillna(0)\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's calculate the transition matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birds</th>\n",
       "      <th>break</th>\n",
       "      <th>wars</th>\n",
       "      <th>holy</th>\n",
       "      <th>dove</th>\n",
       "      <th>bells</th>\n",
       "      <th>light</th>\n",
       "      <th>signs</th>\n",
       "      <th>birth</th>\n",
       "      <th>marriage</th>\n",
       "      <th>...</th>\n",
       "      <th>out</th>\n",
       "      <th>loud</th>\n",
       "      <th>but</th>\n",
       "      <th>like</th>\n",
       "      <th>summoned</th>\n",
       "      <th>up</th>\n",
       "      <th>going</th>\n",
       "      <th>from</th>\n",
       "      <th>me</th>\n",
       "      <th>wo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birds</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sang</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refugee</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows Ã— 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         birds  break  wars  holy  dove  bells  light  signs  birth  marriage  \\\n",
       "the       0.04   0.04  0.04  0.04  0.04   0.24   0.24   0.04   0.04      0.04   \n",
       "birds     0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "they      0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "sang      0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "at        0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "...        ...    ...   ...   ...   ...    ...    ...    ...    ...       ...   \n",
       "heart     0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "love      0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "come      0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "like      0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "refugee   0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "\n",
       "         ...  out  loud  but  like  summoned   up  going  from   me   wo  \n",
       "the      ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "birds    ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "they     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "sang     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "at       ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "...      ...  ...   ...  ...   ...       ...  ...    ...   ...  ...  ...  \n",
       "heart    ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "love     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "come     ...  0.0   0.0  1.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "like     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "refugee  ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "\n",
       "[115 rows x 115 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_df = freq_df.div(freq_df.sum(axis=1), axis=0)\n",
    "trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probability P(everything | in) =  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"Conditional probability P(everything | in) = \", trans_df.loc[\"in\"][\"everything\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### State space\n",
    "\n",
    "- The states in our model are unique words in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: [\"'re\" \"'s\" \"'ve\" '(' ')' ',' 'a' 'add' 'again' 'all' 'and' 'asked' 'at'\n",
      " 'away' 'be' 'bells' 'betrayed' 'birds' 'birth' 'bought' 'break' 'but'\n",
      " 'ca' 'can' 'caught' 'come' 'crack' 'crowd' 'day' 'do' 'dove' 'drum'\n",
      " 'dwell' 'every' 'everything' 'for' 'forget' 'fought' 'free' 'from' 'gets'\n",
      " 'going' 'government' 'has' 'have' 'hear' 'heard' 'heart' 'high' 'holy'\n",
      " 'how' 'i' 'in' 'is' 'killers' 'lawless' 'light' 'like' 'loud' 'love'\n",
      " 'march' 'marriage' 'me' 'more' \"n't\" 'never' 'no' 'of' 'offering' 'on'\n",
      " 'or' 'out' 'parts' 'passed' 'perfect' 'places' 'prayers' 'refugee' 'ring'\n",
      " 'run' 'sang' 'say' 'see' 'sent' 'she' 'signs' 'sold' 'spent' 'start'\n",
      " 'still' 'strike' 'sum' 'summoned' 'that' 'the' 'their' 'them' 'there'\n",
      " 'they' 'thundercloud' 'to' 'up' 'wars' 'we' 'were' 'what' 'while'\n",
      " 'widowhood' 'will' 'with' 'wo' 'yeah' 'yet' 'you' 'your']\n",
      "Number of states: 115\n"
     ]
    }
   ],
   "source": [
    "states = np.unique(list(toy_corpus_tokens))\n",
    "print(\"States:\", states)\n",
    "S = len(states)\n",
    "print(\"Number of states:\", S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text generation using Markov models of languaage \n",
    "\n",
    "- How can we predict next word given a sequence of words? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE GENERATED SEQUENCE:  in ring forget your perfect offering there is a crack , a crack in everything ) that lawless crowd while the light gets in everything ) that 's how the bells that still can strike up a crack in that 's how the light gets in everything ( there is\n"
     ]
    }
   ],
   "source": [
    "seed = \"in\"\n",
    "seq_len = 50\n",
    "seq = \"\"\n",
    "word = seed\n",
    "for i in range(seq_len):\n",
    "    seq += \" \" + word\n",
    "    next_word = npr.choice(\n",
    "        trans_df.columns.tolist(),\n",
    "        p=trans_df.loc[\n",
    "            word,\n",
    "        ].values.flatten(),\n",
    "    )\n",
    "    word = next_word\n",
    "print(\"THE GENERATED SEQUENCE:\", seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In practice, the corpus (dataset) is huge. For example, the full Wikipedia or the text available on the entire Internet, or all the New York Times articles from the last 20 years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extending the bigram model\n",
    "\n",
    "- So far we have been talking about bigram models where we only consider the current word when predicting the next word. \n",
    "- If we want to predict future accurately, it's probably a good idea to use more history. \n",
    "- Can we generalize bigram language model ($n=1$) so that we can incorporate more history in the model ($n \\gt 1$). \n",
    "\n",
    "![](img/bigram-ex.png)\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/bigram-ex.png\" height=\"500\" width=\"500\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extending the bigram model\n",
    "- One way to incorporate more history is by extending the definition of a state. \n",
    "    - Instead of defining state space as unique words in the corpus, we can define it as unique two-word (trigram), three-word (4-gram), ($n-1$)-word sequences of the unique words in the corpus.\n",
    "\n",
    "- Trigram language model ($n=2$):  \n",
    "$$\n",
    "P(\\text{everything} \\mid \\text{there is a crack in}) \\approx P(\\text{everything} \\mid \\text{crack in})\n",
    "$$\n",
    "\n",
    "![](img/trigram-ex.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/trigram-ex.png\" height=\"500\" width=\"500\"> -->\n",
    "<!-- </center> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Considering more history \n",
    "\n",
    "- When we consider three state sequences at a time (2 previous states as history), the model is a **trigram (3-gram) language model**.\n",
    "- When we consider $n-1$ states as history, it's an **n-gram language model**. \n",
    "\n",
    "- Example: trigrams or four-gram language model\n",
    "    - Trigram language model\n",
    "$$\n",
    "P(\\text{everything} \\mid \\text{there is a crack in}) \\approx P(\\text{everything} \\mid \\text{crack in})\n",
    "$$\n",
    "    - Four-gram language model\n",
    "$$\n",
    "P(\\text{everything} \\mid \\text{there is a crack in}) \\approx P(\\text{everything} \\mid \\text{a crack in})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Language model example ($n=2$)\n",
    "\n",
    "- The state space would be all 2-word combinations of unique words in the corpus. \n",
    "    - What would be the size of the state space in our toy corpus? \n",
    "- Not all transitions would be valid transitions. \n",
    "    - Example: _a crack_ to _in everything_ is not a valid transition\n",
    "- Some example states with valid transitions could be: \n",
    "![](img/trigram-ex.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/trigram-ex.png\" height=\"500\" width=\"500\"> -->\n",
    "<!-- </center> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Language model example ($n=3$)\n",
    "- The state space would be all 3-word combinations of unique words in the corpus. \n",
    "    - What would be the size of the state space in our toy corpus?  \n",
    "- Some example states with valid transitions could be: \n",
    "\n",
    "![](img/4-gram-ex.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/4-gram-ex.png\" height=\"500\" width=\"500\"> -->\n",
    "<!-- </center> -->\n",
    "\n",
    "- Now we are able to incorporate more history, without really changing the math of Markov models! \n",
    "- We can calculate probability of sequences, predict the state at a given time step, or calculate the stationary distribution the same way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Are n-grams a good model of language?\n",
    "\n",
    "- In many cases, we can get by with ngram models. \n",
    "- But in general, is it a good assumption that the next word that I utter will be dependent on the last 3 words or 4 words?\n",
    "\n",
    "<blockquote>\n",
    "    The computer I was talking about yesterday when we were having dinner __.     \n",
    "</blockquote>    \n",
    "\n",
    "- Language has long-distance dependencies.  \n",
    "- We can extend it to $3$-grams, $4$-grams, $5$-grams. But then there is sparsity problem. \n",
    "- Also, ngram models have huge RAM requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Language models with word embeddings\n",
    "\n",
    "- N-gram models are great but we are representing context as the exact word.\n",
    "- Suppose in your training data you have the sequence \"feed the cat\" but you do not have the sequence \"feed the dog\".\n",
    "\n",
    "<blockquote>\n",
    "I have to make sure to feed the cat.\n",
    "</blockquote>\n",
    "\n",
    "- Trigram model: $P(\\text{dog} \\mid \\text{feed the}) = 0$\n",
    "- If we represent words with word embeddings instead, we will be able to generalize to dog even if we haven't seen it in the corpus.\n",
    "- We'll come back to this when we learn about Recurrent Neural Networks (RNNs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### (Optional) [Google n-gram viewer](https://books.google.com/ngrams)\n",
    " \n",
    "- All Our N-gram are Belong to You\n",
    "    - https://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-toyou.html\n",
    "\n",
    "<blockquote>\n",
    "Here at Google Research we have been using word n-gram models for a variety\n",
    "of R&D projects, such as statistical machine translation, speech recognition,\n",
    "spelling correction, entity detection, information extraction, and others.\n",
    "That's why we decided to share this enormous dataset with everyone. We\n",
    "processed 1,024,908,267,229 words of running text and are publishing the\n",
    "counts for all 1,176,470,663 five-word sequences that appear at least 40\n",
    "times. There are 13,588,391 unique words, after discarding words that appear\n",
    "less than 200 times.â€\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kvarada/miniconda3/envs/575/lib/python3.10/site-packages/IPython/core/display.py:431: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://books.google.com/ngrams/ width=1000 height=800></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://books.google.com/ngrams/\"\n",
    "HTML(\"<iframe src=%s width=1000 height=800></iframe>\" % url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## â“â“ Questions for you\n",
    "\n",
    "iClicker cloud join link: https://join.iclicker.com/4QVT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) Building a transition matrix for a Markov model means calculating the proportion of how often sequences of states occur in your data. \n",
    "- (B) In our setup, when n=3, each state has 3 letters. So given a three letter sequence, we predict the next three letter sequence. \n",
    "- (C) Suppose you have two corpora from completely different domains. You build two word-based n-gram models one for each corpus. The stationary distribution would be the same for both n-gram models.\n",
    "- (D) For $V=10$ and $n=4$ in our n-gram model setup, the dimension of the transition matrix is $10 \\times 10$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Questions for class discussion\n",
    "\n",
    "- In our setup, n=1 means a bigram model. I'm not sure if our implementation would work with n=0. But think about what does it mean and how would you generate text when n=0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Markov models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Markovâ€™s own application of his chains\n",
    "- Markov studied the transitions between vowels and consonants in a sequence of 20,000 letters in A. S. Pushkin's poem _Eugeny Onegin_. \n",
    "\n",
    " $S = \\{\\text{vowel, consonant}\\}, T =\n",
    "    \\begin{bmatrix}\n",
    "            0.128 & 0.872\\\\ \n",
    "            0.663 & 0.337\\\\\n",
    "    \\end{bmatrix}\n",
    "    $\n",
    "    \n",
    "|               | vowel     | consonant |\n",
    "| ------------- |:---------:| -----:|\n",
    "| vowel         | 0.128       | 0.872   |\n",
    "| consonant     | 0.663      | 0.337   |   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stationary distribution in the context of text \n",
    "\n",
    "- He also gave the stationary distribution for vowels and consonants: $\\pi = \\begin{bmatrix}0.432 & 0.568\\end{bmatrix}$       \n",
    "\n",
    "- In the context of text, stationary distribution is calculated as how often each state occurs in the corpus. \n",
    "- So stationary distribution in this case can be calculated as: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\text{# vowel occurrences}}{\\text{total number of letters}} & \\frac{\\text{# consonant occurrences} }{\\text{total number of letters}}\\\\\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's check whether we get $\\pi T = \\pi$ with this stationary distribution.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = np.array([0.432, 0.568])\n",
    "T = np.array([[0.128, 0.872], [0.663, 0.337]])\n",
    "np.allclose(pi @ T, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43188 0.56812]\n",
      "[0.4319442 0.5680558]\n",
      "[0.43190985 0.56809015]\n"
     ]
    }
   ],
   "source": [
    "# Markov's Pushkin Onegin consonant vowel probabilities\n",
    "print(pi @ T)\n",
    "print(pi @ np.linalg.matrix_power(T, 2))\n",
    "print(pi @ np.linalg.matrix_power(T, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The state probabilities are not quite the same but they are pretty close.\n",
    "- Note that the stationary distribution was calculated **by hand** by Markov and probably he rounded off the probabilities after 3 decimal places. (Tedious calculation!!) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Markovâ€™s own application of his chains\n",
    "\n",
    "- Markov also studied the sequence of 100,000 letters in S. T. Aksakov's novel \"The Childhood of Bagrov, the Grandson\".\n",
    "    * $S = \\{\\text{vowel, consonant}\\}$ \n",
    "    * \n",
    "    $ T = \n",
    "    \\begin{bmatrix}\n",
    "    0.552 & 0.448\\\\\n",
    "    0.365 & 0.635\\\\\n",
    "    \\end{bmatrix}\n",
    "    $\n",
    "\n",
    "|               | vowel     | consonant |\n",
    "| ------------- |:---------:| -----:|\n",
    "| vowel         | 0.552       | 0.448   |\n",
    "| consonant     | 0.365       | 0.635   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- He gave the stationary distribution for vowels and consonants.\n",
    "    * $\\pi = [0.449,0.551]$ \n",
    "- Stationary distribution in this case can be calculated as: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\text{# vowels}}{\\text{total number of letters}} & \\frac{\\text{# consonants}}{\\text{total number of letters}}\\\\\n",
    "\\end{bmatrix} \n",
    "$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = np.array([0.449, 0.551])\n",
    "T = np.array([[0.552, 0.448], [0.365, 0.635]])\n",
    "np.allclose(pi @ T, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.448963 0.551037]\n",
      "[0.44895608 0.55104392]\n"
     ]
    }
   ],
   "source": [
    "# Markov's Pushkin Onegin consonant vowel probabilities\n",
    "print(pi @ T)\n",
    "print(pi @ np.linalg.matrix_power(T, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Again, the state probabilities are not quite the same but they are pretty close. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PageRank "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- One of the algorithms used by Google Search to rank web pages in their search engine results.\n",
    "- Graph-based ranking algorithm, which assigns a rank to a webpage.\n",
    "- The rank indicates a relative score of the page's importance and authority.\n",
    "- Intuition\n",
    "    - Important webpages are linked from other important webpages.\n",
    "    - Don't just look at the number of links coming to a webpage but consider who the links are coming from. \n",
    "\n",
    "![](img/wiki_page_rank.jpg)\n",
    "<!-- <center>     -->\n",
    "<!-- <img src=\"img/wiki_page_rank.jpg\" height=\"400\" width=\"400\">  -->\n",
    "<!-- </center> -->\n",
    "\n",
    "[Credit](https://en.wikipedia.org/wiki/PageRank#/media/File:PageRanks-Example.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### PageRank: scoring\n",
    "\n",
    "- Imagine a browser doing a random walk \n",
    "    - At time t=0, start at a random webpage.\n",
    "    - At time t=1, follow a random link on the current page.\n",
    "    - At time t=2, follow a random link on the current page. \n",
    "    \n",
    "- Intuition\n",
    "    - In the \"steady state\" each page has a long-term visit rate, which is the page's score (rank). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PageRank as a Markov chain\n",
    "\n",
    "- A state is a web page.\n",
    "- Transition probabilities represent probabilities of moving from one page to another.\n",
    "- We derive these from the adjacency matrix of the web graph\n",
    "    - Adjacency matrix $M$ is a $n \\times n$ matrix, if $n$ is the number of states (web pages)\n",
    "    - $M_{ij} = 1$ if there is a hyperlink from page $i$ to page $j$.   \n",
    "- (Optional) If you want to know more details, check out [AppendixA-PageRank](AppendixA-PageRank.ipynb).    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calculate page rank: power iteration method\n",
    "\n",
    "- Start with a random initial probability distribution $\\pi_0$.\n",
    "- Multiply $\\pi_0$ by powers of the transition matrix $T$ until the product looks stable.  \n",
    "    - After one step, we are at $\\pi T$\n",
    "    - After two steps, we are at $\\pi T^2$\n",
    "    - After three steps, we are at $\\pi T^3$\n",
    "    - Eventually (for a large $k$), $\\pi T^k$ we get a stationary distribution.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modern ranking methods are more advanced:\n",
    "\n",
    "- Guarding against methods that exploit algorithm.\n",
    "- Removing offensive/illegal content.\n",
    "- Supervised and personalized ranking methods.\n",
    "- Take into account that you often only care about top rankings.\n",
    "- Also work on diversity of rankings:\n",
    "    - E.g., divide objects into sub-topics and do weighted \"covering\" of topics.\n",
    "- Persistence/freshness as in recommender systems (news articles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic text preprocessing [[video](https://www.youtube.com/watch?v=7W5Q8gzNPBc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introduction \n",
    "- Why do we need preprocessing?\n",
    "    - Text data is unstructured and messy. \n",
    "    - We need to \"normalize\" it before we do anything interesting with it. \n",
    "- Example:     \n",
    "    - **Lemma**: Same stem, same part-of-speech, roughly the same meaning\n",
    "        - Vancouver's &rarr; Vancouver\n",
    "        - computers &rarr; computer \n",
    "        - rising &rarr; rise, rose, rises    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "- Sentence segmentation\n",
    "    - Split text into sentences\n",
    "- Word tokenization \n",
    "    - Split sentences into words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization: sentence segmentation\n",
    "\n",
    "<blockquote>\n",
    "MDS is a Master's program at UBC in British Columbia. MDS teaching team is truly multicultural!! Dr. Beuzen did his Ph.D. in Australia. Dr. Timbers, Dr. Ostblom, Dr. RodrÃ­guez-Arelis, and Dr. Kolhatkar did theirs in Canada. Dr. George did his in Scotland. Dr. Gelbart did his PhD in the U.S.\n",
    "</blockquote>\n",
    "\n",
    "- How many sentences are there in this text? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"MDS is a Master's program at UBC in British Columbia\", ' MDS teaching team is truly multicultural!! Dr', ' Beuzen did his Ph', 'D', ' in Australia', ' Dr', ' Timbers, Dr', ' Ostblom, Dr', ' RodrÃ­guez-Arelis, and Dr', ' Kolhatkar did theirs in Canada', ' Dr', ' George did his in Scotland', ' Dr', ' Gelbart did his PhD in the U', 'S', '']\n"
     ]
    }
   ],
   "source": [
    "### Let's do sentence segmentation on \".\"\n",
    "text = (\n",
    "    \"MDS is a Master's program at UBC in British Columbia. \"\n",
    "    \"MDS teaching team is truly multicultural!! \"\n",
    "    \"Dr. Beuzen did his Ph.D. in Australia. \"\n",
    "    \"Dr. Timbers, Dr. Ostblom, Dr. RodrÃ­guez-Arelis, and Dr. Kolhatkar did theirs in Canada. \"\n",
    "    \"Dr. George did his in Scotland. \"\n",
    "    \"Dr. Gelbart did his PhD in the U.S.\"\n",
    ")\n",
    "\n",
    "print(text.split(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentence segmentation\n",
    "\n",
    "- In English, period (.) is quite ambiguous. (In Chinese, it is unambiguous.)\n",
    "    - Abbreviations like Dr., U.S., Inc.  \n",
    "    - Numbers like 60.44%, 0.98\n",
    "- ! and ? are relatively ambiguous.\n",
    "- How about writing regular expressions? \n",
    "- A common way is using off-the-shelf models for sentence segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"MDS is a Master's program at UBC in British Columbia.\", 'MDS teaching team is truly multicultural!!', 'Dr. Beuzen did his Ph.D. in Australia.', 'Dr. Timbers, Dr. Ostblom, Dr. RodrÃ­guez-Arelis, and Dr. Kolhatkar did theirs in Canada.', 'Dr. George did his in Scotland.', 'Dr. Gelbart did his PhD in the U.S.']\n"
     ]
    }
   ],
   "source": [
    "### Let's try to do sentence segmentation using nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenized = sent_tokenize(text)\n",
    "print(sent_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word tokenization\n",
    "\n",
    "<blockquote>\n",
    "MDS is a Master's program at UBC in British Columbia. \n",
    "</blockquote>\n",
    "\n",
    "- How many words are there in this sentence?  \n",
    "- Is whitespace a sufficient condition for a word boundary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word tokenization \n",
    "\n",
    "<blockquote>\n",
    "MDS is a Master's program at UBC in British Columbia. \n",
    "</blockquote>\n",
    "\n",
    "- What's our definition of a word?\n",
    "    - Should British Columbia be one word or two words? \n",
    "    - Should punctuation be considered a separate word?\n",
    "    - What about the punctuations in `U.S.`?\n",
    "    - What do we do with words like `Master's`?\n",
    "- This process of identifying word boundaries is referred to as **tokenization**.\n",
    "- You can use regex but better to do it with off-the-shelf ML models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting on whitespace:  [['MDS', 'is', 'a', \"Master's\", 'program', 'at', 'UBC', 'in', 'British', 'Columbia.'], ['MDS', 'teaching', 'team', 'is', 'truly', 'multicultural!!'], ['Dr.', 'Beuzen', 'did', 'his', 'Ph.D.', 'in', 'Australia.'], ['Dr.', 'Timbers,', 'Dr.', 'Ostblom,', 'Dr.', 'RodrÃ­guez-Arelis,', 'and', 'Dr.', 'Kolhatkar', 'did', 'theirs', 'in', 'Canada.'], ['Dr.', 'George', 'did', 'his', 'in', 'Scotland.'], ['Dr.', 'Gelbart', 'did', 'his', 'PhD', 'in', 'the', 'U.S.']]\n",
      "\n",
      "\n",
      "\n",
      "Tokenized:  [['MDS', 'is', 'a', 'Master', \"'s\", 'program', 'at', 'UBC', 'in', 'British', 'Columbia', '.'], ['MDS', 'teaching', 'team', 'is', 'truly', 'multicultural', '!', '!'], ['Dr.', 'Beuzen', 'did', 'his', 'Ph.D.', 'in', 'Australia', '.'], ['Dr.', 'Timbers', ',', 'Dr.', 'Ostblom', ',', 'Dr.', 'RodrÃ­guez-Arelis', ',', 'and', 'Dr.', 'Kolhatkar', 'did', 'theirs', 'in', 'Canada', '.'], ['Dr.', 'George', 'did', 'his', 'in', 'Scotland', '.'], ['Dr.', 'Gelbart', 'did', 'his', 'PhD', 'in', 'the', 'U.S', '.']]\n"
     ]
    }
   ],
   "source": [
    "### Let's do word segmentation on white spaces\n",
    "print(\"Splitting on whitespace: \", [sent.split() for sent in sent_tokenized])\n",
    "\n",
    "### Let's try to do word segmentation using nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenized = [word_tokenize(sent) for sent in sent_tokenized]\n",
    "# This is similar to the input format of word2vec algorithm\n",
    "print(\"\\n\\n\\nTokenized: \", word_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word segmentation \n",
    "\n",
    "For some languages you need much more sophisticated tokenizers. \n",
    "- For languages such as Chinese, there are no spaces between words.\n",
    "    - [jieba](https://github.com/fxsjy/jieba) is a popular tokenizer for Chinese. \n",
    "- German doesn't separate compound words.\n",
    "    * Example: _rindfleischetikettierungsÃ¼berwachungsaufgabenÃ¼bertragungsgesetz_\n",
    "    * (the law for the delegation of monitoring beef labeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types and tokens\n",
    "- Usually in NLP, we talk about \n",
    "    - **Type** an element in the vocabulary\n",
    "    - **Token** an instance of that type in running text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise for you \n",
    "\n",
    "<blockquote>    \n",
    "UBC is located in the beautiful province of British Columbia. It's very close \n",
    "to the U.S. border. You'll get to the USA border in about 45 mins by car.     \n",
    "</blockquote>  \n",
    "\n",
    "- Consider the example above. \n",
    "    - How many types? (task dependent)\n",
    "    - How many tokens? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other commonly used preprocessing steps\n",
    "\n",
    "- Punctuation and stopword removal\n",
    "- Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Punctuation and stopword removal\n",
    "\n",
    "- The most frequently occurring words in English are not very useful in many NLP tasks.\n",
    "    - Example: _the_ , _is_ , _a_ , and punctuation\n",
    "- Probably not very informative in many tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'out', 'few', 'with', 'had', 'shouldn', 'needn', 'after', 'themselves', \"haven't\", \"you'll\", 'ourselves', 'their', 'now', 'was', \"hasn't\", 'it', 'down', 'on', 'before', 'only', 'being', \"needn't\", 'should', 'didn', \"aren't\", 'those', 'are', 've', 'here', \"isn't\", 'until', \"it's\", 'of', 't', 'why', 'then', 'm', 'haven', 'during', 'and', 'doing', 'such', 'can', 'more', 'ma', 'so', 're', 'any', \"that'll\", 'mightn', 'having', 'this', \"mightn't\", 'doesn', 'who', 'that', 'him', 'while', 'whom', 'its', 'they', 'where', 'the', 'don', 'ain', 'up', 'his', 'myself', 'd', \"shouldn't\", 'himself', 'which', 'some', 'above', 'in', 'between', 'yourself', \"mustn't\", 'do', 'been', 'what', 'won', 'through', \"she's\", 'herself', \"should've\", 'hers', 'all', \"you've\", 'weren', 'them', 'does', 'into', \"shan't\", 'an', 'for', 'about', 'i', \"you'd\", 'aren', 'were', 'very', 'against', 'if', 'under', 's', \"you're\", \"wasn't\", \"don't\", 'from', 'over', 'you', \"won't\", \"didn't\", \"doesn't\", \"wouldn't\", 'couldn', 'at', 'shan', 'wouldn', 'these', 'further', 'your', 'by', 'there', 'but', 'or', 'our', 'below', 'each', 'just', 'll', 'too', 'my', 'her', 'he', 'wasn', 'mustn', 'because', 'y', 'ours', 'other', 'theirs', \"couldn't\", 'hasn', 'own', 'am', 'than', 'itself', 'as', 'hadn', 'a', 'both', 'again', 'me', 'we', 'isn', 'yourselves', 'most', 'will', 'yours', 'has', 'how', 'not', \"hadn't\", 'did', 'be', 'once', 'she', 'when', 'is', 'same', \"weren't\", 'no', 'off', 'to', 'have', 'nor', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# Let's use `nltk.stopwords`.\n",
    "# Add punctuations to the list.\n",
    "stop_words = list(set(stopwords.words(\"english\")))\n",
    "import string\n",
    "\n",
    "punctuation = string.punctuation\n",
    "stop_words += list(punctuation)\n",
    "# stop_words.extend(['``','`','br','\"',\"â€\", \"''\", \"'s\"])\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mds', 'master', \"'s\", 'program', 'ubc', 'british', 'columbia', 'mds', 'teaching', 'team', 'truly', 'multicultural', 'dr.', 'beuzen', 'ph.d.', 'australia', 'dr.', 'timbers', 'dr.', 'ostblom', 'dr.', 'rodrÃ­guez-arelis', 'dr.', 'kolhatkar', 'canada', 'dr.', 'george', 'scotland', 'dr.', 'gelbart', 'phd', 'u.s']\n"
     ]
    }
   ],
   "source": [
    "### Get rid of stop words\n",
    "preprocessed = []\n",
    "for sent in word_tokenized:\n",
    "    for token in sent:\n",
    "        token = token.lower()\n",
    "        if token not in stop_words:\n",
    "            preprocessed.append(token)\n",
    "print(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lemmatization \n",
    "\n",
    "- For many NLP tasks (e.g., web search) we want to ignore morphological differences between words\n",
    "    - Example: If your search term is \"studying for ML quiz\" you might want to include pages containing \"tips to study for an ML quiz\" or \"here is how I studied for my ML quiz\"\n",
    "- Lemmatization converts inflected forms into the base form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/kvarada/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/kvarada/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma of studying:  study\n",
      "Lemma of studied:  study\n"
     ]
    }
   ],
   "source": [
    "# nltk has a lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"Lemma of studying: \", lemmatizer.lemmatize(\"studying\", \"v\"))\n",
    "print(\"Lemma of studied: \", lemmatizer.lemmatize(\"studied\", \"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stemming\n",
    "\n",
    "- Has a similar purpose but it is a crude chopping of affixes \n",
    "    * _automates, automatic, automation_ all reduced to _automat_.\n",
    "- Usually these reduced forms (stems) are not actual words themselves.  \n",
    "- A popular stemming algorithm for English is PorterStemmer. \n",
    "- Beware that it can be aggressive sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming:  UBC is located in the beautiful province of British Columbia... It's very close to the U.S. border.\n",
      "\n",
      "\n",
      "After stemming:  ubc is locat in the beauti provinc of british columbia ... it 's veri close to the u.s. border .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "text = (\n",
    "    \"UBC is located in the beautiful province of British Columbia... \"\n",
    "    \"It's very close to the U.S. border.\"\n",
    ")\n",
    "ps = PorterStemmer()\n",
    "tokenized = word_tokenize(text)\n",
    "stemmed = [ps.stem(token) for token in tokenized]\n",
    "print(\"Before stemming: \", text)\n",
    "print(\"\\n\\nAfter stemming: \", \" \".join(stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other tools for preprocessing \n",
    "\n",
    "- We used [Natural Language Processing Toolkit (nltk)](https://www.nltk.org/) above\n",
    "    - You already have used it in 571 and 573  \n",
    "- Many available tools    \n",
    "- [spaCy](https://spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [spaCy](https://spacy.io/)\n",
    "\n",
    "- We already have used spaCy before in 573 and 563. \n",
    "- Industrial strength NLP library. \n",
    "- Lightweight, fast, and convenient to use. \n",
    "- spaCy does many things that we did above in one line of code! \n",
    "- Also has [multi-lingual](https://spacy.io/models/xx) support. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "text = (\n",
    "    \"MDS is a Master's program at UBC in British Columbia. \"\n",
    "    \"MDS teaching team is truly multicultural!! \"\n",
    "    \"Dr. Beuzen did his Ph.D. in Australia. \"\n",
    "    \"Dr. George did his in Scotland. \"\n",
    "    \"Dr. Timbers, Dr. Ostblom, Dr. RodrÃ­guez-Arelis, and Dr. Kolhatkar did theirs in Canada. \"\n",
    "    \"Dr. Gelbart did his PhD in the U.S.\"\n",
    ")\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:  [MDS, is, a, Master, 's, program, at, UBC, in, British, Columbia, ., MDS, teaching, team, is, truly, multicultural, !, !, Dr., Beuzen, did, his, Ph.D., in, Australia, ., Dr., George, did, his, in, Scotland, ., Dr., Timbers, ,, Dr., Ostblom, ,, Dr., RodrÃ­guez, -, Arelis, ,, and, Dr., Kolhatkar, did, theirs, in, Canada, ., Dr., Gelbart, did, his, PhD, in, the, U.S.]\n",
      "\n",
      "Lemmas:  ['mds', 'be', 'a', 'Master', \"'s\", 'program', 'at', 'UBC', 'in', 'British', 'Columbia', '.', 'mds', 'teaching', 'team', 'be', 'truly', 'multicultural', '!', '!', 'Dr.', 'Beuzen', 'do', 'his', 'ph.d.', 'in', 'Australia', '.', 'Dr.', 'George', 'do', 'his', 'in', 'Scotland', '.', 'Dr.', 'Timbers', ',', 'Dr.', 'Ostblom', ',', 'Dr.', 'RodrÃ­guez', '-', 'Arelis', ',', 'and', 'Dr.', 'Kolhatkar', 'do', 'theirs', 'in', 'Canada', '.', 'Dr.', 'Gelbart', 'do', 'his', 'phd', 'in', 'the', 'U.S.']\n",
      "\n",
      "POS:  ['NOUN', 'AUX', 'DET', 'PROPN', 'PART', 'NOUN', 'ADP', 'PROPN', 'ADP', 'PROPN', 'PROPN', 'PUNCT', 'NOUN', 'NOUN', 'NOUN', 'AUX', 'ADV', 'ADJ', 'PUNCT', 'PUNCT', 'PROPN', 'PROPN', 'VERB', 'PRON', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'PROPN', 'PROPN', 'VERB', 'PRON', 'ADP', 'PROPN', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'CCONJ', 'PROPN', 'PROPN', 'VERB', 'PRON', 'ADP', 'PROPN', 'PUNCT', 'PROPN', 'PROPN', 'VERB', 'PRON', 'NOUN', 'ADP', 'DET', 'PROPN']\n"
     ]
    }
   ],
   "source": [
    "# Accessing tokens\n",
    "tokens = [token for token in doc]\n",
    "print(\"\\nTokens: \", tokens)\n",
    "\n",
    "# Accessing lemma\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(\"\\nLemmas: \", lemmas)\n",
    "\n",
    "# Accessing pos\n",
    "pos = [token.pos_ for token in doc]\n",
    "print(\"\\nPOS: \", pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other typical NLP tasks \n",
    "In order to understand text, we usually are interested in extracting information from text. Some common tasks in NLP pipeline are: \n",
    "- Part of speech tagging\n",
    "    - Assigning part-of-speech tags to all words in a sentence.\n",
    "- Named entity recognition\n",
    "    - Labelling named â€œreal-worldâ€ objects, like persons, companies or locations.    \n",
    "- Coreference resolution\n",
    "    - Deciding whether two strings (e.g., UBC vs University of British Columbia) refer to the same entity\n",
    "- Dependency parsing\n",
    "    - Representing grammatical structure of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extracting named-entities using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    University of British Columbia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is located in the beautiful province of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    British Columbia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named entities:\n",
      " [('University of British Columbia', 'ORG'), ('British Columbia', 'GPE')]\n",
      "\n",
      "ORG means:  Companies, agencies, institutions, etc.\n",
      "GPE means:  Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(\n",
    "    \"University of British Columbia \"\n",
    "    \"is located in the beautiful \"\n",
    "    \"province of British Columbia.\"\n",
    ")\n",
    "displacy.render(doc, style=\"ent\")\n",
    "# Text and label of named entity span\n",
    "print(\"Named entities:\\n\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "print(\"\\nORG means: \", spacy.explain(\"ORG\"))\n",
    "print(\"GPE means: \", spacy.explain(\"GPE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dependency parsing using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"d105f63f7fe44238b62a6f13a5f638b7-0\" class=\"displacy\" width=\"575\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">like</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">cats</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d105f63f7fe44238b62a6f13a5f638b7-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d105f63f7fe44238b62a6f13a5f638b7-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d105f63f7fe44238b62a6f13a5f638b7-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d105f63f7fe44238b62a6f13a5f638b7-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"I like cats\")\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Many other things possible\n",
    "\n",
    "- A powerful tool \n",
    "- All my Capstone groups last year used this tool. \n",
    "- You can build your own rule-based searches. \n",
    "- You can also access word vectors using spaCy with bigger models. (Currently we are using `en_core_web_sm` model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## â“â“ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Discuss the following questions with your neighbours \n",
    "\n",
    "1. Why your text might become unreadable after stemming? \n",
    "2. What's the difference between sentence segmentation and word tokenization? Which step would you carry out first: sentence segmentation or word tokenization?\n",
    "3. Tokenize the following sentence and identify named entities in the sentence manually. Compare your annotations with what you get with spaCy. \n",
    "\n",
    "> The MadeUpOrg founder John Fakename lists his Point Grey penthouse for $15 million.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 2.3: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "1. Stemming carries out crude chopping of affixes and converts words to reduced forms called stems. Often these reduced forms are not actual words. For instance, in the example we saw, _located_ was reduced to _locat_ and _beautiful_ was reduced to _beauti_. So after applying stemming the text might become unreadable. \n",
    "2. Sentence segmentation is about identifying sentence boundaries and splitting text into sentences whereas word tokenization is about identifying word boundaries and splitting sentences into words. The general practice is to carry out sentence segmentation before word tokenization. \n",
    "3. Manual NER: \n",
    "The [MadeUpOrg ORGANIZATION] founder [John Fakename PERSON] lists his [POINT Grey LOCATION] penthouse for [$15 million MONEY] . \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The MadeUpOrg founder \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John Fakename\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " lists his \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Point Grey\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " penthouse for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $15 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(\n",
    "    \"The MadeUpOrg founder John Fakename lists his Point Grey penthouse for $15 million.\"\n",
    ")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy was not able to identify ORGANIZATION and LOCATION entities in the sentence.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final comments, summary, and reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary: Markov models \n",
    "\n",
    "- Markov models are the class of probabilistic models which assume that we can predict the probability of being in a particular state in future without looking too far into the past. \n",
    "- We looked at two applications of Markov models in language.\n",
    "    - N-gram language models\n",
    "    - PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary: Language models  \n",
    "\n",
    "- A model that computes the probability of a sequence of words (or characters) or the probability of an upcoming word (or character) is called a **language model**.\n",
    "- Language models are central to many NLP applications such as smart compose, spelling correction, machine translations, voice assistants. \n",
    "- Markov models are the simplest models of language. \n",
    "- They are also referred to as **n-gram models**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary: N-gram language models\n",
    "\n",
    "- We can build character-based or word-based n-gram models.\n",
    "- We build a bigram model of language by assuming unique words or characters as states. \n",
    "- We can extend a bigram model by extending the definition of a state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary: PageRank\n",
    "- Another application of Markov chains in language is the PageRank algorithm. \n",
    "    - The intuition is that important webpages are linked from other important webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Preprocessing is an important step when you deal with text data.\n",
    "- Some common preprocessing steps include:\n",
    "    - Sentence segmentation\n",
    "    - Word tokenization\n",
    "    - Lemmatization\n",
    "    - Stemming\n",
    "- Some common tasks in NLP pipeline are:\n",
    "    - POS tagging\n",
    "    - Named-entity recognition\n",
    "    - Coreference resolution\n",
    "    - Dependency parsing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "- [GPT-3 AI Automation](https://www.nytimes.com/2020/07/29/opinion/gpt-3-ai-automation.html)\n",
    "- [spaCy's Python for data science cheat sheet](http://datacamp-community-prod.s3.amazonaws.com/29aa28bf-570a-4965-8f54-d6a541ae4e06)\n",
    "\n",
    "- [Regular Expressions, Text Normalization, Edit Distance](https://web.stanford.edu/~jurafsky/slp3/2.pdf)\n",
    "- [Try preprocessing using unix shell](https://web.stanford.edu/class/cs124/lec/124-2020-UnixForPoets.pdf)\n",
    "- [Flair](https://github.com/flairNLP/flair) is another library with state-of-the-art NLP tools.  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
