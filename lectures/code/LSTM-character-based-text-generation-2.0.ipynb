{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvpAmw1VzuBo"
   },
   "source": [
    "# Character-based LSTM text generation using data generators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVFXo-LUJwwU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys, gc\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import string\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCkNF6u4Jwwb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5aj6vTwJwwh"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "I recommend that you run this notebook on the cloud using [Google colab](https://colab.research.google.com/notebooks/welcome.ipynb). This will allow you to train on a GPU and assess the benefits of training neural networks on GPUs.\n",
    "\n",
    "To do so, follow the steps below.\n",
    "\n",
    "- Go to [Google colab](https://colab.research.google.com/). \n",
    "- Make an account if you don't have one.\n",
    "- Select \"UPLOAD\" and upload this notebook itself.\n",
    "- Runtime --> change runtime type --> Select GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4JYtLYdJww9"
   },
   "outputs": [],
   "source": [
    "class DLTextGenerator():\n",
    "    def __init__(self, \n",
    "                 text, \n",
    "                 seq_length = 100,\n",
    "                 vocab_size = 100):\n",
    "        \"\"\"\n",
    "        Init method of the class DLTextGenerator\n",
    " \n",
    "        Parameters\n",
    "        -------------\n",
    "        text : str\n",
    "            the corpus as plain text \n",
    "        seq_length : int\n",
    "            sequence length for the RNN \n",
    "        vocab_size : int\n",
    "            maximum vocabulary size to be considered        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.seq_length = seq_length        \n",
    "        self.vocab_size = vocab_size        \n",
    "        char_count = Counter(text)\n",
    "        comn = char_count.most_common(self.vocab_size)\n",
    "        self.vocab = [letter for letter, count in comn]\n",
    "        self.text = text.translate({ord(c): None for c in char_count if c not in self.vocab}) \n",
    "        # (above) characters not in vocabulary are removed from the training string\n",
    "        #self.vocab = sorted(set(text))\n",
    "        self.n_vocab = len(self.vocab)\n",
    "        self.n_examples = len(self.text) - self.seq_length        \n",
    "        print ('{} total characters in text'.format(len(self.text)))\n",
    "        print ('{} unique characters'.format(self.n_vocab))\n",
    "        print ('{} sequence length'.format(self.seq_length))       \n",
    "        \n",
    "        self.char2idx = {u:i for i, u in enumerate(self.vocab)}\n",
    "        self.idx2char = np.array(self.vocab)\n",
    "        self.model = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Create X and y for text generation using text and \n",
    "        sequence length. \n",
    "        \n",
    "        Parameters\n",
    "        -------------\n",
    "        None\n",
    "        \n",
    "        Returns\n",
    "        -------------\n",
    "        X and y for the text generation tasl\n",
    "                \n",
    "        \"\"\"\n",
    "        data_X = np.zeros((self.n_examples, self.seq_length, \n",
    "                          self.n_vocab),dtype=bool)\n",
    "        data_y = np.zeros((self.n_examples, self.n_vocab))\n",
    "\n",
    "        for i in range(self.n_examples):\n",
    "            seq_in = self.text[i:i + self.seq_length]\n",
    "            char_out = self.text[i + self.seq_length]\n",
    "            for j, char in enumerate(seq_in):\n",
    "                data_X[i, j, self.char2idx[char]] = 1.0\n",
    "            data_y[i, self.char2idx[char_out]] = 1.0\n",
    "        print('Total examples: %d'%(len(data_X)))\n",
    "        print('Total examples: %d'%(len(data_y)))            \n",
    "        return data_X, data_y\n",
    "\n",
    "    def build_LSTM(self, layer_size=256, \n",
    "                   dropout_amount=0.5):\n",
    "        \"\"\"\n",
    "        Given layer_size and dropout_amount, build an LSTM network\n",
    "        using Keras and tensorflow and print summary of the model. \n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        layer_size : int\n",
    "          The number of units to be passed in the LSTM layer\n",
    "        dropout_amount : float\n",
    "          the dropout amount to be passed in the Dropout layer. \n",
    "\n",
    "        Return\n",
    "        -----------\n",
    "          None\n",
    "          print the summary of the model\n",
    "        \"\"\"\n",
    "\n",
    "        print('Building model...')\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(layer_size, input_shape=(self.seq_length, \n",
    "                                                     self.n_vocab), \n",
    "                                                     return_sequences=True))\n",
    "        self.model.add(Dropout(dropout_amount))\n",
    "        self.model.add(LSTM(layer_size, return_sequences=True))\n",
    "        self.model.add(Dropout(dropout_amount))\n",
    "        self.model.add(LSTM(layer_size))\n",
    "        self.model.add(Dropout(dropout_amount))\n",
    "        self.model.add(Dense(self.n_vocab, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', \n",
    "                           optimizer='adam', \n",
    "                           metrics=['accuracy'])    \n",
    "        print(self.model.summary())\n",
    "        \n",
    "\n",
    "    def data_generator(self, X, y, num_features, batch_size = 128):\n",
    "        \"\"\"        \n",
    "        Generates batches of vectorized texts for training/validation.\n",
    " \n",
    "        Parameters\n",
    "        -------------\n",
    "            x: np.matrix, feature matrix.\n",
    "            y: np.ndarray, labels.\n",
    "            num_features: int, number of features.\n",
    "            batch_size: int, number of samples per batch.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "            Yields feature and label data in batches.\n",
    "        Attribution: The code below is heavily based on the following code. \n",
    "        https://developers.google.com/machine-learning/guides/text-classification/appendix\n",
    "        \"\"\"\n",
    "        num_samples = X.shape[0]\n",
    "        num_batches = num_samples // batch_size\n",
    "        if num_samples % batch_size:\n",
    "            num_batches += 1\n",
    "\n",
    "        while 1:\n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = (i + 1) * batch_size\n",
    "                if end_idx > num_samples:\n",
    "                    end_idx = num_samples\n",
    "                X_batch = X[start_idx:end_idx]\n",
    "                y_batch = y[start_idx:end_idx]\n",
    "                yield X_batch, y_batch                \n",
    "                                \n",
    "    def fit(self, \n",
    "              X, y, \n",
    "              batch_size = 128, \n",
    "              epochs = 10, \n",
    "              checkpoint_dir='./training_checkpoints'): # Directory where the checkpoints will be saved\n",
    "       \"\"\"        \n",
    "        Given the parameters, train a deep learning model and save it.  \n",
    "        \n",
    "        Parameters\n",
    "        -------------\n",
    "        X : (list) \n",
    "          the X values\n",
    "        y : (list) \n",
    "          the y values\n",
    "        batch_size : (int) \n",
    "          the batch_size for the training\n",
    "        epochs : (int) \n",
    "          the number of epochs for training \n",
    "        checkpoint_dir : (str) the path to save the model        \n",
    "        \"\"\"        \n",
    "        # Name of the checkpoint files\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "        checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_prefix,\n",
    "            save_weights_only=True)\n",
    "        \n",
    "        training_generator = self.data_generator(X, y, self.n_vocab, batch_size)      \n",
    "        print(training_generator)\n",
    "        # fit the model\n",
    "        # Fit the model without data generator\n",
    "        # If you want to convince yourself why we need data generators, \n",
    "        # try calling this `fit` method with `X`, `y` instead of calling `fit`\n",
    "        # with the generator object.\n",
    "        #self.model.fit(X, y,  \n",
    "        #          epochs=epochs, \n",
    "        #          batch_size=128, \n",
    "        #          callbacks=[checkpoint_callback], \n",
    "        #          validation_split=0.20)\n",
    "\n",
    "        # Fit the model using data generator        \n",
    "        steps_per_epoch = X.shape[0] // batch_size\n",
    "        if X.shape[0] % batch_size:\n",
    "          steps_per_epoch += 1\n",
    "        print('Steps per epoch: ', steps_per_epoch)\n",
    "        history = self.model.fit_generator(\n",
    "                                    generator=training_generator,\n",
    "                                    steps_per_epoch=steps_per_epoch,\n",
    "                                    callbacks=[checkpoint_callback],\n",
    "                                    epochs=epochs,\n",
    "                                    verbose=2)  # Logs once per epoch.   \n",
    "                                         \n",
    "    def generate(self, seed, temperature=0.5, num_gen=1000, \n",
    "                 checkpoint_dir='./training_checkpoints'):\n",
    "        \"\"\"        \n",
    "        Given the parameters abd saved path for the model, generate text.  \n",
    "        \n",
    "        Parameters\n",
    "        -------------\n",
    "        seed : (str) \n",
    "            the seed for text generation\n",
    "        temperature : (float) \n",
    "            the temparature to generate the text\n",
    "        num_gen : (int) \n",
    "            the number of characters to generate\n",
    "        checkpoint_dir : (str) \n",
    "            the path where the model is saved\n",
    "        \"\"\"       \n",
    "    \n",
    "        result = ''\n",
    "        print('Loading model: ', tf.train.latest_checkpoint(checkpoint_dir))\n",
    "        self.model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))        \n",
    "        #for p in seed:\n",
    "        #    result += self.idx2char[np.argmax(p)]\n",
    "        #result += '\\n\\n'\n",
    "        print (\"Seed:\")\n",
    "        for p in seed:\n",
    "            sys.stdout.write(self.idx2char[np.argmax(p)])\n",
    "        print(\"\\n--------------------------------------------\\n\")\n",
    "        pattern = seed\n",
    "\n",
    "        pattern = seed\n",
    "\n",
    "        # generate characters\n",
    "        for i in range(num_gen):\n",
    "            prediction = self.model.predict(pattern[None], verbose=0)\n",
    "\n",
    "            probabilities = prediction.flatten()\n",
    "            if temperature != 1:\n",
    "                # maybe not exactly right but close enough, and probably exactly right\n",
    "                probabilities = probabilities**(1.0/temperature) \n",
    "                probabilities /= np.sum(probabilities)\n",
    "\n",
    "            index = np.random.choice(self.n_vocab, p=probabilities)\n",
    "                \n",
    "            #result += self.idx2char[index]            \n",
    "            #sys.stdout.flush()\n",
    "            result = self.idx2char[index]\n",
    "            sys.stdout.write(result)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            new_char_one_hot = np.zeros(self.n_vocab)\n",
    "            new_char_one_hot[index] = 1.0\n",
    "            pattern = np.append(pattern[1:], new_char_one_hot[None], axis=0)\n",
    "\n",
    "        print(\"\\nDone.\")\n",
    "        gc.collect() # http://stackoverflow.com/questions/40560795/tensorflow-attributeerror-nonetype-object-has-no-attribute-tf-deletestatus\n",
    "        #return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XzIvO8ltJwxB"
   },
   "outputs": [],
   "source": [
    "# This is the data we used in the lab \n",
    "data_url = 'https://raw.github.ubc.ca/MDS-2019-20/datasets/master/data/wiki1MB.txt?token=AAAANP3GW2AJKUV4F77Z6VS6UZG26'\n",
    "text = urlopen(data_url).read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "colab_type": "code",
    "id": "bsj8hy8gk-Sm",
    "outputId": "26b2af48-c419-4723-c9db-29d5f833fcee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993199 total characters in text\n",
      "100 unique characters\n",
      "100 sequence length\n",
      "Total examples: 993099\n",
      "Total examples: 993099\n",
      "Building model...\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_33 (LSTM)               (None, 100, 256)          365568    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 100, 256)          525312    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               25700     \n",
      "=================================================================\n",
      "Total params: 1,441,892\n",
      "Trainable params: 1,441,892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create LSTM text generator object\n",
    "lstm_text_generator = DLTextGenerator(text)\n",
    "\n",
    "# Prepare data \n",
    "X, y = lstm_text_generator.prepare_data()\n",
    "\n",
    "# Build the network \n",
    "lstm_text_generator.build_LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qPe9o-8gvmwU",
    "outputId": "ec11ee7b-7753-4008-fe9c-4f5185fc9ce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "nOWMoe3mJwxF",
    "outputId": "a17de6a3-26a5-4ac2-b699-5ddb47be6035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object DLTextGenerator.data_generator at 0x7f2ec5349f10>\n",
      "Steps per epoch:  7759\n",
      "Epoch 1/10\n",
      "7759/7759 - 924s - loss: 2.7020 - accuracy: 0.2733\n",
      "Epoch 2/10\n",
      "7759/7759 - 922s - loss: 2.0574 - accuracy: 0.4224\n",
      "Epoch 3/10\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "lstm_text_generator.fit(X,y, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tflTvcjLzuCB"
   },
   "source": [
    "## Generate text \n",
    "Now we can load the model and start generating text. You may also want to try other text of your choice to train the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "sZaujvNSUzXI",
    "outputId": "b66c4bdc-9ca2-410f-b67e-6026db760b71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model:  ./training_checkpoints/ckpt_10\n",
      "Seed:\n",
      "l/export-0.3/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.\n",
      "--------------------------------------------\n",
      "\n",
      "l6P.PvvUI8X-9X7PXS689I174TPmk9xbI1_4PP9-L8X_0_7s05P1P78r:P934_P_gT7P6U-:3iXi7_8lXggP1__T_v043XvPv69P_1f7PxX66PXo/7P486ff<x\"X5.h99X=5X_lfP_9g20rX-PIIt4T62UggPP2bT5Xf6558cXPoW_v4:S:SPbX7v97LgX_XT06Tv09_62IxSSMS0X4Xf02hiLX8MTf>x54o4 IWLi82xSf9ix8_Pv3wLP.LXPM_X5P9v.SX9PI_XI2996-SbI 99P4P20T48TS4v9S21f99h_/2209UyM7W>wr29\n",
      "owr>bTg959P6_9ioLf4XXX4bg9xlT6Thf ox6\"l.xs_\"Xr5b91Uxl4o82T3g>_2\"x3WcS4PPIX5ybPf07_2M7-7PW=Xx65PvxWoTIXx8Sc=_88SPPSPPlS6PI3vb9v83Imvv1_8_.P88x10oiXi\n",
      "S882v_6b5l9vv:57Um676X236SPlTX__b3\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Generate text \n",
    "# Create a random seed to generate text \n",
    "start = np.random.randint(0, len(X)-1)\n",
    "seed = X[start]\n",
    "lstm_text_generator.generate(seed, temperature=0.5, num_gen=500)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "LSTM-character-based-text-generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
