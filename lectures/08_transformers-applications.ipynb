{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/575_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 8: Self-attention and transformers\n",
    "\n",
    "UBC Master of Data Science program, 2021-22\n",
    "\n",
    "Instructor: Varada Kolhatkar\n",
    "\n",
    "> [Attention is all you need!](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture plan, imports, LO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "### Lecture plan \n",
    "\n",
    "- iClicker questions\n",
    "- Recap and limitations of LSTMs \n",
    "- Self-attention layers\n",
    "- Transformer blocks \n",
    "- Break\n",
    "- iClicker questions\n",
    "- Multihead attention\n",
    "- Transfer learning \n",
    "- Course conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kvarada/miniconda3/envs/563/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learning outcomes\n",
    "\n",
    "From this lecture you will be able to \n",
    "\n",
    "- Broadly explain the limitations of LSTMs. \n",
    "- Explain the idea of self-attention. \n",
    "- Describe the three core operations in self-attention. \n",
    "- Explain the query, key, and value roles in self-attention. \n",
    "- Explain the role of linear projections for query, key, and value in self-attention. \n",
    "- Explain transformer blocks. \n",
    "- Explain the advantages of using transformers over LSTMs. \n",
    "- Broadly explain the idea of multihead attention. \n",
    "- Broadly explain the idea of transfer learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributions\n",
    "\n",
    "This material is heavily based on [Jurafsky and Martin, Chapter 9]((https://web.stanford.edu/~jurafsky/slp3/9.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "What kind of neural network models are at the core of all state-of-the-art NLP models (e.g., BERT, GPT2, GPT3)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge torchdata -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c pytorch torchtext -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        seq_len = data.size(0)\n",
    "        if seq_len != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:seq_len, :seq_len]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 196.68 | loss  8.14 | ppl  3432.89\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 193.74 | loss  6.89 | ppl   981.10\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 212.90 | loss  6.43 | ppl   621.54\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 210.35 | loss  6.30 | ppl   544.39\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 210.32 | loss  6.19 | ppl   486.61\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 203.33 | loss  6.16 | ppl   471.11\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 201.39 | loss  6.11 | ppl   451.71\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 201.97 | loss  6.11 | ppl   449.90\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 202.64 | loss  6.03 | ppl   414.45\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 204.23 | loss  6.01 | ppl   409.52\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 205.87 | loss  5.90 | ppl   364.72\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 207.63 | loss  5.97 | ppl   393.37\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 205.02 | loss  5.95 | ppl   384.11\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 205.29 | loss  5.88 | ppl   356.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 614.83s | valid loss  5.84 | valid ppl   345.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 207.95 | loss  5.86 | ppl   350.34\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 207.18 | loss  5.85 | ppl   345.86\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 210.51 | loss  5.67 | ppl   289.47\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 216.76 | loss  5.70 | ppl   298.03\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 222.27 | loss  5.65 | ppl   284.14\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 209.99 | loss  5.68 | ppl   294.22\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 209.13 | loss  5.69 | ppl   294.87\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 209.08 | loss  5.70 | ppl   299.68\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 208.63 | loss  5.65 | ppl   284.20\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 212.65 | loss  5.67 | ppl   289.80\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 209.20 | loss  5.55 | ppl   257.27\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 209.32 | loss  5.65 | ppl   283.62\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 210.34 | loss  5.64 | ppl   281.71\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 208.25 | loss  5.58 | ppl   264.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 633.66s | valid loss  5.65 | valid ppl   285.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 208.49 | loss  5.60 | ppl   269.59\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 218.41 | loss  5.62 | ppl   275.28\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 213.91 | loss  5.42 | ppl   225.42\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 195.57 | loss  5.48 | ppl   238.99\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 193.72 | loss  5.43 | ppl   228.73\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 193.80 | loss  5.47 | ppl   237.57\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 193.30 | loss  5.48 | ppl   240.68\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 193.66 | loss  5.52 | ppl   249.51\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 193.66 | loss  5.47 | ppl   236.30\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 194.31 | loss  5.48 | ppl   239.42\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 194.83 | loss  5.35 | ppl   211.05\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 193.60 | loss  5.45 | ppl   233.58\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 193.21 | loss  5.46 | ppl   235.95\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 193.60 | loss  5.40 | ppl   222.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 594.84s | valid loss  5.54 | valid ppl   255.25\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransformerModel' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m data_process(text)\n\u001b[1;32m      3\u001b[0m input_ids\n\u001b[0;32m----> 5\u001b[0m greedy_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(input_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.10/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TransformerModel' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "text = 'I enjoy walking with my friend'\n",
    "input_ids = data_process(text)\n",
    "input_ids\n",
    "\n",
    "greedy_output = model.generate(input_ids, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI enjoy walking with my friend\u001b[39m\u001b[38;5;124m'\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# generate text until the output length (which includes the context length) reaches 50\u001b[39;00m\n\u001b[1;32m      4\u001b[0m greedy_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('I enjoy walking with my friend', return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('I enjoy walking with my friend', return_tensors='pt')\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_output = model.generate(input_ids, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning: Text summarization using T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "summary_model = AutoModelWithLMHead.from_pretrained('t5-base', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = ('''\n",
    "           A transformer is a deep learning model that adopts the mechanism of self-attention, \n",
    "           differentially weighting the significance of each part of the input data. \n",
    "           It is used primarily in the fields of natural language processing (NLP) and computer vision (CV).\n",
    "           Like recurrent neural networks (RNNs), transformers are designed to process sequential input data, \n",
    "           such as natural language, with applications towards tasks such as translation and text summarization. \n",
    "           However, unlike RNNs, transformers process the entire input all at once. \n",
    "           The attention mechanism provides context for any position in the input sequence. \n",
    "           For example, if the input data is a natural language sentence, \n",
    "           the transformer does not have to process one word at a time. \n",
    "           This allows for more parallelization than RNNs and therefore reduces training times.\n",
    "           \n",
    "           Transformers were introduced in 2017 by a team at Google Brain and are increasingly the model of choice \n",
    "           for NLP problems, replacing RNN models such as long short-term memory (LSTM). \n",
    "           The additional training parallelization allows training on larger datasets. \n",
    "           This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) \n",
    "           and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, \n",
    "           such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks. \n",
    "           \n",
    "           Before transformers, most state-of-the-art NLP systems relied on gated RNNs, \n",
    "           such as LSTMs and gated recurrent units (GRUs), with added attention mechanisms. \n",
    "           Transformers also make use of attention mechanisms but, unlike RNNs, do not have a recurrent structure. \n",
    "           This means that provided with enough training data, attention mechanisms alone can match the performance \n",
    "           of RNNs with attention.\n",
    "           \n",
    "           Gated RNNs process tokens sequentially, maintaining a state vector that contains \n",
    "           a representation of the data seen prior to the current token. To process the \n",
    "           nth token, the model combines the state representing the sentence up to token n-1 with the information \n",
    "           of the new token to create a new state, representing the sentence up to token n. \n",
    "           Theoretically, the information from one token can propagate arbitrarily far down the sequence, \n",
    "           if at every point the state continues to encode contextual information about the token. \n",
    "           In practice this mechanism is flawed: the vanishing gradient problem leaves the model's state at \n",
    "           the end of a long sentence without precise, extractable information about preceding tokens. \n",
    "           The dependency of token computations on the results of previous token computations also makes it hard \n",
    "           to parallelize computation on modern deep-learning hardware. This can make the training of RNNs inefficient.\n",
    "           \n",
    "           These problems were addressed by attention mechanisms. Attention mechanisms let a model draw \n",
    "           from the state at any preceding point along the sequence. The attention layer can access \n",
    "           all previous states and weigh them according to a learned measure of relevance, providing \n",
    "           relevant information about far-away tokens.\n",
    "           \n",
    "           A clear example of the value of attention is in language translation, where context is essential \n",
    "           to assign the meaning of a word in a sentence. In an English-to-French translation system, \n",
    "           the first word of the French output most probably depends heavily on the first few words of the English input. \n",
    "           However, in a classic LSTM model, in order to produce the first word of the French output, the model \n",
    "           is given only the state vector after processing the last English word. Theoretically, this vector can encode \n",
    "           information about the whole English sentence, giving the model all the necessary knowledge. \n",
    "           In practice, this information is often poorly preserved by the LSTM. \n",
    "           An attention mechanism can be added to address this problem: the decoder is given access to the state vectors of every English input word, \n",
    "           not just the last, and can learn attention weights that dictate how much to attend to each English input state vector.\n",
    "            ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"summarize: \" + sequence,\n",
    "                          return_tensors='pt',\n",
    "                          max_length=512,\n",
    "                          truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = summary_model.generate(inputs, max_length=150, min_length=80, length_penalty=5., num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tokenizer.decode(summary_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention networks: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of LSTMs\n",
    "\n",
    "- LSTMs are better than vanilla RNNs in mitigating the problem of loss of distant information caused by vanishing gradients. \n",
    "- But the underlying problem remains.\n",
    "- For long sequences, there is still a loss of relevant information and difficulties in training. \n",
    "- Also, inherently sequential nature of LSTMs make them hard to parallelize. So they are slow to train. \n",
    "- This led to development of **transformers**. \n",
    "    - They are much faster to train compared to LSTMs and much better at capturing long distance dependencies. \n",
    "    - They are at the core of all state-of-the-art NLP models (e.g., BERT, GPT2, GPT3). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers \n",
    "\n",
    "- Transformers provide an approach to sequence processing but they eliminate recurrent connections in RNNs and LSTMs.   \n",
    "- Similar to RNNs or LSTMs, they map sequences of input vectors $(x_1, \\dots, x_n)$ to sequences of output vectors $(y_1, \\dots, y_n)$ of the same length.  \n",
    "- They are made up of **transformer blocks** which are multilayer networks made by combining \n",
    "    - simple linear layers,\n",
    "    - feedforward layers, and \n",
    "    - **self-attention layers**, which is the key innovation of transformers. \n",
    "- You will see these transformer blocks in modern language model architectures. \n",
    "- Let's first focus on self-attention layer. \n",
    "- Later we'll see how it fits in the larger transformer blocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention layer\n",
    "\n",
    "- Self-attention allows a network to directly extract and use information from arbitrarily large contexts without the need to pass it through intermediate recurrent connections as in RNNs. \n",
    "- Below is a single backward looking self-attention layer which maps sequences of input vectors $(x_1, \\dots, x_n)$ to sequences of output vectors $(y_1, \\dots, y_n)$ of the same length. \n",
    "- When processing an item at time $t$, the model has access to all of the inputs up to and including the one under consideration. \n",
    "- It does not have access to the input beyond the current one. \n",
    "- Note that unlike RNNs or LSTMs, each computation can be done independently; it does not depend upon the previous computation which allows us to easily parallelize forward pass and the training of such models. \n",
    "\n",
    "![](img/self_attention.png)\n",
    "<!-- <img src=\"img/self_attention.png\" width=\"600\" height=\"600\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention: Core idea \n",
    "\n",
    "- For each token in the sequence, we assign a weight based on how relevant they are to the token under consideration. \n",
    "- Calculate the output for the current token based on these weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the output $y$ for the token _nuts_ in the given context\n",
    "\n",
    "![](img/self_attention_ex_nuts.png)\n",
    "\n",
    "<!-- <img src=\"img/self_attention_ex_nuts.png\" width=\"600\" height=\"600\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The key operations in self-attention\n",
    "\n",
    "So in order to calculate the output $y_i$\n",
    "\n",
    "- We score token $x_i$ with all previous tokens $x_j$ by taking the dot product between them. \n",
    "$$\\text{score}(x_i, x_j) = x_i \\cdot x_j$$\n",
    "\n",
    "- We apply $\\text{softmax}$ on these scores to get probability distribution over these scores. \n",
    "$$\\alpha_{ij} = \\text{softmax}(\\text{score}(x_i \\cdot x_j)), \\forall j \\leq i$$\n",
    "\n",
    "- The output is the weighted sum of the inputs seen so far, where the weights correspond to the $\\alpha$ values calculated above. \n",
    " $$y_i = \\sum_{j \\leq i} \\alpha_{ij}x_j$$\n",
    " \n",
    "These three operations represent the core of an attention-based approach. These operations can be carried out independently for each input allowing easy parallelism. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query, Key, and Value roles\n",
    "\n",
    "Note that in the process of calculating outputs corresponding to each input, each input embedding plays three kinds of roles. \n",
    "\n",
    "- **Query**: _the current focus of attention_ when being compared to all previous inputs. \n",
    "- **Key**: _a preceding input_ being compared to the current focus of attention.    \n",
    "- **Value**: used to compute the output for the current focus of attention. \n",
    "\n",
    "For these three roles transformer introduces three weight matrices: $W^Q, W^K, W^V$. These weights will be used to project each input vector $x_i$ into its role as a key, query, or value.\n",
    "\n",
    "$$q_i = W^Qx_i$$\n",
    "$$k_i = W^Kx_i$$\n",
    "$$v_i = W^Vx_i$$\n",
    "\n",
    "For now let's assume that all these weight matrices have the same dimensionality and so the projected vectors in each case are going to be of the same size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these projections our equations become: \n",
    "\n",
    "- We score the $x_i$ with all previous tokens $x_j$ by taking the dot product between $x_i$'s query vector $q_i$ and $x_j$'s key vector $k_j$:  \n",
    "$$\\text{score}(x_i, x_j) = q_i \\cdot k_j$$\n",
    "\n",
    "- The softmax calculation remains the same but the output calculation for $y_i$ is now based on a weighted sum over the projected vectors $v$:\n",
    " $$y_i = \\sum_{j \\leq i} \\alpha_{ij}v_j$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention: Calculating the value of $y_3$\n",
    "\n",
    "![](img/self_attention_ex.png)\n",
    "\n",
    "<!-- <img src=\"img/self_attention_ex.png\" width=\"400\" height=\"400\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention example: Calculating the value of $y_3$\n",
    "\n",
    "- Let's calculate the output of _**nuts**_ in the following sequence with $K, Q, V$ matrices.  \n",
    "> had bowl nuts \n",
    "- Suppose input embedding is of size 300. \n",
    "- Suppose the projection matrices $W^k, W^q, W^v$ are of shape $300 \\times 100$. \n",
    "- So word$_k$, word$_q$, word$_v$ provide 100-dimensional projections of each word corresponding to the key, query and value roles. For example, nuts$_k$, nuts$_q$, nuts$_v$ represent 100-dimensional projections of the word **nuts** corresponding to its key, query, and value roles, respectively.\n",
    "- The dot products will be calculated between the appropriate query and key projections. In this example, we will calculate the following dot products:\n",
    "    - $\\text{nuts}_q \\cdot \\text{had}_k$\n",
    "    - $\\text{nuts}_q \\cdot \\text{bowl}_k$    \n",
    "    - $\\text{nuts}_q \\cdot \\text{nuts}_k$\n",
    "- We apply softmax on these dot products. Suppose the softmax output in this toy example is \n",
    "\\begin{bmatrix} 0.005 & 0.085 & 0.91 \\end{bmatrix}\n",
    "- So we have weights associated with three input words: _had_ (0.005), _bowl_ (0.085) and _nuts_ (0.91)\n",
    "- We can calculate the output as the weighted sum of the inputs. Here we will use the value projections of the inputs: $0.005 \\times \\text{had}_v + 0.085 \\times \\text{bowl}_v + 0.91 \\times \\text{nuts}_v$\n",
    "- Since we will be adding 100 dimensional vectors (size of our projections), the dimensionality of the output $y_3$ is going to be 100. \n",
    "\n",
    "![](img/self_attention_nuts.png)\n",
    "\n",
    "<!-- <img src=\"img/self_attention_ex.png\" width=\"400\" height=\"400\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the dot products\n",
    "\n",
    "- The result of a dot product can be arbitrarily large and exponentiating such values can lead to numerical issues and problems during training. \n",
    "- So the dot products are usually scaled before applying the softmax. \n",
    "- The most common scaling is where we divide the dot product by the square root of the dimensionality of the query and the key vectors. \n",
    "$$\\text{score}(x_i, x_j) = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is how we calculate a single output of a single time step $i$. \n",
    "- Would the output calculation at different time steps be dependent upon each other? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient calculations with matrix multiplication \n",
    "\n",
    "- $X_{N \\times d} \\rightarrow$ matrix of all tokens in a sequence of length $N$ with each token represented with a $d$ dimensional embedding. Each row of $X$ is embedding representation of one token of the input. The we can calculate $Q, K, V$ as follows.\n",
    "\n",
    "$$Q = XW^Q$$\n",
    "$$K = XW^K$$\n",
    "$$V = XW^V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With these, we can now calculate all the query-key scores simultaneously as $Q \\times K$. \n",
    "\n",
    "![](img/self_attention_calc_all.png)\n",
    "\n",
    "<!-- <img src=\"img/self_attention_calc_all.png\" width=\"300\" height=\"300\"> -->\n",
    "\n",
    "- We can them apply softmax on all rows and multiply the resulting matrix by $V$.\n",
    "\n",
    "$$SelfAttention(Q, K, V) = \\text{softmax}(\\frac{QK}{\\sqrt{d_k}})V$$\n",
    "\n",
    "- Finally, we get output sequence $y_1, \\dots, y_n$.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What's the problem with the approach above?\n",
    "    - This process goes a bit too far since the calculation of the comparisons in $QK$ results in a score for each value to each key value, _including those that follow the query_. \n",
    "    - Is this appropriate in the setting of language modeling? \n",
    "\n",
    "![](img/self_attention_calc_partial.png)\n",
    "\n",
    "<!-- <img src=\"img/self_attention_calc_partial.png\" width=\"300\" height=\"300\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer blocks\n",
    "\n",
    "- In many advanced architectures, you will see transformer blocks which consists of\n",
    "    - The self-attention layer\n",
    "    - Additional feedforward layers\n",
    "    - Residual connections\n",
    "    - Normalizing layers\n",
    "\n",
    "![](img/transformer_block.png)\n",
    "\n",
    "<!-- <img src=\"img/transformer_block.png\" width=\"350\" height=\"350\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The input and output dimensions of these layers are matched so that they can be stacked. \n",
    "- In deep networks, **residual connections** are connections that pass information from a lower layer to a higher layer without going through the intermediate layer. Why? It has been shown that allowing information from the activation going forward and the gradient going backwards to skip a layer improves learning and gives higher level layers direct access to information from lawer layers. \n",
    "- We then have a summed vector (projected output of the attention or feedforward layer + input of the attention or feedforward layers). \n",
    "- **Layer normalization or layer norm** normalizes the resulting vector which improves training performance in deep neural networks keeping the values of a hidden layer in a range that facilitates gradient-based training. Layer norm applies something similar to `StandardScaler` so that the mean is 0 and standard deviation is 1 in the vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break\n",
    "\n",
    "![](img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you\n",
    "\n",
    "iClicker cloud join link: https://join.iclicker.com/4QVT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 8.2: Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) The main difference between the RNN layer and a self-attention layer is that \n",
    "in self-attention, we pass the information without intermediate recurrent connections. \n",
    "- (B) In self-attention, the output $y_i$ of input $x_i$ at time $i$ is a single number. \n",
    "- (C) Calculating attention weights is quadratic in the length of the input \n",
    "since we need to compute dot products between each pair of tokens in\n",
    "the input.  \n",
    "- (D) Self-attention results in contextual embeddings. \n",
    "- (E) Transformers seem to be more intuitive compared to LSTMs. \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 8.2: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "- (A) True\n",
    "- (B) False\n",
    "- (C) True\n",
    "- (D) True\n",
    "- (E) True (for me)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention\n",
    "\n",
    "- Different words in a sentence can relate to each other in many different ways simultaneously. \n",
    "- Consider the sentence below. \n",
    "> The cat was scared because it didn't recognize me in my mask. \n",
    "\n",
    "Let's look at all the dependencies in this sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I gave my cat some food\")\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So a single attention layer usually is not enough to capture all different kinds of parallel relations between inputs. \n",
    "- Transformers address this issue with **multihead self-attention layers**.\n",
    "- These self-attention layers are called **heads**.\n",
    "- They are at the same depth of the model, operate in parallel, each with a different set of parameters. \n",
    "- The idea is that with these different sets of parameters, each head can learn different aspects of the relationships that exist among inputs.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "MultiHeadAttn(X) &= (\\text{head}_1 \\oplus \\text{head}_2 \\dots \\oplus \\text{head}_h)W^O\\\\\n",
    "               Q &= XW_i^Q ; K = XW_i^K ; V = XW_i^V\\\\\n",
    "               \\text{head}_i &= SelfAttention(Q,K,V)\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/multihead_attention.png)\n",
    "\n",
    "<!-- <img src=\"img/multihead_attention.png\" width=\"600\" height=\"600\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention visualization\n",
    "- Similar to RNNs you can stack self-attention layers or multihead self-attention layers on the top of each other.\n",
    "- Let's look at this visualization which shows where the attention of different attention heads is going in multihead attention. \n",
    "    - [Multi-head attention interactive visualization](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=OJKU36QAfqOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Let's try it out with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 12\n",
    "num_heads = 4\n",
    "seqlen = 20\n",
    "\n",
    "multi_attn = nn.MultiheadAttention(embed_dim, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.rand(10, embed_dim)  # target_seq_len, query_embedding_dim\n",
    "key = torch.rand(seqlen, embed_dim)  # source_seq_len, key_embedding_dim\n",
    "value = torch.rand(seqlen, embed_dim)  # source_seq_len, value_embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output, attn_output_weights = multi_attn(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers as language models\n",
    "\n",
    "- Given a training corpus of plain text we want to train a model to predict the next word in a sequence with semi-supervised learning. \n",
    "- At each time step, given all the preceding words, the final transformer layer produces an output distribution over the entire vocabulary. \n",
    "- During training the probability assigned to the correct word is used to calculate the cross-entropy loss for each item in the sequence. \n",
    "- Similar to RNNs, the loss for the training sequence is the average cross-entropy loss over the entire sequence. \n",
    "\n",
    "![](img/transformer_language_model.png)\n",
    "\n",
    "<!-- <img src=\"img/transformer_language_model.png\" width=\"700\" height=\"700\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN as language models\n",
    "\n",
    "- What's the difference between RNN-based language models vs. Transformers-based language model? \n",
    "    - The calculation of outputs and the losses at each time step was inherently serial in RNNs.\n",
    "    - With transformers, each training item can be processed in parallel.\n",
    "\n",
    "![](img/RNN_language_model.png)\n",
    "\n",
    "<!-- <img src=\"img/RNN_language_model.png\" width=\"700\" height=\"700\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual generation \n",
    "\n",
    "- A seed is provided and the model is asked to generate a possible completition to it. \n",
    "- Also called **autoregressive text completion**. \n",
    "- During the generation process, the model has direct access to the priming context as well as the generated tokens. \n",
    "\n",
    "![](img/transformer_autoregressive_text_generation.png)\n",
    "\n",
    "<!-- <img src=\"img/transformer_autoregressive_text_generation.png\" width=\"700\" height=\"700\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional transformer encoders\n",
    "\n",
    "- Models such as [**BERT**](https://en.wikipedia.org/wiki/BERT_(language_model)) and its variant **RoBERTa** are **bidirectional transformer models**.  \n",
    "- Remember the [sentence transformers](https://www.sbert.net/) you used in DSCI 563 lab1 to get sentence embeddings? They are based on **BERT**.  \n",
    "- In the lab you're doing transfer learning using RoBERTa which was proposed in 2019. \n",
    "    - If you look at the `config_spacy.cfg`, you'll see the name of the model which is `roberta-base`.  \n",
    "- What underlies these models? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward looking self-attention \n",
    "\n",
    "- We have seen backward looking self-attention. \n",
    "- Each output is computed using only information seen **earlier** in the context. \n",
    "- This is appropriate for language models. \n",
    "\n",
    "![](img/self_attention.png)\n",
    "\n",
    "<!-- <img src=\"img/self_attention.png\" width=\"700\" height=\"700\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidirectional self-attention\n",
    "\n",
    "- Information flows in both directions in bidirectional self attention. \n",
    "- The model attends to all inputs, both before and after the current one.\n",
    "\n",
    "![](img/bidirectional_self_attention.png)\n",
    "\n",
    "<!-- <img src=\"img/bidirectional_self_attention.png\" width=\"700\" height=\"700\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the computations are exactly the same as before. \n",
    "- The matrix below shows $q_i \\cdot k_i$ comparisons. We do not set the values in the upper triangle to $\\infty$ anymore. \n",
    "\n",
    "![](img/self_attention_calc_all.png)\n",
    "\n",
    "<!-- <img src=\"img/self_attention_calc_all.png\" width=\"400\" height=\"400\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original bidirectional transformer encoder model (BERT) consisted of the following:\n",
    "\n",
    "- A subword vocabulary of 30,000 tokens \n",
    "- Hidden layers of size 768 (If you recall the sentence embeddings from DSCI 563 lab 1 were 768 dimensional.)\n",
    "- 12 layers of transformer blocks with 12 multihead attention layers each! \n",
    "\n",
    "The model has over 100M parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual embeddings \n",
    "\n",
    "- The methods like word2vec learned a single vector embedding for each unique word $w$ in the vocabulary. \n",
    "- By contrast, with contextual embeddings, such as those learned by popular methods such like BERT or GPT or their descendants, each word $w$ will be represented by a different vector each time it appears in a different context. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "- Pretraining is the process of learning some representation of input (words or sentences in NLP context) by processing large amounts of text.  \n",
    "- Fine-tuning is the process of taking the representations from these pretrained models and further training the model often by adding a neural net classifier to perform some downstream task such as named entity recognition similar to what you are doing in the lab. \n",
    "- The intuition is that the pretrained phase learns a language model which in the process learns a rich representation of contextual word meaning enabling the model to be fine-tuned to the requirements of the downstream language understanding task.   \n",
    "- This pretrain and fine-tune paradigm is called **transfer learning** in machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/BERT_sequence_labeling.png)\n",
    "\n",
    "<!-- <img src=\"img/BERT_sequence_labeling.png\" width=\"700\" height=\"700\"> -->\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many things related to transformers which we have not covered. You can look up the following if you want to know more. [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) is an excellent resource. \n",
    "- Seq2seq models with attention \n",
    "- Masked language modeling \n",
    "- Contextual embeddings \n",
    "- ...\n",
    "\n",
    "Transformers are not only for NLP. They have been successfully applied in many other domains often with state-of-the-art results. For example, \n",
    "- [Vision Transformers](https://arxiv.org/pdf/2010.11929.pdf)\n",
    "- Bioinformatics: See [this](https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology) and [this](http://people.csail.mit.edu/tommi/papers/Ingraham_etal_neurips19.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "Attention-mechanisms and transformers are quite new. But there are many resources on transformers. I'm listing a few resources here. \n",
    "\n",
    "- [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Transformers](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Transformers documentation](https://huggingface.co/transformers/index.html)\n",
    "- [A funny video: I taught an AI to make pasta](https://www.youtube.com/watch?v=Y_NvR5dIaOY)\n",
    "- [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary and wrap up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 1 ✅\n",
    "\n",
    "- Markov models, language models, text generation \n",
    "\n",
    "![](img/Markov_autocompletion.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/Markov_autocompletion.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applications of Markov models\n",
    "\n",
    "![](img/Markov_chain_applications.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/Markov_chain_applications.png\" width=\"500\" height=\"500\"> -->\n",
    "<!-- </center>     -->\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 2 ✅\n",
    "\n",
    "- Hidden Markov models, speech recognition, POS tagging\n",
    "\n",
    "![](img/hmm_eks.gif)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/hmm_eks.gif\" height=\"800\" width=\"800\"> -->\n",
    "<!-- </center> -->\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 3 ✅\n",
    "\n",
    "- Topic modeling (Latent Dirichlet Allocation (LDA)), organizing documents \n",
    "- Introduction to Recurrent Neural Networks (RNNs)\n",
    "\n",
    "![](img/TM_food_magazines.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_food_magazines.png\" height=\"1000\" width=\"1000\">  -->\n",
    "<!-- </center>     -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 4 ✅\n",
    "\n",
    "- LSTMs, Transformers, Custom NER using transfer learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-accomplished.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Final remarks \n",
    "\n",
    "That's all! I had fun teaching you this complex material. I very much appreciate your support, patience, and great questions ♥️!   \n",
    "\n",
    "It has been a challenging year but we all tried to make the best out of it. I wish you every success in your job search. Stay in touch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time for course evaluations\n",
    "\n",
    "I would love to hear your thoughts on this course. When you get a chance, it'll be great if you fill in the evaluation survey for this course on [Canvas](https://canvas.ubc.ca/courses/83559/external_tools/4732). \n",
    "\n",
    "The evaluation closing date is: **April 29th, 2022**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:563]",
   "language": "python",
   "name": "conda-env-563-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
